{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fe88401-e78b-4706-aace-2b49d47fddba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:39:39,307 - 2210765 - root - INFO - first line of multilex_scraper_xform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korea\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.reuters.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "2022-06-23 13:40:44,226 - 2210765 - root - INFO - Reuters function ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTC\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:40:51,178 - 2210765 - root - INFO - GoogleAlert function ended\n",
      "2022-06-23 13:40:51,179 - 2210765 - root - INFO - live.com: invoking requests.url()=https://www.livemint.com/Search/Link/Keyword/ipo\n",
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.livemint.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:40:51,389 - 2210765 - root - INFO - live.com: completed requests.url()=https://www.livemint.com/Search/Link/Keyword/ipo\n",
      "2022-06-23 13:40:51,396 - 2210765 - root - INFO - Live function ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standartnews bulgaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:41:13,524 - 2210765 - root - INFO - www.kontan.co.id: invoking requests.url()=https://www.kontan.co.id/search/?search=ipo&Button_search=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:41:14,290 - 2210765 - root - INFO - www.kontan.co.id: completed invoking requests.url()=https://www.kontan.co.id/search/?search=ipo&Button_search=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:41:55,823 - 2210765 - root - INFO - www.xinhuanet.com: invoking requests.url()=http://www.xinhuanet.com/english/mobile/business.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/casia/uzbekistan/3579837.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/business/finance/3582478.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/casia/uzbekistan/3596383.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/business/economy/3607527.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/business/energy/3605277.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/casia/uzbekistan/3582506.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/business/economy/3564275.html\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:41:57,405 - 2210765 - root - INFO - www.xinhuanet.com: completed invoking requests.url()=http://www.xinhuanet.com/english/mobile/business.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFR\n",
      "Moneycontrol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:44:03,251 - 2210765 - root - INFO - cnbc.com: invoking requests.url()=https://www.cnbc.com/id/10000666/device/rss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.cnbc.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "2022-06-23 13:44:03,457 - 2210765 - root - INFO - cnbc.com: completed requests.url()=https://www.cnbc.com/id/10000666/device/rss\n",
      "2022-06-23 13:44:03,650 - 2210765 - root - INFO - CNBC_seeking function ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toi\n",
      "toi: err: Failed to find title in page. Link: https://timesofindia.indiatimes.com/business/india-business/an-impossible-ipo-problem-in-the-age-of-unicorns/articleshow/89964001.cms\n",
      "toi: err: Failed to find title in page. Link: https://timesofindia.indiatimes.com/business/india-business/survey-author-explains-why-govt-is-bullish-on-the-economy/articleshow/89247812.cms\n",
      "German\n",
      "['https://www.tagesschau.de/wirtschaft/finanzen/marktberichte/dax-chance-nach-kurseinbruch-dow-gold-oel-101.html', 'https://www.tagesschau.de/wirtschaft/unternehmen/gorillas-entlassungen-boersengang-101.html', 'https://www.tagesschau.de/wirtschaft/finanzen/marktberichte/dax-dow-geldanlage-ukraine-inflation-ezb-fed-anleihen-oel-wti-brent-101.html', 'https://www.tagesschau.de/wirtschaft/unternehmen/porsche-volkswagen-boersengang-101.html', 'https://www.tagesschau.de/wirtschaft/unternehmen/biotech-branche-verzeichnet-hohe-investments-101.html', 'https://www.tagesschau.de/wirtschaft/unternehmen/gostudent-bewertung-drei-milliarden-101.html', 'https://www.tagesschau.de/wirtschaft/finanzen/boersengaenge-2021-101.html', 'https://www.tagesschau.de/wirtschaft/finanzen/marktberichte/marktbericht-dax-dow-jones-147.html', 'https://www.tagesschau.de/wirtschaft/unternehmen/daimler-truck-ipo-boersengang-frankfurt-101.html', 'https://www.tagesschau.de/wirtschaft/finanzen/marktberichte/marktbericht-dax-dow-jones-169.html']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:44:12,783 - 2210765 - root - INFO - Japannews: invoking requests.url()=https://www.japantimes.co.jp/tag/ipo/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japannews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:44:14,201 - 2210765 - root - INFO - Japannews: Completed invoking requests.url()=https://www.japantimes.co.jp/tag/ipo/\n",
      "2022-06-23 13:45:59,853 - 2210765 - root - INFO - Romania: invoking requests.url()=https://adevarul.ro/cauta/?terms=ofert%C4%83%20public%C4%83%20ini%C8%9Bial%C4%83&fromDate=2012-1-1&toDate=2021-3-10&tab=mrarticle&page=1&sortBy=cronologic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romania\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:46:04,650 - 2210765 - root - INFO - Romania: Completed invoking requests.url()=https://adevarul.ro/cauta/?terms=ofert%C4%83%20public%C4%83%20ini%C8%9Bial%C4%83&fromDate=2012-1-1&toDate=2021-3-10&tab=mrarticle&page=1&sortBy=cronologic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian\n",
      "['https://ipo.einnews.com/article/578082384?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/578039287?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/578022925?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/578056724?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/578005153?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/578002041?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/578001116?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/578001268?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/577996211?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/577989069?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:46:50,623 - 2210765 - root - INFO - Swedish: invoking requests.url()=https://www.svd.se/sok?q=ipo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swedish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:46:51,084 - 2210765 - root - INFO - Swedish: Completed invoking requests.url()=https://www.svd.se/sok?q=ipo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not pass FilterFunction\n",
      "GoogleAlert1\n",
      "GoogleAlert2\n",
      "GoogleAlert3\n",
      "GoogleAlert4\n",
      "GoogleAlert5\n",
      "IPOmonitor\n",
      "Globallegal\n",
      "Seenews\n",
      "Bisnis\n",
      "['https://market.bisnis.com/read/20220623/192/1547141/ipo-aman-agrindo-gula-dan-prospek-pasar-gula-indonesia', 'https://market.bisnis.com/read/20220623/192/1547085/perusahaan-gula-aman-agrindo-gula-mau-ipo-tawarkan-rp250rp300-per-saham', 'https://market.bisnis.com/read/20220623/192/1547072/produsen-kimia-tekstil-chemstar-chem-tawarkan-harga-ipo-rp150-rp190', 'https://market.bisnis.com/read/20220623/192/1546966/baru-ipo-april-sico-gelar-rups-mau-bagi-dividen-laba-2021', 'https://ekonomi.bisnis.com/read/20220622/47/1546938/saraswanti-indoland-akan-bangun-dua-menara-apartemen-senilai-rp1815-miliar', 'https://market.bisnis.com/read/20220622/192/1546846/autopedia-aslc-yakin-kinerja-tumbuh-2-kali-lipat-dan-serap-dana-ipo-rp150-miliar-di-2022', 'https://ekonomi.bisnis.com/read/20220622/620/1546741/regulator-china-beri-lampu-hijau-untuk-ant-group-bangun-holding', 'https://market.bisnis.com/read/20220622/192/1546500/goto-gojek-tokopedia-gencar-transaksi-afiliasi-saham-goto-ngebut', 'https://koran.bisnis.com/read/20220622/441/1546401/rencana-ipo-arko-chem-galang-dana-ekspansi', 'https://market.bisnis.com/read/20220621/192/1546176/arkora-hydro-arko-bidik-dana-ipo-rp17999-miliar-ini-rencana-ekspansinya']\n",
      "RomaniaNew\n",
      "RomaniaInsider\n",
      "spacemoney\n",
      "Kontan1\n",
      "['https://insight.kontan.co.id/news/kondisi-ekonomi-global-tak-menentu-sejumlah-bank-tunggu-waktu-ipo', 'https://investasi.kontan.co.id/news/bank-sumut-bjb-syariah-dan-bank-sampoerna-bersiap-ipo-untuk-perkuat-permodalan', 'https://keuangan.kontan.co.id/news/perkuat-permodalan-perbankan-berencana-ipo', 'https://insight.kontan.co.id/news/arkora-hydro-bersiap-gelar-akuisisi-setelah-ipo', 'https://stocksetup.kontan.co.id/news/dua-perusahaan-yang-berencana-ipo-berstatus-canceled-apa-sebabnya', 'https://investasi.kontan.co.id/news/dua-perusahaan-yang-akan-ipo-berstatus-canceled-apa-artinya', 'https://internasional.kontan.co.id/news/perusahaan-infrastruktur-milik-miliarder-razon-melirik-ipo-di-manila', 'https://stocksetup.kontan.co.id/news/setelah-ipo-harga-saham-ini-cenderung-turun-investor-harus-jual-atau-beli', 'https://investasi.kontan.co.id/news/harga-saham-pasca-ipo-tahun-2022-banyak-yang-turun-saatnya-jual-atau-beli', 'https://investasi.kontan.co.id/news/begini-rekomendasi-dari-analis-untuk-saham-saham-yang-baru-ipo', 'https://stocksetup.kontan.co.id/news/saraswanti-indoland-optimistis-kinerjanya-kian-meningkat-setelah-ipo-1', 'https://stocksetup.kontan.co.id/news/kementerian-bumn-dorong-bumn-dan-anak-usaha-ipo-tahun-ini', 'https://investasi.kontan.co.id/news/bidak-dana-ipo-rp-68-miliar-saraswanti-indoland-optimistis-kinerjanya-kian-meningkat', 'https://investasi.kontan.co.id/news/simak-saran-dan-rekomendasi-analis-untuk-saham-saham-yang-baru-ipo', 'https://investasi.kontan.co.id/news/lima-perusahaan-ini-gelar-ipo-mana-yang-menarik', 'https://insight.kontan.co.id/news/adu-cantik-saham-ipo-antara-chem-swid-trgu-hill-dan-arko-siapa-lebih-menarik', 'https://insight.kontan.co.id/news/chemstar-indonesia-chem-ipo-pancing-minat-calon-investor-dengan-iming-iming-waran', 'https://investasi.kontan.co.id/news/arkora-hydro-mengincar-dana-ipo-hingga-rp-17977-miliar', 'https://investasi.kontan.co.id/news/chemstar-indonesia-membidik-dana-segar-rp-95-miliar-dari-ipo', 'https://investasi.kontan.co.id/news/kementerian-bumn-siap-dorong-ipo-bumn-dan-anak-usaha-tahun-ini']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO', 'Dua Perusahaan yang Akan IPO Berstatus Canceled, Apa Artinya?']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO', 'Dua Perusahaan yang Akan IPO Berstatus Canceled, Apa Artinya?', 'Perusahaan Infrastruktur Milik Miliarder Razon Melirik IPO di Manila']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO', 'Dua Perusahaan yang Akan IPO Berstatus Canceled, Apa Artinya?', 'Perusahaan Infrastruktur Milik Miliarder Razon Melirik IPO di Manila', 'Harga Saham Pasca IPO Tahun 2022 Banyak Yang Turun, Saatnya Jual atau Beli?']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO', 'Dua Perusahaan yang Akan IPO Berstatus Canceled, Apa Artinya?', 'Perusahaan Infrastruktur Milik Miliarder Razon Melirik IPO di Manila', 'Harga Saham Pasca IPO Tahun 2022 Banyak Yang Turun, Saatnya Jual atau Beli?', 'Begini Rekomendasi dari Analis untuk Saham-saham yang Baru IPO']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO', 'Dua Perusahaan yang Akan IPO Berstatus Canceled, Apa Artinya?', 'Perusahaan Infrastruktur Milik Miliarder Razon Melirik IPO di Manila', 'Harga Saham Pasca IPO Tahun 2022 Banyak Yang Turun, Saatnya Jual atau Beli?', 'Begini Rekomendasi dari Analis untuk Saham-saham yang Baru IPO', 'Bidak Dana IPO Rp 68 Miliar, Saraswanti Indoland Optimistis Kinerjanya Kian Meningkat']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO', 'Dua Perusahaan yang Akan IPO Berstatus Canceled, Apa Artinya?', 'Perusahaan Infrastruktur Milik Miliarder Razon Melirik IPO di Manila', 'Harga Saham Pasca IPO Tahun 2022 Banyak Yang Turun, Saatnya Jual atau Beli?', 'Begini Rekomendasi dari Analis untuk Saham-saham yang Baru IPO', 'Bidak Dana IPO Rp 68 Miliar, Saraswanti Indoland Optimistis Kinerjanya Kian Meningkat', 'Simak Saran dan Rekomendasi dari Analis untuk Saham-saham yang Baru IPO']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO', 'Dua Perusahaan yang Akan IPO Berstatus Canceled, Apa Artinya?', 'Perusahaan Infrastruktur Milik Miliarder Razon Melirik IPO di Manila', 'Harga Saham Pasca IPO Tahun 2022 Banyak Yang Turun, Saatnya Jual atau Beli?', 'Begini Rekomendasi dari Analis untuk Saham-saham yang Baru IPO', 'Bidak Dana IPO Rp 68 Miliar, Saraswanti Indoland Optimistis Kinerjanya Kian Meningkat', 'Simak Saran dan Rekomendasi dari Analis untuk Saham-saham yang Baru IPO', 'Lima Perusahaan Ini Gelar IPO, Mana yang Menarik?']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO', 'Dua Perusahaan yang Akan IPO Berstatus Canceled, Apa Artinya?', 'Perusahaan Infrastruktur Milik Miliarder Razon Melirik IPO di Manila', 'Harga Saham Pasca IPO Tahun 2022 Banyak Yang Turun, Saatnya Jual atau Beli?', 'Begini Rekomendasi dari Analis untuk Saham-saham yang Baru IPO', 'Bidak Dana IPO Rp 68 Miliar, Saraswanti Indoland Optimistis Kinerjanya Kian Meningkat', 'Simak Saran dan Rekomendasi dari Analis untuk Saham-saham yang Baru IPO', 'Lima Perusahaan Ini Gelar IPO, Mana yang Menarik?', 'Arkora Hydro Mengincar Dana IPO Hingga Rp 179,77 Miliar']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO', 'Dua Perusahaan yang Akan IPO Berstatus Canceled, Apa Artinya?', 'Perusahaan Infrastruktur Milik Miliarder Razon Melirik IPO di Manila', 'Harga Saham Pasca IPO Tahun 2022 Banyak Yang Turun, Saatnya Jual atau Beli?', 'Begini Rekomendasi dari Analis untuk Saham-saham yang Baru IPO', 'Bidak Dana IPO Rp 68 Miliar, Saraswanti Indoland Optimistis Kinerjanya Kian Meningkat', 'Simak Saran dan Rekomendasi dari Analis untuk Saham-saham yang Baru IPO', 'Lima Perusahaan Ini Gelar IPO, Mana yang Menarik?', 'Arkora Hydro Mengincar Dana IPO Hingga Rp 179,77 Miliar', 'Chemstar Indonesia Membidik Dana Segar Rp 95 Miliar dari IPO']\n",
      "['Bank Sumut, BJB Syariah dan Bank Sampoerna Bersiap IPO untuk Perkuat Permodalan ', 'Perkuat Permodalan, Perbankan Berencana IPO', 'Dua Perusahaan yang Akan IPO Berstatus Canceled, Apa Artinya?', 'Perusahaan Infrastruktur Milik Miliarder Razon Melirik IPO di Manila', 'Harga Saham Pasca IPO Tahun 2022 Banyak Yang Turun, Saatnya Jual atau Beli?', 'Begini Rekomendasi dari Analis untuk Saham-saham yang Baru IPO', 'Bidak Dana IPO Rp 68 Miliar, Saraswanti Indoland Optimistis Kinerjanya Kian Meningkat', 'Simak Saran dan Rekomendasi dari Analis untuk Saham-saham yang Baru IPO', 'Lima Perusahaan Ini Gelar IPO, Mana yang Menarik?', 'Arkora Hydro Mengincar Dana IPO Hingga Rp 179,77 Miliar', 'Chemstar Indonesia Membidik Dana Segar Rp 95 Miliar dari IPO', 'Kementerian BUMN Siap Dorong IPO BUMN dan Anak Usaha Tahun Ini']\n",
      "Euronews\n",
      "chinatoday\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 13:55:57,489 - 2210765 - urllib3.connection - WARNING - Certificate did not match expected hostname: www.taipanpublishinggroup.com. Certificate: {'subject': ((('commonName', 'forextrading.company'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', \"Let's Encrypt\"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '034E402ECD154C425E1CA5CBD61A798897D6', 'notBefore': 'Jun 15 10:50:42 2022 GMT', 'notAfter': 'Sep 13 10:50:41 2022 GMT', 'subjectAltName': (('DNS', 'forextrading.company'),), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}\n",
      "2022-06-23 13:59:35,135 - 2210765 - urllib3.connection - WARNING - Certificate did not match expected hostname: www.hksfc.org.hk. Certificate: {'subject': ((('countryName', 'HK'),), (('localityName', 'Hong Kong'),), (('organizationName', 'Securities and Futures Commission'),), (('commonName', 'www.sfc.hk'),)), 'issuer': ((('countryName', 'GB'),), (('stateOrProvinceName', 'Greater Manchester'),), (('localityName', 'Salford'),), (('organizationName', 'Sectigo Limited'),), (('commonName', 'Sectigo RSA Organization Validation Secure Server CA'),)), 'version': 3, 'serialNumber': 'AE737834AA040781BAD541C7BF8075BB', 'notBefore': 'Jul 24 00:00:00 2020 GMT', 'notAfter': 'Oct 16 23:59:59 2022 GMT', 'subjectAltName': (('DNS', 'www.sfc.hk'), ('DNS', 'sfc.hk')), 'OCSP': ('http://ocsp.sectigo.com',), 'caIssuers': ('http://crt.sectigo.com/SectigoRSAOrganizationValidationSecureServerCA.crt',), 'crlDistributionPoints': ('http://crl.sectigo.com/SectigoRSAOrganizationValidationSecureServerCA.crl',)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al jazeera Qatar\n",
      "Al Jazeera not working\n",
      "Koreatimes\n",
      "zdnet\n",
      "zdnet not working\n",
      "Arabnews\n",
      "Chosun\n",
      "kngnet\n",
      "['https://www.koreanewsgazette.com/one-store-scraps-ipo-plan-due-to-weak-investor-confidence-sources/', 'https://www.koreanewsgazette.com/sk-shieldus-eyes-raising-up-to-3-55-tln-won-through-ipo-next-month/', 'https://www.koreanewsgazette.com/hyundai-mipo-wins-94-bln-won-product-carrier-order-in-asia/', 'https://www.koreanewsgazette.com/sk-ons-ipo-unlikely-to-take-place-until-after-2025-sk-innovation-ceo/', 'https://www.koreanewsgazette.com/money-raised-through-ipos-hits-record-high-in-2021-amid-ample-liquidity-bullish-sentiment/', 'https://www.koreanewsgazette.com/hyundai-mipo-dockyard-remains-in-red-in-2021/', 'https://www.koreanewsgazette.com/hyundai-mipo-dockyard-remains-in-red-in-q4/', 'https://www.koreanewsgazette.com/lg-to-unveil-life-size-omnipod-self-driving-concept-car/']\n",
      "kedg\n",
      "kedg not working\n",
      "Wealthx\n",
      "Antara\n",
      "Asia Insurance\n",
      "economic_times\n",
      "['https://economictimes.indiatimes.com/markets/ipos/fpos/skincare-startup-mamaearth-eyes-3-bln-valuation-in-2023-ipo/articleshow/92359379.cms', 'https://economictimes.indiatimes.com/markets/ipos/fpos/noida-based-india-exposition-mart-gets-sebi-nod-for-ipo/articleshow/92355937.cms', 'https://economictimes.indiatimes.com/markets/ipos/fpos/stitched-textiles-files-for-rs-200-crore-ipo-with-sebi/articleshow/92355212.cms', 'https://economictimes.indiatimes.com/markets/ipos/fpos/india-exposition-mart-gets-sebis-go-ahead-to-float-ipo/articleshow/92342939.cms', 'https://economictimes.indiatimes.com/markets/ipos/fpos/inox-green-energy-services-files-fresh-draft-papers-with-sebi-for-rs-740-cr-ipo/articleshow/92336392.cms']\n",
      "prnnewswire\n",
      "Arab Finanace\n",
      "Vccircle\n",
      "Allafrica\n",
      "zawya\n",
      "['https://www.zawya.com/en/projects/bri/china-construction-bank-singapores-addx-to-partner-on-china-offshore-investments-t3b88e3p', 'https://www.zawya.com/en/wealth/wealth-management/investcorp-appoints-md-for-private-wealth-in-saudi-arabia-yha3y50r', 'https://www.zawya.com/en/world/china-and-asia-pacific/beijing-gives-initial-nod-to-revive-ant-ipo-after-crackdown-cools-sources-b1izo76u', 'https://www.zawya.com/en/press-release/companies-news/addx-is-first-singapore-financial-institution-to-recognise-crypto-assets-of-accredited-investors-xcoejn3p', 'https://www.zawya.com/en/press-release/companies-news/addx-raises-us58mln-ijglok0v', 'https://www.zawya.com/en/markets/equities/indias-lic-shares-set-to-slide-in-market-debut-after-record-ipo-udct9yyt', 'https://www.zawya.com/en/opinion/markets-insights/how-wall-street-banks-made-a-killing-on-spac-craze-f449d3vq', 'https://www.zawya.com/en/markets/equities/retal-plans-ipo-and-listing-shares-on-saudi-stock-exchange-u2qbhjdz', 'https://www.zawya.com/en/press-release/companies-news/retal-urban-development-company-announcement-of-intention-to-float-on-the-saudi-stock-exchange-fmrhh8lb', 'https://www.zawya.com/en/press-release/people-in-the-news/nutanix-appoints-rukmini-sivaraman-as-chief-financial-officer-lh2qwav1', 'https://www.zawya.com/en/opinion/business-insights/how-covid-propelled-trillion-dollar-valuations-k1ofrumy', 'https://www.zawya.com/en/economy/abu-dhabis-economy-going-from-strength-to-strength-official-says-w6ciutl7', 'https://www.zawya.com/en/press-release/research-and-studies/iridium-study-finds-that-less-than-1-3-of-ipos-in-the-gcc-succeed-rdfzoenf', 'https://www.zawya.com/en/press-release/research-and-studies/iridium-study-finds-that-less-than-1-3-of-ipos-in-the-gcc-succeed-kya2lr0x', 'https://www.zawya.com/en/wealth/funds/oman-india-fund-to-invest-nearly-10mln-in-senco-gold-kiokkkz4', 'https://www.zawya.com/en/business/professional-services/millennial-secures-35mln-capital-commitment-from-us-group-f3rtc15h', 'https://www.zawya.com/en/press-release/companies-news/millennial-brands-secures-usd35mln-capital-commitment-jpugvl8w', 'https://www.zawya.com/en/press-release/companies-news/markaz-oil-crosses-120-b-and-propels-gcc-markets-even-higher-eth29nas', 'https://www.zawya.com/en/press-release/companies-news/saudi-corporates-funding-mix-is-evolving-nspzfslj', 'https://www.zawya.com/en/markets/equities/dubai-dewas-dividends-will-be-consistent-on-strong-cash-flows-g7oh73ys', 'https://www.zawya.com/en/press-release/companies-news/markaz-2021-marks-a-year-of-achievements-despite-challenges-driven-by-expertise-innovation-and-ongoing-progress-umpk9ht4', 'https://www.zawya.com/en/wealth/funds/kia-seeks-to-up-investments-capital-markets-targeted-hfoidhtj', 'https://www.zawya.com/en/markets/equities/superyacht-maker-ferretti-presses-ahead-with-1bln-hk-listing-despite-choppy-markets-sources-wjxhslko', 'https://www.zawya.com/en/press-release/companies-news/sir-anthony-ritossas-18th-gfois-welcomes-unicorn-hunters-the-creators-of-unicoin-ph62ls2s', 'https://www.zawya.com/en/press-release/events-and-conferences/sir-anthony-ritossas-18th-global-family-office-investment-summit-welcomes-unicorn-hunters-jh3r52b1']\n",
      "Aljarida\n",
      "['https://www.aljarida.com/articles/1655901257451709500/', 'https://www.aljarida.com/articles/1655923392892692400/', 'https://www.aljarida.com/articles/1655917924212482000/', 'https://www.aljarida.com/articles/1655920472252543900/', 'https://www.aljarida.com/articles/1655972312663603900/', 'https://www.aljarida.com/articles/1655965664413472000/', 'https://www.aljarida.com/articles/1655964913743470000/', 'https://www.aljarida.com/articles/1655922299272621800/', 'https://www.aljarida.com/articles/1655972677333606600/', 'https://www.aljarida.com/articles/1655922910962660300/', 'https://www.aljarida.com/articles/1655922844472655600/', 'https://www.aljarida.com/articles/1655923272582684100/', 'https://www.aljarida.com/articles/1655972565523604800/', 'https://www.aljarida.com/articles/1655967997853592900/', 'https://www.aljarida.com/articles/1655918421602491800/', 'https://www.aljarida.com/articles/1655912693172186500/', 'https://www.aljarida.com/articles/1655970915683600700/', 'https://www.aljarida.com/articles/1655969354463597300/', 'https://www.aljarida.com/articles/1655968894313593800/', 'https://www.aljarida.com/articles/1655967587493473800/', 'https://www.aljarida.com/articles/1655971881823601800/', 'https://www.aljarida.com/articles/1655970337073598400/', 'https://www.aljarida.com/articles/1655913348442192200/', 'https://www.aljarida.com/articles/1655913194132191200/', 'https://www.aljarida.com/articles/1655970604863600100/', 'https://www.aljarida.com/articles/1655919105642509100/', 'https://www.aljarida.com/articles/1655919320712516600/', 'https://www.aljarida.com/articles/1655916132762235200/', 'https://www.aljarida.com/articles/1655920460472543500/', 'https://www.aljarida.com/articles/1655920524792544900/', 'https://www.aljarida.com/articles/1655915814402231100/', 'https://www.aljarida.com/articles/1655915889352231700/', 'https://www.aljarida.com/articles/1643208767910217000/', 'https://www.aljarida.com/articles/1632757382053976300/', 'https://www.aljarida.com/articles/1621263866321751100/', 'https://www.aljarida.com/articles/1619707912396232300/', 'https://www.aljarida.com/articles/1616517361283975900/', 'https://www.aljarida.com/articles/1611593100831065500/', 'https://www.aljarida.com/articles/1599582428072951000/', 'https://www.aljarida.com/articles/1589289936299863000/', 'https://www.aljarida.com/articles/1578925320746051600/', 'https://www.aljarida.com/articles/1578935072036835200/', 'https://www.aljarida.com/articles/1655972677333606600/', 'https://www.aljarida.com/articles/1655972565523604800/', 'https://www.aljarida.com/articles/1655972312663603900/', 'https://www.aljarida.com/articles/1655970915683600700/', 'https://www.aljarida.com/articles/1655970604863600100/', 'https://www.aljarida.com/articles/1655969354463597300/', 'https://www.aljarida.com/articles/1655968894313593800/', 'https://www.aljarida.com/articles/1655967997853592900/', 'https://www.aljarida.com/articles/1655967587493473800/', 'https://www.aljarida.com/articles/1655966824783472900/', 'https://www.aljarida.com/articles/1655965664413472000/', 'https://www.aljarida.com/articles/1655964913743470000/', 'https://www.aljarida.com/articles/1655920460472543500/', 'https://www.aljarida.com/articles/1655920524792544900/', 'https://www.aljarida.com/articles/1655901257451709500/', 'https://www.aljarida.com/articles/1655923392892692400/', 'https://www.aljarida.com/articles/1655917924212482000/', 'https://www.aljarida.com/articles/1655920472252543900/', 'https://www.aljarida.com/articles/1655919105642509100/', 'https://www.aljarida.com/articles/1655920882362549900/', 'https://www.aljarida.comhttps://storage.googleapis.com/jarida-cdn/pdfs/1655924106852731400/1655924151000/file.pdf', 'https://www.aljarida.com/articles/1655901257451709500/', 'https://www.aljarida.com/articles/1655923392892692400/', 'https://www.aljarida.com/articles/1655922299272621800/', 'https://www.aljarida.com/articles/1655922374642623800/', 'https://www.aljarida.com/articles/1655917924212482000/', 'https://www.aljarida.com/articles/1655835119984232600/', 'https://www.aljarida.com/articles/1655894955570449600/', 'https://www.aljarida.com/articles/1655733556838921300/', 'https://www.aljarida.com/articles/1655796138912157000/', 'https://www.aljarida.com/articles/1655747830189729600/', 'https://www.aljarida.com/articles/1654363527739264700/', 'https://www.aljarida.com/articles/1654784521384207800/', 'https://www.aljarida.com/articles/1654084996554750000/', 'https://www.aljarida.com/articles/1654364103509289300/', 'https://www.aljarida.com/articles/1654363594819267900/', 'https://www.aljarida.com/articles/1655657427875755000/', 'https://www.aljarida.com/articles/1654451968047897800/', 'https://www.aljarida.com/articles/1654447658116648100/', 'https://www.aljarida.com/articles/1654076963353447900/', 'https://www.aljarida.com/articles/1655919105642509100/', 'https://www.aljarida.com/articles/1655901257451709500/', 'https://www.aljarida.com/articles/1655915889352231700/', 'https://www.aljarida.com/articles/1655922299272621800/', 'https://www.aljarida.com/articles/1655919248372514100/', 'https://www.aljarida.com/articles/1655835119984232600/', 'https://www.aljarida.com/articles/1655913794012201000/', 'https://www.aljarida.com/articles/1655835208584233600/', 'https://www.aljarida.com/articles/1655747830189729600/', 'https://www.aljarida.com/articles/1655919105642509100/', 'https://www.aljarida.com/articles/1654363527739264700/', 'https://www.aljarida.com/articles/1655835119984232600/', 'https://www.aljarida.com/articles/1655654913435692400/', 'https://www.aljarida.com/articles/1654182798303922900/', 'https://www.aljarida.com/articles/1654448297966707200/', 'https://www.aljarida.com/articles/1655920589512546900/']\n",
      "Did not pass FilterFunction\n",
      "Dziennik\n",
      "Swissinfo\n",
      "IPO EinNews\n",
      "EINnews not working\n",
      "Sabah\n",
      "Livemint India\n",
      "livemint: err: Empty dataframe\n",
      "Livemint not working\n",
      "Bahamas\n",
      "Azernews\n",
      "['https://www.azernews.az/region/158329.html', 'https://www.azernews.az/region/157895.html', 'https://www.azernews.az/region/193841.html', 'https://www.azernews.az/oil_and_gas/159169.html', 'https://www.azernews.az/region/155527.html', 'https://www.azernews.az/region/158089.html', 'https://www.azernews.az/region/158106.html', 'https://www.azernews.az/oil_and_gas/151598.html', 'https://www.azernews.az/region/154768.html', 'https://www.azernews.az/region/150410.html', 'https://www.azernews.az/region/159338.html', 'https://www.azernews.az/business/150521.html']\n",
      "chosenilboenglish\n",
      "['http://english.chosun.com/site/data/html_dir/2021/04/15/2021041500417.html', 'http://english.chosun.com/site/data/html_dir/2021/07/09/2021070900458.html', 'http://english.chosun.com/site/data/html_dir/2021/03/11/2021031101655.html', 'http://english.chosun.com/site/data/html_dir/2021/11/12/2021111200575.html', 'http://english.chosun.com/site/data/html_dir/2021/03/19/2021031901360.html', 'http://english.chosun.com/site/data/html_dir/2021/03/15/2021031501373.html', 'http://english.chosun.com/site/data/html_dir/2020/09/22/2020092200559.html', 'http://english.chosun.com/site/data/html_dir/2021/12/04/2021120400575.html', 'http://english.chosun.com/site/data/html_dir/2021/07/14/2021071401132.html']\n",
      "Brazil\n",
      "Otempo brazil not working\n",
      "Chile\n",
      "lastampa Italy\n",
      "Liputan Indo\n",
      "Mexico\n",
      "Mexico not working\n",
      "New Zealand\n",
      "supchina\n",
      "Romania Insider Dominician Republic\n",
      "Aquila (ticker: AQ), one of the biggest distributors of fast-moving consumer goods (FMCG) in Romania has raised RON 367 mln (EUR 74 mln) in its initial public offering (IPO) on the Bucharest Stock Exchange. This is the largest IPO competed by a private company in Romania in the last four years.The final IPO price was set at RON 5.5 per share, at the bottom end of the interval targeted by the company. The demand from retail investors was lower than in the previous IPOs carried out on the Bucharest Stock Exchange this year. Still, the tranche allotted to individual investors, which represented 15% of the total number of shares on sale, was 100% underwritten.âAquila completed a capital increase of RON 367 million (equivalent to EUR 74 million) through an initial public offering, this being the largest primary public offer made by a company with private shareholding through the Bucharest Stock Exchange. The successful completion of the Public Offer and the trust received from investors give us the opportunity to continue the path of expanding and consolidating the Aquila business to another level,â said Jean Dumitrescu, Investor Relations Director, Aquila.The new shares sold in the IPO represent a third of the companyâs total number of shares after the transaction. Aquila was thus valued at EUR 222 mln, which is 18.3 times more than the net profit reported for 2020 - EUR 12.1 mln.Local brokerage firm Swiss Capital managed the IPO and BRK Financial Group and Tradeville were distribution agents. Local law firm Schoenherr si Asociatii provided the legal assistance for the IPO.editor@romania-insider.com(Photo source: Aquila Facebook page)\n",
      "Romaniaâs biggest naval transport group â Transport Trade Services (TTS) â will carry out an initial public offering (IPO) through which its current shareholders will sell shares worth up to RON 315 mln (EUR 64 mln).The company has set a price range of RON 19-21 for its shares in the IPO. The offer will take place between May 24 and June 4 and will have two tranches: one for institutional investors and one for retail investors.Local brokerage firm Swiss Capital will manage the IPO, with Tradeville and BRK Financial Group part of the distribution group.The group's shareholders will sell half of their holdings, adding up to 15 million shares or 50% of the company's total equity. The targeted valuation for the whole company is thus RON 630 mln (EUR 128 mln), at the top of the IPO price range.TTS group recorded consolidated revenues of RON 520 mln (EUR 107 mln), down 5.7% compared to 2019, while its net profit was RON 48 mln (almost EUR 10 mln), down 9.4% year-on-year.The group's majority shareholder is local businessman Mircea Mihailescu, who owns a 50.4% stake. Notably, the third-biggest shareholder is former finance minister Viorel Stefan, who holds a 10.1% stake.The IPO prospectus is available here.editor@romania-insider.com\n",
      "The shares of Romanian developer One United (ONE) have lost 9% since their listing on the Bucharest Stock Exchange as some investors were disappointed with their weak debut.On July 12, the companyâs shares started trading on BVB at RON 2.14, up 7% compared to the IPO price of RON 2 but closed the first trading day just under RON 2. After that, the share price gradually went down, culminating with a 5.4% drop on Monday, July 19, amid turbulence in international equity markets.One United closed the trading session on Wednesday, July 21, at RON 1.815, down 9% compared to the IPO price.The negative price evolution prompted brokerage firm BRK Financial Group to step in and try to stabilize the price through purchases. The IPOâs intermediary can buy up to 7.5 million shares in the first 30 days after listing to counter negative price evolutions. So far, it has bought some 450,000 shares representing about 15% of the trading volume in the respective days.One United, one of the fastest-growing real estate developers in Romania, raised RON 260 million (EUR 53 mln) in its initial public offering, which ended on July 2 at a pre-money valuation of EUR 530 mln, or a post-IPO capitalization of over EUR 580 mln.The company reported consolidated revenues of RON 542 mln (EUR 111 mln) and a net profit of RON 178 mln (EUR 36.5 mln) in 2020.editor@romania-insider.com(Photo source: BVB.ro)\n",
      "Aquila, one of the biggest distributors of fast-moving consumer goods (FMCG) in Romania, aims to raise up to RON 433 mln (EUR 87.5 million) through an initial public offering (IPO) on the Bucharest Stock Exchange.This is the third major IPO of a private company on the Bucharest Stock Exchangeâs main market this year, after those carried out by local transport group TTS and real estate develop One United.Aquila plans to use most of the money raised in the IPO (between 50% and 70%) to buy other FMCG companies while the rest of the funds will go into digitization projects, developing the companyâs brands and working capital.The company will sell 66.66 million shares representing a third of its total number of shares after the IPO, targeting a post-money valuation of EUR 263 mln. The pre-monet valuation of EUR 175 mln stands for a multiple of about 14.5 times the net profit reported for 2020, which was approximately RON 60 mln (EUR 12.1 mln).Aquila recorded revenues of RON 1.79 bln (EUR 362 mln) in 2020, up 6% compared with 2019, despite a 25% drop in the volume of goods distributed, especially to HORECA companies, because of the pandemic. However, the average price of the distributed goods increased by 27% due to the lower share of cheaper products distributed to HORECA, the company said in its IPO prospectus.With a history of 26 years, Aquila is one of the biggest FMCG distributors in Romania, serving a network of about 67,000 points of sale, which includes traditional retailers, modern retailers and HORECA companies. Its top partners include Unilever, Mars, Ferrero, Philips, Essity, Coca-Cola, Lavazza and ETI Romania. The companyâs founders are Romanian entrepreneurs Constantin-Cataliu Vasile and Alin-Adrian Dociu, who currently hold 50% of its shares each. After the IPO, they will hold stakes of 33.3% of the company.The public offer starts on Monday, November 8 and ends on November 16. The IPO will have two tranches, one dedicated to institutional investors, which will represent 85% of the shares on sale, and one dedicated to retail investors, representing 15% of the shares on sale. The retail tranche will also be split into two sub-tranches: one with guaranteed allocation (investors get the exact number of shares underwritten) on a first-come-first-served basis and the other one with pro-rata allocation. The minimum subscription for retail investors is 750 shares or RON 4,875 (almost EUR 1,000) while the maximum is 12,000 shares.Local brokerage firm Swiss Capital manages the IPO and BRK Financial Group and Tradeville are distribution agents.editor@romania-insider.com(Photo source: the company's Facebook page)\n",
      "Romanian software company Bitdefender, a global leader in the cyber security solutions market, officially confirmed that it had started procedures for future listing in the US.\"Bitdefender Holding BV has confidentially filed a draft Form F-1 registration statement with the US Capital Markets Authority (SEC) for a future initial public offering (IPO) in the United States,\" the company announced.The timing, size and price for the future offer are to be determined. The initial public offering is subject to evaluations by the SEC and other regulatory processes, as well as the market and other conditions.Bitdefender hired investment banks JP Morgan and Morgan Stanley to help it with the launch of an IPO on the US market, local media announced over the weekend.The IPO would envisage a valuation of nearly USD 2 bln for the Romanian company, according to sources familiar with the deal quoted by Ziarul Financiar daily.(Photo: LCVA | Dreamstime.com)andrei@romania-insider.com\n",
      "Romaniaâs energy minister Razvan Nicolescu had a meeting with representatives of the European Bank for Reconstruction and Development (EBRD) in London, on Tuesday, June 18, to talk about EBRDâs participation to Electricaâs IPO and the possibility to become a long term investor in the company.âWe expressed our wish and trust that EBRD will be a long term partner for the Romania state in Electrica and that it will use its vast experience to support implementing international corporate governance standards in the company. We hope this IPO will promote Electrica and, implicitly, Romania as a good business destination, as well as stable and predictable for investments,â Nicolescu said.Romanian state owned electricity distributor Electrica launched, on June 16, an initial public offering (IPO) to sell 177.19 million shares, which make some 51 percent of the companyâs capital. The company aims to get between EUR 440 million and EUR 540 million from the IPO.EBRD might be one of the largest investors in the Electrica IPO. The representatives of the financial institution haveÂ already been involved in the making of the new incorporation document for Electrica, and insisted that some clauses beÂ introduced which will not allow the Romanian state to take important decisions for the company by itself, sources from the capital market told Romania-Insider.com.After the IPO, the Romanian state will remain the companyâs main shareholder, with 49 percent of the voting rights, this is why institutional investors wanted guarantees that the state will not be able to take decisions incongruous with the interests of the other shareholders.EBRD might hold between 5 percent and 10 percent of the company and will have a representative in the companyâs administrative board, the same sources said.The Romanian energy minister and the companyâs management, together with members of the intermediation consortium for the IPO, met with EBRD at the start of their international road-show which is part of the IPO marketing to international institutional investors. The road-show will cover all the major financial centers in Europe as well as New York and Boston in the U.S.The IPO will end on June 25. Citigroup, Societe General, Raiffeisen Bank Romania, BRD and Swiss Capital make the consortium which intermediates the offering.The small tranche for individual investors in Electricaâs IPO was subscribed in just two days.Andrei Chirileasa, andrei@romania-insider.com\n",
      "Romaniaâs electricity distributor Electricaâs initial public offering (IPO) should be fully subscribed by Monday, June 23, two days before it is set to end, energy minister Razvan Nicolescu said.Electrica, which is currently 100 percent state-owned, launched an IPO on Monday, June 16, to sell 177.19 million shares, representing 51 percent of the companyâs capital. The minimum value of the offer is EUR 443 million, as the company sells its shares at a minimum of RON 11 and a maximum of RON 13.5. The shares will then be listed on the Bucharest Stock Exchange (BVB).A part of the offer will be structured in global depository receipts (GDRs) which are financial instruments to be traded on the London Stock Exchange. One GDR stands for four Electrica shares and the price for GDRs is between USD 13.55 and USD 16.63.âElectricaâs IPO is going well. I expect that it will be fully subscribed by Monday, June 23, at the latest,â Nicolescu said.He went in an international roadshow to promote the IPO along with the companyâs management and the brokers of the offering. They first met with representatives of the European Reconstruction and Development Bank (EBRD) in London, on Tuesday, to talk about EBRDâs participation in the IPO as a long term investor in Electrica.âElectrica is a company with good perspectives and thatâs why its prospectus is worth analyzing. We wish it to become a model for Romania on corporate governance. The agreement with EBRD on this matter will help the company a lot,â Nicolescu mentioned.According to the prospectus, 85 percent of the shares in the IPO are allotted for institutional investors and 15 percent for individual investors, which have two tranches, one for small subscriptions, of up to 20,000 shares, which is 7 percent of the total number of shares, and one for large subscriptions of more than 20,000 shares, which stands for 8 percent. The final allotment may change depending on the level of subscriptions on each tranche.The small tranche has already been oversubscribed by 76 percent. On Thursday, June 19, at 17:00, there were more than 10,500 orders registered on this tranche, for a total of 21.87 million shares. Some 7 million shares were subscriptions smaller than 1,000 shares, which get guaranteed allocation (one share allotted for each share subscribed), while some 14.8 million shares were subscriptions larger than 1,000 shares, which get a 5 percent discount on the final IPO price.On the tranche for large subscriptions, there were 68 orders for almost 4 million shares, which is about 28 percent of the number of shares dedicated to this tranche.The total value of subscriptions from individual investors is EUR 67 million.Andrei Chirileasa, andrei@romania-insider.com\n",
      "UiPath, the biggest robotic processing automation (RPA) company in the world, launched by two Romanian entrepreneurs in Bucharest in 2005, has set a price of USD 56 per share for its initial public offering (IPO), according to an official announcement.The total number of shares sold in the IPO is 23.89 million, which places the total value of the deal at over USD 1.33 bln. The companyâs valuation was thus set at USD 29 bln. UiPathâs shares will start trading on the New York Stock Exchange (NYSE) Today (April 21) under the ticket PATH.The company will sell 9.41 mln new shares in the IPO, for which it will get USD 527 mln. Existing shareholders, including co-founder and CEO Daniel Dines, will sell another 14.47 mln shares worth USD 810 mln. In addition, UiPath has granted the underwriters a 30-day option to purchase up to 3.58 million additional shares of Class A common stock at the IPO price.Daniel Dines will sell about 1.38 million UiPath shares in the IPO, for which he will get USD 77 mln. After the IPO, he will still hold 109.5 mln UiPath class A and Class B shares, worth USD 6.1 bln (at the IPO price). Dines thus officially becomes the richest Romanian. Currently, former tennis player Ion Tiriac (81) is the only Romanian in the Forbes list of the world's billionaires, with a fortune of USD 1.7 bln.UiPath filed the documentation for its NYSE IPO at the end of March. In mid-April, the company announced a price range of USD 43-50 per share for its IPO, which implied a valuation of up to USD 26 bln. A few days later, the company increased the price range to USD 52-54 per share and the number of shares it planned to sell in the IPO from 6.8 mln to 9.4 mln.The company was most recently valued at USD 35 billion in the latest funding round earlier this year. In the financial year ended in January 2021, UiPath recorded annualized recurring revenues (ARR) of USD 580 mln, up by 65% compared to the previous year, and a net loss of USD 92 mln, down from USD 520 mln in the previous year.Morgan Stanley and J.P. Morgan are the lead bookrunners for UiPath's IPO. BofA Securities, Credit Suisse, Barclays, and Wells Fargo Securities are active bookrunners while SMBC Nikko, BMO Capital Markets, Mizuho Securities, KeyBanc Capital Markets, TD Securities, Truist Securities, Cowen, Evercore ISI, Macquarie Capital, Nomura, and RBC Capital Markets are passive bookrunners. Canaccord Genuity, D.A. Davidson & Co., Oppenheimer & Co., and Needham & Company are co-managers for the offering.editor@romania-insider.com(Photo source: UiPath Facebook page)\n",
      "Small individual investors who will subscribe on the small tranche of Romanian electricity distributor Electrica initial public offering (IPO) will have the option to choose between having their orders guaranteed up to 1,000 shares or getting a 5 percent discount to the final IPO price, according to the prospectus. The procedure, however, is not the easiest.The tranche for small investors in the Electrica IPO is 7 percent of the total number of shares sold by the company. That is 12.4 million shares out of a total of 177.19 million shares. The price interval for the IPO is RON 11 to 13.5 per share.Subscriptions for the small retail tranche start at a minimum of 250 shares and go up to 20,000 shares. When making the subscription, investors must pay the maximum price per share of RON 13.5. At the end of the IPO period, which is June 16-25, the price of the IPO will be set based on subscriptions made by institutional investors, who get 85 percent of the total shares.Retail investors will pay the same price as institutional investors. For example, if this price is RON 12 per share, small investors will get back the difference of RON 1.5 per share.Small investors who make their subscriptions in the first five days of the offering period (up until and including June 20) will get a 5 percent discount to the final price. Again, as an example, if the price is RON 12, then the discounted price would be RON 11.4. However, investors who choose to have their subscriptions guaranteed do not get the discount.Investors can choose to have their subscriptions guaranteed up to 1,000 shares. In this case, the advantage is that they know from the start how many shares they will get in the IPO, so the allocation ratio is one on one. A maximum of 10 million shares will be set aside for guaranteed subscriptions, with a first come, first served policy.Itâs uncertain if an investor who wants to subscribe between 250 and 1,000 shares can also choose not to have his subscriptions guaranteed, in order to get the discount. However, in this case he could get significantly less shares than the number he applies for, because the allocation will be pro-rata. So if the non-guaranteed portion of the tranche is five time oversubscribed for example, he will get only 20 percent of the shares he subscribed.Small investors who want to subscribe more than 1,000 shares also have more options. For example, someone who wants to subscribe 10,000 shares can make two orders, one for 1,000 shares with guaranteed subscription, and one for 9,000 shares non-guaranteed, for which he gets the 5 percent discount. He can also go with a single order for 10,000 shares non-guaranteed.Itâs not the easiest subscription procedure and investors will have to be very careful how they structure their orders in order to get the expected results. Assistance from a broker would prove useful, but even among brokers there are different interpretations given to the prospect. Hopefully all these will be clarified by Monday, June 16, when the offer starts.Investors in Romania can subscribe in the Electrica IPO through BRD, Raiffeisen Bank and brokerage firm Swiss Capital, which are members of the intermediary consortium. But most local brokerage firms can also receive subscriptions from investors which they pass forward to the syndicate.More details in the IPO prospectus.Andrei Chirileasa, andrei@romania-insider.comÂ\n",
      "The price for which Romanian electricity distributor Electrica will sell 51 percent of its own shares both on the Bucharest Stock Exchange (BVB) and the London Stock Exchange (LSE) will be announced tomorrow (Wednesday, June 11), following approval from the Government, sources from the Romanian capital market told Romania-Insider.com.Electricaâs initial public offering (IPO), which shuld be the largest transaction of this kind ever made on the local capital market, is set to start on Monday, June 16, and will close on June 25, according to statements from Romanian state officials.The company announced its intention to launch an IPO at the end of May. The offer consists of 177 million new shares, representing 105 percent of the companyâs total number of existing shares. They include shares and GDRs (global depository receipts), with one GDR standing for four shares, the company announced.During the last two weeks, the managers of the offering were engaged in the pre-marketing activities for the IPO, which means they contacted potential investors and presented them the company while at the same time asking them how many shares Â they would be interested to buy and at what price. Based on that feedback from investors, they established the price range for the offering.This price range remains to be harmonized with the Governmentâs expectations, which still requires some negotiations. Following these negotiations, which should end today (June 10), the official price range will be presented on Wednesdayâs weekly Government meeting for approval.The offering price cannot, by law, be lower than the sharesâ face value, which is RON 10, which means that the offering will for sure be larger than RON 1.77 billion (EUR 400 million). This will make it larger than the Romgaz IPO last year, in which the Romania state sold 15 percent of the company for EUR 390 million.After it gets the Governmentâs approval, the price will be filled up in the IPO prospectus, which should then be approved by the Romanian Financial Supervision Authority ASF. After this step, the prospectus should be made public, either on Wednesday evening or on Thursday this week. The prospectus will be available on the Electrica website, on the Bucharest Stock Exchange website, but on sites of the intermediaries as well.The consortium which manages the Electrica IPO consists of Citigroup Global Markets Limited, Raiffeisen Bank Romania and SociÃ©tÃ© GÃ©nÃ©rale Corporate & Investment Banking, who are joint global coordinators and joint bookrunners for the offer, BRD-Groupe SociÃ©tÃ© GÃ©nÃ©rale (Romania) as manager, and local brokerage firm Swiss Capital, as distribution agent.Andrei Chirileasa, andrei@romania-insider.com\n",
      "CNBC Barbados\n",
      "AIF Russia\n",
      "Monitor uganda\n",
      "the sun UK\n",
      "Parool Netherlands\n",
      "Netherlands is not working\n",
      "Oman shabiba\n",
      "Koreannewsgazette\n",
      "['https://www.koreanewsgazette.com/one-store-scraps-ipo-plan-due-to-weak-investor-confidence-sources/', 'https://www.koreanewsgazette.com/sk-shieldus-eyes-raising-up-to-3-55-tln-won-through-ipo-next-month/', 'https://www.koreanewsgazette.com/hyundai-mipo-wins-94-bln-won-product-carrier-order-in-asia/', 'https://www.koreanewsgazette.com/sk-ons-ipo-unlikely-to-take-place-until-after-2025-sk-innovation-ceo/', 'https://www.koreanewsgazette.com/money-raised-through-ipos-hits-record-high-in-2021-amid-ample-liquidity-bullish-sentiment/', 'https://www.koreanewsgazette.com/hyundai-mipo-dockyard-remains-in-red-in-2021/', 'https://www.koreanewsgazette.com/hyundai-mipo-dockyard-remains-in-red-in-q4/', 'https://www.koreanewsgazette.com/lg-to-unveil-life-size-omnipod-self-driving-concept-car/']\n",
      "Hungary\n",
      "Hungary not working\n",
      "jauns\n",
      "['https://jauns.lv/raksts/bizness/504222-indexo-latvijai-ir-nepieciesama-nozimiga-vieteja-kapitala-banka', 'https://jauns.lv/raksts/bizness/486641-airbaltic-cer-sasniegt-miljarda-eiro-apgrozijumu-2026gada', 'https://jauns.lv/raksts/bizness/485953-indexo-birza-plano-piesaistit-5-6-miljonus-eiro', 'https://jauns.lv/raksts/bizness/484618-indexo-plano-sakt-akciju-kotaciju-birza-nasdaq-riga-un-dibinat-banku', 'https://jauns.lv/raksts/bizness/478234-eiropas-komisija-atlavusi-latvijai-airbaltic-pamatkapitala-investet-45-miljonus-eiro', 'https://jauns.lv/raksts/zinas/460884-gatis-kokins-politikas-un-baleta-nebus-bet-nauda-bus-visiem', 'https://jauns.lv/raksts/bizness/434979-delfingroup-plano-akciju-sakotnejo-publisko-piedavajumu-nasdaq-riga', 'https://jauns.lv/raksts/bizness/432598-gauss-ietekme-uz-aviacijas-nozari-ir-pratam-neaptverama', 'https://jauns.lv/raksts/sports/53686-speka-sportistam-kindzulim-otra-vieta-prestizas-sacensibas-vacija', 'https://jauns.lv/raksts/bizness/442100-par-nebanku-kreditetaja-delfingroup-netieso-lidzipasnieku-kluvusi-kesenfeldu-gimene', 'https://jauns.lv/raksts/bizness/142281-facebook-sasniedz-vienu-miljardu-lietotaju', 'https://jauns.lv/raksts/bizness/44815-vai-tviterim-pedejie-ciepstieni-jauni-lietotaji-klat-nenak-akciju-cena-rekordzema', 'https://jauns.lv/raksts/zinas/50812-rigas-valsts-1gimnazijas-skolniece-darta-sanem-bronzu-starptautiskaja-filozofijas-olimpiade', 'https://jauns.lv/raksts/bizness/459308-kina-plano-liegt-tehnologiju-uznemumiem-kotet-akcijas-arvalstis', 'https://jauns.lv/raksts/bizness/362346-saudi-aramco-klust-par-pasaule-lielako-birza-koteto-uznemumu']\n",
      "Pulse\n",
      "Pulse not working\n",
      "vnexpress\n",
      "['https://vnexpress.net/ipo-la-gi-4459152.html', 'https://vnexpress.net/ttg-holdlings-ki-vong-ipo-trong-nam-2025-4472635.html', 'https://vnexpress.net/chu-tich-aura-capital-chia-se-kinh-nghiem-chuan-bi-ipo-cho-smes-4472648.html', 'https://vnexpress.net/nova-consumer-cong-bo-gia-co-phan-ipo-4435110.html', 'https://startup.vnexpress.net/tin-tuc/hanh-trinh-khoi-nghiep/bukalapak-chuan-bi-ipo-4307403.html', 'https://vnexpress.net/tp-hcm-dao-tao-500-startup-huong-toi-ipo-4400998.html', 'https://vnexpress.net/vng-du-kien-ipo-tai-my-4340110.html', 'https://vnexpress.net/airbnb-nop-don-ipo-4193122.html', 'https://vnexpress.net/cach-startup-dau-tien-cua-viet-nam-ipo-tai-nhat-ban-4420207.html', 'https://vnexpress.net/doanh-nghiep-chay-dua-ipo-ky-luc-4300883.html', 'https://startup.vnexpress.net/tin-tuc/xu-huong/lalamove-nop-ho-so-ipo-tai-my-4299931.html', 'https://vnexpress.net/airbnb-nop-don-ipo-4149389.html', 'https://vnexpress.net/cong-ty-cong-nghe-viet-dau-tien-ipo-tai-nhat-ban-4407266.html', 'https://vnexpress.net/doanh-nghiep-trung-quoc-bi-cam-ipo-tai-my-4333411.html', 'https://vnexpress.net/didi-uber-trung-quoc-nop-don-ipo-tai-my-4292453.html', 'https://startup.vnexpress.net/tin-tuc/xu-huong/ipo-se-khai-sinh-them-nhieu-startup-dong-nam-a-4292261.html', 'https://startup.vnexpress.net/tin-tuc/hanh-trinh-khoi-nghiep/nhung-thach-thuc-grab-phai-doi-mat-sau-ipo-4265891.html', 'https://vnexpress.net/uber-trung-quoc-bi-mat-nop-don-ipo-tai-my-4261189.html', 'https://vnexpress.net/ong-trinh-van-quyet-muon-ipo-bamboo-airways-tai-my-4263069.html', 'https://vnexpress.net/cong-ty-cua-jessica-alba-nop-don-ipo-4260907.html', 'https://startup.vnexpress.net/tin-tuc/xu-huong/ky-lan-giao-duc-cua-my-chuan-bi-ipo-4258872.html', 'https://vnexpress.net/trung-quoc-se-cam-cac-cong-ty-co-du-lieu-quan-trong-ipo-o-nuoc-ngoai-4347924.html', 'https://vnexpress.net/lei-jun-lan-dau-tiet-lo-ngay-ipo-tham-hai-cua-xiaomi-4340355.html', 'https://startup.vnexpress.net/tin-tuc/hanh-trinh-khoi-nghiep/ipo-con-dao-2-luoi-cho-cac-ky-lan-4237294.html']\n",
      "vnexpress not working\n",
      "jamaicaobserver\n",
      "independent\n",
      "Albania\n",
      "albaniandailynews: err: Failed to find title in page. Link: https://albaniandailynews.com/news/dp-denounces-govt-for-rewarding-incriminated-people-\n",
      "albaniandailynews: err: Failed to find title in page. Link: https://albaniandailynews.com/news/second-member-of-gjoka-family-members-passes-away-after-blast-1\n",
      "EWN\n",
      "err: Failed to find date in page. Link: https://ewnews.com/i-was-just-trying-to-get-him-help-mother-of-inmate-found-dead-says-mentally-ill-son-was-supposed-to-go-to-sandilands\n",
      "Bloombergquint\n",
      "https://www.bloombergquint.com/bq-prime-videos/ipo-adda-ethos-on-its-listing-plans\n",
      "https://www.bloombergquint.com/business/ipos-sebi-mulls-option-for-pre-filing-of-offer-documents\n",
      "https://www.bloombergquint.com/author/27956/pti\n",
      "https://www.bloombergquint.com/pti/5g-spectrum-auction-dot-invites-players-for-pre-bid-conference-on-june-20\n",
      "https://www.bloombergquint.com/author/27956/pti\n",
      "https://www.bloombergquint.com/bq-prime-videos/ipo-adda-ethos-on-its-listing-plans\n",
      "Bloomberg Quint : err: None type object found for https://www.bloombergquint.com/bq-prime-videos/ipo-adda-ethos-on-its-listing-plans\n",
      "Bloomberg Quint : err: None type object found for https://www.bloombergquint.com/author/27956/pti\n",
      "Bloomberg Quint : err: None type object found for https://www.bloombergquint.com/author/27956/pti\n",
      "Bloomberg Quint : err: None type object found for https://www.bloombergquint.com/bq-prime-videos/ipo-adda-ethos-on-its-listing-plans\n",
      "ECNS\n",
      "{'title': ['Study finds coffee linked to lower risk of death', 'China Life: Local official promoting Sichuan tourism goes viral on internet', \"UK paper's false report on WHO chief's remarks again proves 'lab leak' theory is anti-China politicization: FM\", \"Highlights of Hong Kong's development achievements since its return to motherland\", 'Ethnic dragon canoe festival celebrated in Guizhou', 'Texas official calls police response to Uvalde shooting a \"failure\": NBC', \"192 key fugitives arrested in China's special campaign\", \"China's non-financial outbound direct investment up 2.3 pct in January-May\", 'China launches Yaogan-35 02 remote sensing satellites', 'China launches new batch of remote sensing satellites', \"'Art@Harbor' exhibition kicks off in Hong Kong\", 'Stunning summer desert landscape in Xinjiang', 'At least 1,000 dead in Afghan earthquake', 'Early humans dating back to 600,000-year-old discovered in UK', 'Over 1,700 internet hospitals are established in China', 'Shenzhen ships 630 thousand tons of vegetables to Hong Kong annually', 'Saudi crown prince pays 1st visit to Turkey since death of Khashoggi', 'Vibrant HK to celebrate 25th anniversary of return to motherland', 'Shanghai reviews progress ahead of congress', 'Tourism sector keen for revival in summertime', 'MC12 outcomes raise hopes for WTO reform', \"HK Palace Museum to tell China's story to world audience\", 'Officials recall pressure at Jan 6 hearing', 'Beijing continues to find scattered infections', 'Hong Kong Palace Museum holds opening ceremony', 'Chinese FM meets Indian new ambassador', \"Death toll of Afghanistan's quake surpasses 1,000: official\", \"China's ambassador says Scotland ties mean 'opportunities up for grabs'\", 'Officials: 5th CIIE preparations in full swing', \"Xi urges int'l solidarity to promote growth\", 'Li: Stable, large grain harvests key', \"Leung hails success of 'one country, two systems' in Hong Kong\", 'Political advisers call for further low-carbon transition', 'China looks set to build space solar power station', 'Chinese scientists achieve important breakthrough in silkworm research', \"NASA extends InSight lander's mission on Mars\", 'China issues regulatory protocols for webcast performers', 'Chinese premier stresses work on flood relief, grain harvest', 'U.S. ban on Xinjiang products violates trade rules: Chinese association', 'Chinese envoy calls for lifting arms embargo on CAR', \"U.S. Fed chief says recession is 'certainly a possibility'\", '1 dead, 1 injured in U.S. San Francisco shooting', 'Biden calls on Congress to suspend federal gas tax for 90 days', 'Chinese FM meets outgoing ROK ambassador', 'China to continue measures for flood control, disaster relief', 'Veterans encouraged to work as teachers in China', 'China to continue to enhance opening-up: Xi', 'Xi: China to strive to meet 2022 economic, social targets, minimize COVID-19 impacts', 'Xi calls for promoting sound development of globalization', \"China's first domestically developed anti-HIV drug clinically safe, effective: study\", \"The world's largest 24,000-TEU container ship, built by CSSC, is completed in Shanghai\", \"ASEAN defense ministers vow to strengthen bloc's centrality, unity\", 'Earthquake kills at least 285, injures 600 in eastern Afghanistan', 'Hefei cancels regular mass COVID testing', 'Guangdong to beef up flood prevention measures', 'Landscape of populus euphratica forest before dawn', 'China launches Tianxing-1 test satellite', \"Flower displays set up to celebrate 25th anniversary of Hong Kong's return to motherland\", \"'Insect banquet' held to bid farewell to university\", 'Universal Studios Beijing to reopen on Saturday', \"China opposes U.S. implementation of  'Uyghur Forced Labor Prevention Act'\", 'China, ROK join hands in anti-desertification fight', 'S.Korea reports 1st suspected monkeypox cases', 'U.S. must protect its people from gun violence: spokesperson', 'Quake kills at least 280, injures 595 in eastern Afghanistan', 'Quake kills at least 255, injures 500 in eastern Afghanistan', 'China launches new test satellite', 'S.Korea launches homegrown Nuri space rocket', \"Former HKSAR chief executive hails 'one country, two systems' as viable, vibrant\", 'Twin-city cooperation offers business launchpad for Hong Kong youths', \"Chinese enterprises boost HKSAR's enduring prosperity, stability\", 'Hong Kong begins celebration of 25th anniversary of return to motherland', 'Flooding hits Shaoguan, Guangdong', 'People perform Yoga to greet Summer Solstice in New York', 'Horses gallop on grassland in Xinjiang', \"World's largest freshwater fish found in Cambodia's Mekong River\", '618 shopping spree stimulates pent-up appetites', 'COVID-19 rebound a reality after antivirals', \"Sotheby's goes artfully digital amid COVID-19 era\", 'New education center opens in HK', 'Draft set to strengthen quality, safety of agricultural products', \"Chinese scientists find high-pressure minerals in Chang'e-5 samples for the first time\", 'Chinese researchers discover new bacterial tRNA species', \"Fiscal revenue of China's central gov't exceeds 9 trillion yuan in 2021\", 'Russia to deploy Sarmat ballistic missile system by year end: Putin', 'UN Biodiversity Conference to be relocated from China to Canada', 'BRICS speaks as one voice for shared growth', 'Li stresses key role of grain, energy output', 'Lee pledges utmost efforts to build caring, inclusive HK', 'E-CNY gathers momentum on 618', 'Study suggests reinfection risk of Omicron', 'Suspects in Tangshan assault case linked to other crimes', 'COP15 event relocated, China keeps presidency', 'U.S. act on Xinjiang strongly condemned', 'China honors Shenzhou-13 mission astronauts with medals', 'Turkey to host meeting with UN, Russia, Ukraine on grain corridor: media', 'CEO says China to leapfrog in technology development', 'Chinese envoy warns against antagonism over Ukraine conflict', \"Indonesia's Jakarta-Bandung High-Speed Railway hits new milestone, all tunnels drilled through\", 'U.S. must protect its people from gun violence: spokesperson'], 'link': ['http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5803248.shtml', 'http://www.ecns.cn/photo/2022-06-23/detail-ihazpzfn5803200.shtml', 'http://www.ecns.cn/photo/2022-06-23/detail-ihazpzfn5803107.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5802940.shtml', 'http://www.ecns.cn/photo/2022-06-23/detail-ihazpzfn5802895.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5802770.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5802772.shtml', 'http://www.ecns.cn/news/economy/2022-06-23/detail-ihazpzfn5802633.shtml', 'http://www.ecns.cn/photo/2022-06-23/detail-ihazpzfn5802425.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5802268.shtml', 'http://www.ecns.cn/photo/2022-06-23/detail-ihazpzfn5802173.shtml', 'http://www.ecns.cn/photo/2022-06-23/detail-ihazpzfn5802117.shtml', 'http://www.ecns.cn/photo/2022-06-23/detail-ihazpzfn5802110.shtml', 'http://www.ecns.cn/photo/2022-06-23/detail-ihazpzfn5802103.shtml', 'http://www.ecns.cn/cns-wire/2022-06-23/detail-ihazpzfn5802086.shtml', 'http://www.ecns.cn/cns-wire/2022-06-23/detail-ihazpzfn5802066.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5802040.shtml', 'http://www.ecns.cn/photo/2022-06-23/detail-ihazpzfn5801859.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801845.shtml', 'http://www.ecns.cn/business/2022-06-23/detail-ihazpzfn5801843.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801841.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801847.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801723.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801721.shtml', 'http://www.ecns.cn/news/culture/2022-06-23/detail-ihazpzfn5801655.shtml', 'http://www.ecns.cn/news/politics/2022-06-23/detail-ihazpzfn5801652.shtml', 'http://www.ecns.cn/news/society/2022-06-23/detail-ihazpzfn5801649.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801631.shtml', 'http://www.ecns.cn/business/2022-06-23/detail-ihazpzfn5801625.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801627.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801623.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801629.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801621.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-23/detail-ihazpzfn5801618.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-23/detail-ihazpzfn5801611.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-23/detail-ihazpzfn5801604.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801580.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801578.shtml', 'http://www.ecns.cn/business/2022-06-23/detail-ihazpzfn5801576.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801568.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801562.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801566.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801564.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801570.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801572.shtml', 'http://www.ecns.cn/news/2022-06-23/detail-ihazpzfn5801574.shtml', 'http://www.ecns.cn/news/politics/2022-06-22/detail-ihazpzfn5801063.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5801019.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5800975.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5800907.shtml', 'http://www.ecns.cn/business/2022-06-22/detail-ihazpzfn5800609.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5800585.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5800587.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5800391.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5800393.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5800329.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5800185.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5800152.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5800115.shtml', 'http://www.ecns.cn/cns-wire/2022-06-22/detail-ihazpzfn5800079.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5799803.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5799797.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5799795.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5799780.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5799722.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5799683.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-22/detail-ihazpzfn5799624.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5799607.shtml', 'http://www.ecns.cn/voices/2022-06-22/detail-ihazpzfn5799545.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5799549.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5799547.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5799257.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5799250.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5799243.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5799234.shtml', 'http://www.ecns.cn/photo/2022-06-22/detail-ihazpzfn5798878.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798874.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798876.shtml', 'http://www.ecns.cn/business/2022-06-22/detail-ihazpzfn5798870.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798868.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798872.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-22/detail-ihazpzfn5798685.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-22/detail-ihazpzfn5798678.shtml', 'http://www.ecns.cn/news/economy/2022-06-22/detail-ihazpzfn5798675.shtml', 'http://www.ecns.cn/news/military/2022-06-22/detail-ihazpzfn5798672.shtml', 'http://www.ecns.cn/news/society/2022-06-22/detail-ihazpzfn5798669.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798665.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798661.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798657.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798651.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798655.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798663.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798659.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798653.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-22/detail-ihazpzfn5798642.shtml', 'http://www.ecns.cn/news/politics/2022-06-22/detail-ihazpzfn5798635.shtml', 'http://www.ecns.cn/business/2022-06-22/detail-ihazpzfn5798628.shtml', 'http://www.ecns.cn/news/politics/2022-06-22/detail-ihazpzfn5798621.shtml', 'http://www.ecns.cn/cns-wire/2022-06-22/detail-ihazpzfn5797181.shtml', 'http://www.ecns.cn/news/2022-06-22/detail-ihazpzfn5798595.shtml'], 'publish_date': ['23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022', '22-06-2022'], 'scraped_date': ['23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022', '23-06-2022'], 'text': ['Moderate consumption of coffee, whether unsweetened or sugar-sweetened, was associated with lower risk of death, according to research by a Chinese university.', \"Liu Hong, a local official in Ganzi Tibetan Autonomous prefecture, Southwest China's Sichuan province, has become an internet sensation due to his dedication to tourism promotion on social media.\", 'The COVID-19 \"lab leak\" theory totally twisted facts and it\\'s merely a lie fabricated by anti-China forces for political purposes, Chinese Foreign Ministry spokesperson said on Wednesday in response to a media report that claimed WHO Director-General Tedros Adhanom Ghebreyesus had once acknowledged the theory. ', 'Since its return to the motherland in 1997, Hong Kong has made significant progress in the fields of politics, economy, livelihood, society and culture.', \"Dragon canoes race against each on Qingshui river to celebrate the Dragon Canoe Festival at Shibing county, southwest China's Guizhou Province, June 22, 2022. \", 'A top Texas official said Tuesday that law enforcement\\'s response to the mass shooting at Robb Elementary School in Uvalde, Texas, was an \"abject failure,\" NBC News has reported.', 'Chinese police have caught 192 key fugitives of crimes against the control of the national border and frontier in an ongoing campaign since September 2021, the National Immigration Administration said Thursday.', \"China's non-financial outbound direct investment (ODI) reached 287.06 billion yuan in the first five months of the year, up 2.3 percent year on year, official data showed Thursday.\\n\\n\", \"A Long March-2D rocket carrying the Yaogan-35 02 satellites blasts off from the Xichang Satellite Launch Center in southwest China's Sichuan Province, June 23, 2022.\", \"China successfully launched three new remote sensing satellites from the Xichang Satellite Launch Center in southwest China's Sichuan Province Thursday.\", 'The opening ceremony of the exhibition \"Art@Harbor\" was held at the Central and Western District Promenade (Central Section) on Wednesday. ', \"Aerial view shows beautiful desert landscape with sandy dunes in Hami, northwest China's Xinjiang Uyghur Autonomous Region. \", 'A powerful earthquake has left parts of eastern Afghanistan in ruins, killing at least 1,000 people, and more than 1,500 people injured.', 'Archaeological discoveries made on the outskirts of Canterbury, Kent confirm the presence of early humans in southern Britain between 560,000 and 620,000 years ago, making it one of the earliest known Paleolithic sites in northern Europe.', 'More than 1,700 internet hospitals have been approved and established in China and integrated online and offline medical service has formed basically, said National Health Commission. ', 'Shenzhen Customs on Wednesday said it has inspected 630 thousand tons of vegetables destined for Hong Kong Special Administrative Region (HKSAR) on average annually in the past 25 years since 1997.', \"Saudi Arabia's Crown Prince Mohammed bin Salman Al Saud on Wednesday arrived in Ankara, Turkey's capital, to start his first visit to Turkey since the death of the journalist Jamal Khashoggi in 2018.\", \"This year marks the 25th anniversary of Hong Kong's return to the motherland. \", 'The government released the agenda of the 12th CPC Shanghai Congress that will be held from Saturday till next Monday. The congress is held once every five years.', \"Domestic tourism will embrace a robust recovery in the coming summer as students start their vacations in late June, and eased travel restrictions will help boost the sector's growth, according to industry experts.\", \"The package of trade outcomes at the World Trade Organization's 12th Ministerial Conference－MC12－has added resilience to the multilateral trading system, injecting more confidence into economic globalization, experts said on Wednesday.\", 'Hong Kong Palace Museum, a milestone cross-boundary cultural collaboration and must-see tourist attraction, is poised to play a greater role in exerting the charm of Chinese culture to the four corners of the globe.', \"The House select committee investigating last year's Jan 6 attack on the Capitol heard riveting and emotional testimony from state election officials on Tuesday on how defeated president Donald Trump and his allies pressured them to reverse the 2020 election results.\", 'Beijing has continued to report new COVID-19 cases at the community level in recent days, which has added to the uncertainty during the current complicated epidemic control and prevention situation, a senior official said on Wednesday.', 'The Hong Kong Palace Museum (HKPM), located in the West Kowloon Cultural District of the Hong Kong Special Administrative Region (HKSAR), held its opening ceremony on Wednesday.', \"Chinese State Councilor and Foreign Minister Wang Yi met with Pradeep Kumar Rawat, India's new ambassador to China in Beijing Wednesday.\", 'The death toll from an earthquake that struck eastern Afghanistan early Wednesday has surpassed 1,000, while more than 1,500 people were injured, a provincial official said, adding the number of casualties might rise further.', \"China's top diplomat in the United Kingdom is on a weeklong visit to Scotland to enhance the growing economic, cultural, and educational links between the nations, in a year that marks the 50th anniversary of the establishment of China-UK ambassadorial diplomatic relations\", 'The fifth edition of the China International Import Expo will be held as planned in Shanghai from Nov 5 to 10, and preparations for the mega event are in full swing despite challenges like the recent COVID-19 outbreak, officials said on Wednesday.', 'President Xi Jinping called on Wednesday for international solidarity and coordination to maintain world peace and stability and promote global sustainable development.', 'Premier Li Keqiang gave further instructions on policy measures to secure a bumper harvest for the whole year, after hearing a report at the State Council executive meeting on Wednesday on stabilizing grain production.', 'The practice of the \"one country, two systems\" principle in Hong Kong over the past 25 years has proved to be successful, scientific, feasible and viable, said Leung Chun-ying, former chief executive of the special administrative region.', \"China's senior political advisers called for a range of measures to accelerate the nation's low-carbon transition at a three-day gathering that concluded on Wednesday.\", 'China has made a milestone advance in its effort to build a solar power station in space to convert the sunlight in outer space into an electrical supply to drive the satellites in orbits or transmit power back to the Earth.', 'Chinese scientists have unveiled the heterogeneity and transcriptomic atlas of silkworm silk gland cells at single-cell resolution, achieving a significant breakthrough in silkworm research.', 'NASA has decided to extend the science mission of its InSight lander on Mars, operating its seismometer instrument longer than previously planned, according to a release of the agency.', \"China's National Radio and Television Administration and the Ministry of Tourism and Culture have jointly issued a set of regulatory protocols for webcast anchors.\", \"Chinese Premier Li Keqiang Wednesday stressed work on flood control and disaster relief to guarantee the safety of people's lives and property.\", \"The U.S. ban on all imports from China's Xinjiang Uygur Autonomous Region is in violation of international economic and trade rules, the China National Textile and Apparel Council (CNTAC) said Wednesday.\", 'A Chinese envoy on Wednesday called on the Security Council to lift the arms embargo on the Central African Republic (CAR) as soon as possible.', \"U.S. Federal Reserve Chair Jerome Powell said Wednesday that the central bank is trying to bring inflation down without inflicting too much damage, but the Fed's aggressive rate hikes could tip the U.S. economy into recession.\", 'One person was killed and another injured Wednesday morning following a shooting that occurred inside a Muni train in U.S. San Francisco, according to the police.', \"U.S. President Joe Biden on Wednesday called on Congress to suspend federal gas tax for 90 days, as Americans are frustrated by the nation's soaring gas prices.\", 'Chinese State Councilor and Foreign Minister Wang Yi on Wednesday met with Jang Ha-sung, ambassador of the Republic of Korea (ROK) to China, who will soon leave his post and return to the ROK.', \"China will adopt continued measures for flood control and disaster relief to earnestly protect people's lives and property, according to a decision made at the State Council's Executive Meeting chaired by Premier Li Keqiang on Wednesday.\", \"Ex-service personnel have been encouraged to work as teachers in primary and secondary schools amid China's intensified efforts to boost employment of this group and strengthen teaching teams for compulsory education.\", 'Chinese President Xi Jinping on Wednesday said China will continue to enhance opening-up and foster a business environment that is based on market principles, governed by law and up to international standards.', 'Chinese President Xi Jinping said Wednesday that China will strengthen macro-policy adjustment and adopt more effective measures to strive to meet the social and economic development targets for 2022 and minimize the impacts of COVID-19.', 'Chinese President Xi Jinping on Wednesday called for efforts to stay open and inclusive, and clear away all barriers impeding the development of productive forces to promote the sound development of globalization.', \"Clinical trial results showed that China's first domestically developed drug for the treatment of HIV proved safe and effective in a simplified combination regimen.\", \"The world's largest container ship, built by China State Shipbuilding Corporation, can carry 24,000 twenty-foot equivalent unit (TEU) containers, and was handed over in Shanghai to its client on Wednesday.\", \"The defense ministers of the Association of Southeast Asian Nations (ASEAN) vowed on Wednesday to strengthen the bloc's centrality, unity and rules-based order for the benefit of the people in the region, said a joint declaration.\", \"At least 285 people were killed and over 600 injured after an earthquake struck Afghanistan's two eastern provinces early Wednesday, the state-run Bakhtar News Agency reported.\", 'Epidemic prevention and control authorities in Hefei, Anhui province, canceled regular mass testing for COVID-19 on Tuesday.', 'The Pearl River Water Resources Commission raised the flood warning level to its highest when water in the Pearl River basin rose above warning lines on Tuesday night.', \"Scenery of beautiful Milky Way landscape above a millennia-old populus euphratica forest in Tarim Basin, northwest China's Xinjiang Uyghur Autonomous Region, June 21, 2022.\", 'A Kuaizhou-1A carrier rocket carrying the Tianxing-1 test satellite blasts off from the Jiuquan Satellite Launch Center in northwest China, June 22, 2022. ', \"Photo taken on June 22, 2022 shows flower displays set up to celebrate the 25th anniversary of Hong Kong's return to the motherland in the Hong Kong Special Administrative Region. \", 'Huazhong Agricultural University held an \"insect banquet\" to bid farewell to graduates. Locusts, silkworm pupae, bamboo insects and other insects are cooked for teachers and students to taste in Wuhan, central China\\'s Hubei Province, June 21, 2022.', 'The Universal Studios Beijing will resume operation in stages since Saturday, said the recreation park on Wednesday.', 'A Chinese Foreign Ministry spokesperson said Tuesday that China strongly condemns and firmly opposes the so-called \"Uyghur Forced Labor Prevention Act\" formulated by the United States.', 'The verdure, which blocks windblown dust in Maowusu, is a testimony of the joint efforts in fighting desertification between people from China and the Republic of Korea (ROK) over the past decade.', 'South Korea on Wednesday reported its first two suspected monkeypox cases, according to the health authorities.', 'The United States should protect its people from gun violence, as any government that safeguards human rights should do, Chinese Foreign Ministry Spokesperson Wang Wenbin said Tuesday.', \"At least 280 people were killed and 595 injured after an earthquake struck Afghansitan's Paktika province early on Wednesday, the state-run Bakhtar News Agency reported.\", 'At least 255 people were killed and over 500 injured after an earthquake struck the eastern Afghan region early on Wednesday, the state-run Bakhtar News Agency reported.', 'China on Wednesday sent a new satellite into space from the Jiuquan Satellite Launch Center in northwest China.', \"South Korea's homegrown Nuri rocket lifts off at the Naro Space Center in Goheung, South Korea, June 21, 2022. \", 'Former chief executive of the Hong Kong Special Administrative Region (HKSAR) Leung Chun-ying has hailed the practice of \"one country, two systems\" in Hong Kong over the past 25 years as sound, viable and vibrant.', 'It has been four years since Ho Yiu-wai, a Hong Kong young man, established his start-up in the Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone at the Lok Ma Chau Loop in the border area between the two cities.', 'Chinese enterprises in the Hong Kong Special Administrative Region (HKSAR) have promoted local socio-economic development, improved people\\'s livelihood and made great contributions to the long-term prosperity and stability of Hong Kong under \"one country, two systems.\"', \"This year marks the 25th anniversary of Hong Kong's return to the motherland.\", \"Flood control authority in south China's Pearl River basin activated a Level-I emergency response on Tuesday.\", 'Yoga practitioners practice Yoga to greet the Summer Solstice, the 12th of the 24 solar terms of a year, in Times Square, New York city, New York, U.S., June 21, 2022.', 'Zhaosu county, known as the \"hometown of pegasus\" in China, has turned itself into an equine tourism area with breathtaking views of galloping horses.', \"The world's largest freshwater fish, a 661-pound (300-kilogram) giant stingray, has been found near a remote island in the Mekong River in northeast Cambodia's Stung Treng province.\", 'Chinese consumers have shown rising demand for higher-quality and new-generation products containing innovative technologies during the June 18 shopping festival, and ensuring smooth logistics and supply chains is of great significance in promoting the recovery of consumption.', 'Growing reports of people experiencing a COVID-19 rebound after taking the antiviral treatment Paxlovid have \"refreshed our understanding of the novel coronavirus\", a senior Chinese public health expert said on Monday.', \"Sotheby's, a New York-based auction house, said it believes in the long run that China will remain the core market in Asia, and it will invest more in digital promotions and events to further boost its brand awareness online among the broader Chinese-speaking community.\", \"A new cultural and education recreation center that offers all-around support for teachers and students opened in Hong Kong on Tuesday to mark the 25th anniversary of the city's return to the motherland.\", 'The latest revision of the draft will focus on clarifying the quality and safety responsibility of relevant parties with the most rigorous requirements.', \"Chinese scientists found for the first time high-pressure minerals in the lunar regolith samples brought back by the Chang'e-5 lunar probe.\", 'Chinese researchers have recently reported the discovery of a new bacterial transfer RNA (tRNA) species, which can help the synthesis of antibiotics, according to the Chinese Academy of Sciences (CAS).', \"The fiscal revenue of China's central government exceeded 9 trillion yuan (about 1.35 trillion U.S. dollars) in 2021, Chinese Finance Minister Liu Kun said on Tuesday.\", \"Russian President Vladimir Putin said Tuesday that the country's first Sarmat intercontinental ballistic missile system would enter combat duty by the end of the year.\", 'The second phase of the 15th meeting of the Conference of the Parties to the UN Convention on Biological Diversity (COP15) will be held in Montreal, Canada from December 5 to 17 this year.', 'The BRICS cooperation mechanism has developed a multi-tier, systemic set of institutions that is yielding remarkable progress in key areas, and it could now speak as one voice in the global arena on behalf of emerging markets and developing countries.', 'Premier Li Keqiang stressed on Tuesday the fundamental role of grain production and energy supply in stabilizing commodity prices, saying that China has kept its prudent monetary policy in place to curb potential rises in inflation.', 'John Lee Ka-chiu, incoming chief executive of the Hong Kong Special Administrative Region, has pledged to make the utmost efforts in the coming five years of his term to build a caring and inclusive Hong Kong.', 'Tech giants, commercial banks and local-level governments in China further promoted the use of digital yuan in the run-up to and during the June 18 or \"618\" shopping carnival and carried out trials in more fields like lending and tax refunds.', 'People infected with the original version of the COVID-19 Omicron variant virus are vulnerable to reinfection by later mutated subvariants of Omicron, including BA.4, BA.5 and BA.2.12.1, according to a new study published in the journal Nature last week.', 'Some of the nine people suspected of harassing and assaulting four women in Tangshan, Hebei province, on June 10 are also suspected of being involved in other criminal acts, and some officials involved in the case have been accused.', \"The second phase of the United Nations' negotiations on the world's new biodiversity conservation goals through 2030, known as COP15, has been relocated from Kunming, Yunnan province, to the Canadian city of Montreal, between Dec 5 and 17.\", 'China strongly condemns and firmly opposes the United States\\' \"Uyghur Forced Labor Prevention Act\", said Foreign Ministry spokesman Wang Wenbin in Beijing on Tuesday, adding that China will take powerful measures to protect the lawful interests of Chinese companies and citizens.', \"Three astronauts from the Shenzhou-13 crewed mission were awarded medals for their service to China's space endeavors on Tuesday.\", 'Turkey is to host a four-way meeting with the United Nations, Russia and Ukraine in Istanbul next week to discuss a possible Black Sea corridor for exporting Ukrainian grains, local media reported Tuesday.', \"China is progressing rapidly in the development of aerospace, quantum computing and electric vehicles, which lead to more innovative competition, Paddy Cosgrave, the founder and CEO of Web Summit and Collision, two of the world's largest and fastest-growing tech conferences.\", 'A Chinese envoy warned on Tuesday that antagonism within the international community over the conflict in Ukraine is disrupting UN work.', \"The No. 2 tunnel on Indonesia's Jakarta-Bandung High-Speed Railway was drilled thrill on Tuesday, marking the completion of all the 13 tunnels on the railway, which has laid a foundation for the operation of the railway.\", 'The United States should protect its people from gun violence, as any government that safeguards human rights should do, Chinese Foreign Ministry Spokesperson Wang Wenbin said Tuesday.']}\n",
      "energy voice\n",
      "Euronews\n",
      "The free press journal\n",
      "DW\n",
      "Star\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.reuters.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "2022-06-23 14:31:32,358 - 2210765 - root - INFO - Reuters function ended\n",
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.reuters.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 14:31:38,409 - 2210765 - root - INFO - Reuters function ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ipo.einnews.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/ipykernel_launcher.py:846: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/ipykernel_launcher.py:852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "2022-06-23 14:34:42,648 - 2210765 - root - INFO - writing output artifact /home/prachi_multilex2/todays_report.csv to /home/prachi_multilex2\n",
      "2022-06-23 14:34:42,675 - 2210765 - root - INFO - completed writing output artifact /home/prachi_multilex2/todays_report.csv to /home/prachi_multilex2\n",
      "2022-06-23 14:34:42,679 - 2210765 - root - INFO - last line of scraper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from requests_html import HTMLSession\n",
    "from pathlib import Path\n",
    "from googletrans import Translator\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "from datetime import timedelta \n",
    "from newspaper import Article\n",
    "import importlib.util\n",
    "import warnings\n",
    "import pytz\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(process)d - %(name)s - %(levelname)s - %(message)s\")\n",
    "logging.info(\"first line of multilex_scraper_xform\")\n",
    "def infin_transform_all_objects(input_dir, output_dir, **kwargs):\n",
    "    logging.info(\"input_dir=\" + input_dir + \", output_dir=\" + output_dir)\n",
    "    # onlyfiles = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n",
    "    # for f in onlyfiles:\n",
    "    #  logging.info(\"file in directory %s = %s. size = %d\", input_dir, os.path.join(input_dir, f), Path(os.path.join(input_dir, f)).stat().st_size)\n",
    "    for path, subdirs, files in os.walk(input_dir):\n",
    "          for name in files:\n",
    "            logging.info(\"file in directory %s = %s. size = %d\", path, os.path.join(path, name), Path(os.path.join(path, name)).stat().st_size)\n",
    "\n",
    "    \n",
    "    #s1 = dynamic_module_import(os.path.join(input_dir, \"s1.py\"), \"s1\")\n",
    "\n",
    "    multilex_scraper(input_dir, output_dir)\n",
    "\n",
    "\n",
    "def dynamic_module_import(file_path, module_name):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "\n",
    "    logging.info(\"dynamically loaded module %s contents = %s\", file_path, dir(module))\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "def multilex_scraper(input_dir, output_dir):\n",
    "    cur_date = str(date.today())\n",
    "    not_working_functions = []\n",
    "    log_format = (\n",
    "        '[%(asctime)s] %(levelname)-8s %(name)-12s %(message)s')\n",
    "    def emptydataframe(name,df):\n",
    "        if df.empty:\n",
    "            not_working_functions.append(name+\" : err : Empty datframe\")\n",
    "    months = [\"Jan\" , \"Feb\" , \"Mar\" , \"Apr\" , \"May\" , \"Jun\" , \"Jul\" , \"Aug\" , \"Sep\" , \"Oct\" , \"Nov\" , \"Dec\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format=log_format,\n",
    "        filename=os.path.join(output_dir,'debug.log'),\n",
    "    )\n",
    "    def get_time_valid(): #Returns the hour from the time \n",
    "        IST = pytz.timezone('Asia/Kolkata')\n",
    "        time = datetime.now(IST)\n",
    "        time = time.time()\n",
    "        return time.hour\n",
    "    def translate(text):\n",
    "        translator = Translator()\n",
    "        translation = translator.translate(text, dest='en')\n",
    "        return translation.text\n",
    "    def translate_dataframe(df):\n",
    "        try:\n",
    "            for i,row in df.iterrows():\n",
    "                # row[\"publish_date\"]= translate(row[\"publish_date\"])\n",
    "                row[\"title\"] = translate(row[\"title\"])\n",
    "                row[\"text\"] = translate(row[\"text\"])\n",
    "\n",
    "                # time.sleep(0.2)\n",
    "            return df\n",
    "        except:\n",
    "            for i,row in df.iterrows():\n",
    "                # row[\"publish_date\"]= translate(row[\"publish_date\"])\n",
    "                row[\"title\"] = translate(row[\"title\"])\n",
    "                # row[\"text\"] = translate(row[\"text\"])\n",
    "\n",
    "                # time.sleep(0.2)\n",
    "            return df\n",
    "\n",
    "    def link_correction(data):\n",
    "        link = data[\"link\"].to_list()\n",
    "        new = []\n",
    "        for i in link :\n",
    "            try :\n",
    "                if(i.find(\"&ct\")!= -1):\n",
    "                    new.append(i.split(\"&ct\")[0])\n",
    "                else:\n",
    "                    new.append(i)\n",
    "            except :\n",
    "                print(\"Link is messed up \")\n",
    "\n",
    "        new_links = pd.DataFrame(new)\n",
    "        data[\"link\"] = new_links\n",
    "        return data\n",
    "\n",
    "    def correct_link(link):\n",
    "        link = str(link)\n",
    "        if(link.find(\"&ct\")!=-1):\n",
    "            link = link.split(\"&ct\")[0]\n",
    "        return link\n",
    "    def get_date_mname_d_y(date):\n",
    "        #Apr 21, 2022 \n",
    "        #21 Apr 2022\n",
    "        month = re.findall(r'''(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)''',date)\n",
    "        m = str(months.index(month[0][0].strip()) % 12 + 1)\n",
    "        y = str(re.findall(r'\\d{4}',date)[0])\n",
    "        day = str(re.findall(r'\\d{1,2}',date)[0])\n",
    "        return \"-\".join([day,m,y])\n",
    "    def get_date_min_read(date):\n",
    "    #2min read . Updated: 21 Apr 2022, time\n",
    "        return get_date_mname_d_y(date.split(\":\")[1].split(\",\")[0].strip())\n",
    "    def correct_publish_date(i):\n",
    "        try:\n",
    "            i = str(i)\n",
    "            i = i.strip()\n",
    "            if len(re.findall(\"\\d{1,2}/\\d{1,2}/\\d{4}\",i)):\n",
    "                hi = re.findall(\"\\d{1,2}/\\d{1,2}/\\d{4}\",i)\n",
    "                l1 = hi[0].split(\"/\")\n",
    "                temp = l1[0]\n",
    "                l1[0] = l1[1]\n",
    "                l1[1] = temp\n",
    "                i = \"-\".join(l1)\n",
    "                return i\n",
    "            elif(re.findall(\"\\d{1,2}/\\w{3}/\\d{4}\",i)):\n",
    "                i = get_date_mname_d_y(re.findall(\"\\d{1,2}/\\w{3}/\\d{4}\",i)[0])\n",
    "                return i\n",
    "            elif(len(re.findall(\"\\d{1,2}\\s\\w{3}\\s\\d{4}\",i))):\n",
    "\n",
    "                i = get_date_mname_d_y(re.findall(\"\\d{1,2}\\s\\w{3}\\s\\d{4}\",i)[0])\n",
    "                # print(i)\n",
    "                return i\n",
    "            elif(re.findall(r'min read . Updated:',i)):\n",
    "                i = get_date_min_read(i)   \n",
    "                return i   \n",
    "            elif(len(i.split(\".\")) == 3):\n",
    "                if(len(i.split(\".\")[0]) == 4):\n",
    "                    i = \"-\".join(i.split(\".\")[::-1])\n",
    "                    return i\n",
    "                else:\n",
    "                    i = i.replace(\".\",\"-\")\n",
    "                    return i  \n",
    "            elif len(re.findall(r'\\d{1,2}.\\d{1,2}.\\d{4}',i)) and i.find(\":\"):\n",
    "                if len(i.split(\" \"))>1:\n",
    "                    i = i.split(\" \")[2].replace(\".\",\"-\")\n",
    "                    return i\n",
    "            elif(re.findall(r'\\d{1,2}-\\d{1,2}-\\d{4}',i)):\n",
    "                return i\n",
    "            \n",
    "            elif(i.count(\":\") >= 2):\n",
    "                if len(re.findall(r'T',i)):\n",
    "                    i = \"-\".join(i.split(\"T\")[0].split(\"-\")[::-1])\n",
    "                    return i\n",
    "                if len(re.findall(r'Newswire',i)):\n",
    "                    i = \"-\".join(i.strip().split(\" \")[-3].split(\"-\")[::-1])\n",
    "                    return i\n",
    "                i = i.split(\" \")[0].strip()\n",
    "                if len(re.findall(r'[a-zA-Z]+',i)):\n",
    "                    i = get_date_mname_d_y(i)\n",
    "                else:\n",
    "                    i = \"-\".join(i.split(\"-\")[::-1])\n",
    "                    return i\n",
    "            elif len(i.split(\"-\")[0]) == 4:\n",
    "                i = \"-\".join(i.split(\"-\")[::-1])\n",
    "                return i\n",
    "            \n",
    "            i = get_date_mname_d_y(i)\n",
    "            return i\n",
    "        except:\n",
    "            i = i.strip()\n",
    "            \n",
    "            i = translate(i)\n",
    "            month = re.findall(r'''(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)''',i)\n",
    "            if not len(month):\n",
    "                i = \"Date\"\n",
    "                return i\n",
    "            m = str(months.index(month[0][0].strip()) % 12 + 1)\n",
    "            y = str(re.findall(r'\\d{4}',i)[0])\n",
    "            day = str(re.findall(r'\\d{1,2}',i)[0])\n",
    "            i = \"-\".join([day,m,y])\n",
    "            return i\n",
    "    def correct_navigable_string(df1):\n",
    "        err = []\n",
    "        for i , row in df1.iterrows():\n",
    "            try:\n",
    "                if(row[\"publish_date\"] == None):\n",
    "                    row[\"publish_date\"] = \"Date\"\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                soup = BeautifulSoup('''\n",
    "                <html>\n",
    "                    ''' + str(row[\"publish_date\"]) + '''\n",
    "                </html>\n",
    "                ''', \"lxml\")\n",
    "\n",
    "            # Get the whole h2 tag\n",
    "                row[\"publish_date\"]=str(soup.p.string)\n",
    "                row[\"publish_date\"] = str(row[\"publish_date\"]).strip()\n",
    "                row[\"publish_date\"] = correct_publish_date(row[\"publish_date\"])\n",
    "                row[\"link\"] = correct_link(row[\"link\"])\n",
    "            except:\n",
    "                # print(row)\n",
    "                # print(\"\\n\")\n",
    "                err.append(i)\n",
    "        print(err)\n",
    "        df_final=df1\n",
    "        df2 = df_final[df_final[\"publish_date\"] == \"Date\"]\n",
    "        df_final['publish_date']=pd.to_datetime(df_final['publish_date'],format=\"%d-%m-%Y\",errors='coerce',utc=True).dt.strftime(\"%d/%b/%Y\" \" \" \"%H:%M:%S\")\n",
    "        one_year_from_now = datetime.now()\n",
    "        date_formated = one_year_from_now.strftime(\"%d/%b/%Y\" \" \" \"%H:%M:%S\")\n",
    "        df_final['scraped_date'] = date_formated\n",
    "\n",
    "        public_date = pd.to_datetime(df_final['publish_date'],errors='coerce',utc=True).dt.strftime('%d-%m-%Y')\n",
    "        scrap_date= pd.to_datetime(df_final['scraped_date'],errors='coerce',utc=True).dt.strftime('%d-%m-%Y')\n",
    "\n",
    "        ## morning \n",
    "        yesterday = (date.today() - timedelta(days=1)).strftime('%d-%m-%Y')\n",
    "        daybefore = (date.today() - timedelta(days=2)).strftime('%d-%m-%Y')\n",
    "        final_1 = df_final.loc[public_date == yesterday]\n",
    "        final_2 = df_final.loc[public_date == scrap_date]\n",
    "        final_3 = df_final.loc[public_date == daybefore]\n",
    "        ## evening \n",
    "        fn = []\n",
    "        if(int(get_time_valid()) >= 16):\n",
    "            fn = [final_2,df2]\n",
    "        else:\n",
    "            fn = [final_1,final_2,final_3,df2]\n",
    "        final = pd.concat(fn)\n",
    "        return final\n",
    "    \n",
    "    def log_errors(err_logs):\n",
    "        for i in err_logs:\n",
    "            print(i)\n",
    "    def FilterFunction(final):\n",
    "        # import matplotlib.pyplot as plt\n",
    "        try:\n",
    "            keyword = [ 'IPO','IPO','IPO ','SPACs','ipo','pre-IPO','pre-ipo','PRE-IPO','pre-IPO','going public','spac','shares','public']\n",
    "            \n",
    "            # keyword = [ \"Follow-on Offering\", 'FPO', 'Seasoned Equity Offering', 'SEO',' Bookrunner', 'Underwriter', 'Rumour', 'Primary Exchange', 'Currency','raised','IPO','IPO','IPO ','SPACs','ipo','pre-IPO','pre-ipo','PRE-IPO','pre-IPO','going public','public','closes','listing','planning','closing','excellent','Public','Initial','Offering','initial','Announces','Pricing','pricing','announces','launches','Launches','SPAC','spac']\n",
    "            keywords1=['IPO,','IPO,','IPO, ','SPACs,','ipo,','pre-IPO,','pre-ipo,','PRE-IPO,','pre-IPO,','going public,','public,','closes,','listing,','planning,','closing,','excellent,','Public,','Initial,','Offering,','initial,','Announces,','Pricing,','pricing,','announces,','launches,','Launches,','SPAC,','spac,']\n",
    "            keywords=keyword\n",
    "            title=[]\n",
    "            link=[]\n",
    "            published_date=[]\n",
    "            scraped_date=[]\n",
    "            text=[]\n",
    "            flag=False\n",
    "              #Here is the dataframe to be passed\n",
    "                \n",
    "            for i in range(0,final.shape[0]):\n",
    "                article=final[\"title\"][i] + \" \" + final['text'][i]\n",
    "                article=article.split(\" \")\n",
    "                for let in article:\n",
    "                    if let in keywords :\n",
    "                        flag=True\n",
    "                        break\n",
    "                if flag==True:\n",
    "                    title.append(final['title'][i])\n",
    "                    link.append(final['link'][i])\n",
    "                    published_date.append(final['publish_date'][i])\n",
    "                    scraped_date.append(final['scraped_date'][i])\n",
    "                    text.append(final['text'][i])\n",
    "                    flag=False\n",
    "        except:\n",
    "            print('Did not pass FilterFunction')\n",
    "        final = pd.DataFrame(list(zip(title,link,published_date,scraped_date,text)), \n",
    "                   columns =['title','link','publish_date','scraped_date','text'])\n",
    "        final = final[~final['title'].isin([\"private placement\", \"reverse merger\", \"blank check merger\"])]\n",
    "        final = final[~final['text'].isin([\"private placement\", \"reverse merger\", \"blank check merger\"])]\n",
    "        \n",
    "        return final \n",
    "    \n",
    "    def MoneyControl():\n",
    "        try:\n",
    "            print(\"Moneycontrol\")\n",
    "            err_logs = []\n",
    "            baseSearchUrl = \"https://www.moneycontrol.com/rss/iponews.xml\"\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            err_index = []\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            queryUrl = baseSearchUrl\n",
    "            try:\n",
    "                pageSource = requests.get(baseSearchUrl)\n",
    "            except:\n",
    "                err = \"moneycontrol: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                exit()\n",
    "            parsedSource = BeautifulSoup(pageSource.content, \"xml\")\n",
    "            for eachItem in parsedSource.find_all(\"item\"):\n",
    "                currentArticleTitle = eachItem.find(\"title\").text\n",
    "                currentArticleLink = eachItem.find(\"link\").text\n",
    "                currentArticleDate = datetime.strptime(str(eachItem.find(\"pubDate\").text).split(\"+\", maxsplit=2)[0].strip(),\n",
    "                                                    \"%a, %d %b %Y %H:%M:%S\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "                titles.append(currentArticleTitle)\n",
    "                links.append(currentArticleLink)\n",
    "                ArticleDates.append(currentArticleDate)\n",
    "                ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "            scrapedData[\"title\"] = titles\n",
    "            scrapedData[\"link\"] = links\n",
    "            scrapedData[\"publish_date\"] = ArticleDates\n",
    "            scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "            ArticleBody = []\n",
    "            for link in links:\n",
    "                articleText = \"\"\n",
    "                try:\n",
    "                    headers = {\n",
    "                        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                    pageSource = requests.get(link, headers=headers)\n",
    "                    parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                except:\n",
    "                    err = \"moneycontrol: err: Failed to access article link : \" + link\n",
    "                    ArticleDates.append(\"Error\")\n",
    "                    err_index.append(links.append(link))\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                requiredDiv = parsedSource.find(\"div\", class_=\"content_wrapper arti-flow\")\n",
    "                if not requiredDiv:\n",
    "                    articleText = titles[links.index(link)]\n",
    "                    ArticleBody.append(articleText)\n",
    "                    continue\n",
    "                else:\n",
    "                    for item in requiredDiv.find_all(\"p\"):\n",
    "                        try:\n",
    "                            if not (re.search(\"/Click Here/gm\", item.text) or re.search(\"/Also read/gm\", item.text) or\n",
    "                                    re.search(\"/Disclaimer/gm\", item.text)):\n",
    "                                articleText += (\" \" + item.text)\n",
    "                            else:\n",
    "                                continue\n",
    "                        except:\n",
    "                            continue\n",
    "                    ArticleBody.append(articleText)\n",
    "                scrapedData[\"text\"] = ArticleBody\n",
    "            moneycontrolDF = pd.DataFrame(scrapedData)\n",
    "            moneycontrolDF = moneycontrolDF.drop_duplicates(subset=[\"link\"])\n",
    "            if moneycontrolDF.empty:\n",
    "                err = \"moneycontrol: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(moneycontrolDF)\n",
    "            emptydataframe(\"Moneycontrol\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(moneycontrolDF)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Moneycontrol not working\")\n",
    "            not_working_functions.append(\"Moneycontrol\")\n",
    "    def Cnbc_Seeking():\n",
    "        try:\n",
    "            print('2')\n",
    "            s_dates=[]  \n",
    "            title = []\n",
    "            text = []\n",
    "            dates = []\n",
    "            links = []\n",
    "            try:\n",
    "                urls=\"https://www.cnbc.com/id/10000666/device/rss\"\n",
    "                logging.info(\"cnbc.com: invoking requests.url()=\" + urls)\n",
    "                page=requests.get(urls,verify = False)\n",
    "                logging.info(\"cnbc.com: completed requests.url()=\" + urls)\n",
    "                soup=BeautifulSoup(page.content,features='lxml')\n",
    "                soup.find_all()\n",
    "                # print(soup)\n",
    "                import datetime\n",
    "                x=datetime.datetime.now()\n",
    "                y=x.date()\n",
    "                x=x.date()\n",
    "                x=str(x)\n",
    "                x=x.split('-')\n",
    "                curr_date=x[2]\n",
    "                curr_date=int(curr_date)\n",
    "                curr_date=curr_date-1\n",
    "                curr_date=str(curr_date)\n",
    "                # print(curr_date)\n",
    "                for i in range(1,len(soup.find_all('title'))):\n",
    "                    dates.append(soup.find_all('pubdate')[i].text)\n",
    "                    text.append(soup.find_all('description')[i].text)\n",
    "                    s_dates.append(y)\n",
    "                    title.append(soup.find_all('title')[i].text)\n",
    "                    #company country link\n",
    "                    link1=soup.find('item').text\n",
    "                    link1=link1.split(\" \")\n",
    "                    for wordse in link1:\n",
    "                        wordse=wordse.split(':')\n",
    "                    if wordse[0]=='https':\n",
    "                        link1=wordse[0]+\":\"+wordse[1]\n",
    "                        links.append(link1)\n",
    "            except:\n",
    "                print('CNDC_seeking is not working')\n",
    "                not_working_functions.append('Cnbc')\n",
    "\n",
    "            cnbc = pd.DataFrame(list(zip(title,dates,s_dates,links,text)), \n",
    "                            columns =['title','publish_date','scraped_date','link','text'],index=None)\n",
    "            df = FilterFunction(cnbc)\n",
    "            emptydataframe(\"Cnbc\",df)\n",
    "            logging.info(\"CNBC_seeking function ended\")\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def korea():\n",
    "        try :\n",
    "            print(\"Korea\")\n",
    "            err_logs = []\n",
    "            url = \"http://www.koreaherald.com/search/index.php?kr=0&q=IPO\"\n",
    "            domain_url = \"http://www.koreaherald.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Korea : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            for a in soup.find_all('ul', {'class':'main_sec_li'}):\n",
    "                            #links.append(a[\"href\"])\n",
    "                for l in a.find_all('a',href=True):\n",
    "                                #print(l['href'])\n",
    "                    links.append(domain_url + l[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Korea : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    \n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" : \"view_tit_byline_r\"}).text)\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"view_tit\"}).text)       \n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" :\"view_con article\"}).text)\n",
    "                # print(text)\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Korea : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Korea\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Korea not working\")\n",
    "            not_working_functions.append('Korea')\n",
    "    def proactive(keyword):\n",
    "        try:\n",
    "            err_logs = []\n",
    "            url = f\"https://www.proactiveinvestors.com.au/search/advancedSearch/news?url=&keyword={keyword}\"\n",
    "            domain_url = \"https://www.proactiveinvestors.com.au/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "            # Class names of the elements to be scraped\n",
    "            div_class = \"advanced-search-block\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_class = \"h2\"\n",
    "            date_p_itemprop = \"datePublished\"\n",
    "            para_div_itemprop = \"articleBody\"\n",
    "            links = []\n",
    "            for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link[1:]\n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "            collection = []\n",
    "            scrapper_name = \"proactiveinvestors\"\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link, headers=headers)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "                # Scraping the published date\n",
    "                try:\n",
    "                    date_ele = l_soup.find(\"p\", {\"itemprop\": date_p_itemprop})\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                # Scraping the paragraph\n",
    "                try:\n",
    "                    para_ele = (l_soup.findAll(\"div\", {\"itemprop\": para_div_itemprop}))[-1]\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Proactive investors\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"Proactive Inverstors\")\n",
    "            print(\"Proactive investors not working\")\n",
    "    def Reuters(keyword):\n",
    "        print('7')\n",
    "        try:\n",
    "            title = []\n",
    "            text = []\n",
    "            s_dates = []\n",
    "            links=[]\n",
    "            pub_date=[]\n",
    "            try:\n",
    "                url = f'https://www.reuters.com/search/news?blob={keyword}&sortBy=date&dateRange=all'\n",
    "                url1 = 'https://www.reuters.com'\n",
    "                page=requests.get(url,verify = False)\n",
    "                soup=BeautifulSoup(page.content,'html.parser')\n",
    "                y = soup.findAll(\"h5\", {\"class\" : \"search-result-timestamp\"})\n",
    "                for x in y:\n",
    "                    import re\n",
    "                    TAG_RE = re.compile(r'<[^>]+>')\n",
    "                    pubdate = TAG_RE.sub('',str(x))\n",
    "                    pub_date.append(str(pubdate))\n",
    "                for i in range(1,len(soup.find_all('h3'))):\n",
    "                    pdd=soup.find_all('h3')[i]\n",
    "                    for a in pdd.find_all('a',href=True):\n",
    "                        links.append(url1 + a[\"href\"])\n",
    "                # print(links)\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'lxml')\n",
    "                    t = sp\n",
    "                    x=sp.find(\"h1\", { \"class\" : \"text__text__1FZLe text__dark-grey__3Ml43 text__medium__1kbOh text__heading_2__1K_hh heading__base__2T28j heading__heading_2__3Fcw5\" })\n",
    "                    if x is not None:\n",
    "                        n = x.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    z=sp.find(\"div\", { \"class\":\"article-body__content__17Yit paywall-article\" })\n",
    "                    if z is not None:\n",
    "                        k = z.text\n",
    "                    else:\n",
    "                        k = None\n",
    "                    #print(z)\n",
    "                    text.append(k)\n",
    "                    title.append(n)\n",
    "                    s_dates.append(cur_date)\n",
    "            except:\n",
    "                print('Reuters is not working')\n",
    "                # not_working_functions.append('Reuters')\n",
    "            percentile_list ={'publish_date': pub_date,'scraped_date': s_dates,'title': title,'link': links,'text':text}\n",
    "            reuters = pd.DataFrame.from_dict(percentile_list, orient='index')\n",
    "            df= reuters.transpose()\n",
    "            df.dropna(inplace=True)\n",
    "            # df = FilterFunction(reuters)\n",
    "            emptydataframe(\"reuters\",df)\n",
    "            logging.info(\"Reuters function ended\")\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def TradingChart():\n",
    "        try :\n",
    "            print(\"FTC\")\n",
    "            err_logs = []\n",
    "            url = \"https://futures.tradingcharts.com/search.php?keywords=IPO&futures=1\"\n",
    "            domain_url = \"https://futures.tradingcharts.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"FTC : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"clUnSeResItemTitle\"})\n",
    "            for div in all_divs:\n",
    "                links.append(\"https:\"+div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"FTC : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"h2\" , {\"class\" : \"fe_heading2\"}).text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                div = soup.find(\"div\" , {\"class\" : \"news_story m-cellblock m-padding\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"i\"):\n",
    "                    t += i.text + \" \"\n",
    "                pub_date.append(t)\n",
    "               \n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"news_story m-cellblock m-padding\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"FTC : err: Empty datframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"FTC\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"FTC not working\")\n",
    "            not_working_functions.append('FTC')\n",
    "    def einnews():\n",
    "        try:\n",
    "            print(\"IPO EinNews\")\n",
    "            err_logs = []\n",
    "\n",
    "            baseSearchUrl = \"https://ipo.einnews.com/\"\n",
    "            domainUrl = \"https://ipo.einnews.com\"\n",
    "            keywords = ['IPO', 'pre-IPO', 'initial public offering']\n",
    "\n",
    "            # use this for faster testing\n",
    "            tkeywords = [\"IPO\"]\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            err_index = []\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            ArticleBody = []\n",
    "            for keyword in tkeywords:\n",
    "                queryUrl = baseSearchUrl\n",
    "                try:\n",
    "                    session = HTMLSession()\n",
    "                    resp = session.post(queryUrl)\n",
    "                    resp.html.render()\n",
    "                    pageSource = resp.html.html\n",
    "                    parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "                except:\n",
    "                    err = \"einnews: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                # with open(\"response.html\", \"w\") as f:\n",
    "                #     f.write(pageSource)\n",
    "                # break\n",
    "                for item in parsedSource.find(\"ul\", class_=\"pr-feed\").find_all(\"li\"):\n",
    "                    requiredTag = item.find(\"h3\")\n",
    "                    currentArticleTitle = str(requiredTag.find(\"a\").text).strip()\n",
    "                    # print(currentArticleTitle)\n",
    "                    currentArticleLink = requiredTag.find(\"a\")[\"href\"]\n",
    "                    # print(currentArticleLink)\n",
    "                    if currentArticleLink[0] == \"/\":\n",
    "                        links.append(domainUrl + currentArticleLink)\n",
    "                    else:\n",
    "                        links.append(currentArticleLink)\n",
    "                    titles.append(currentArticleTitle)\n",
    "                    currentArticleDateText = item.find(\"span\", class_=\"date\").text\n",
    "                    if re.search(\"^\\d.*\", currentArticleDateText):\n",
    "                        try:\n",
    "                            currentArticleDate = datetime.today().strftime(\"%d-%m-%Y\")\n",
    "                        except:\n",
    "                            err = \"einnews: err: Failed to retrieve date from article : \" + currentArticleLink\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(currentArticleLink))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        ArticleDates.append(currentArticleDate)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    else:\n",
    "                        try:\n",
    "                            currentArticleDate = datetime.strptime(currentArticleDateText,\n",
    "                                                                \"%b %d, %Y\").strftime(\"%d-%m-%Y\")\n",
    "                        except:\n",
    "                            err = \"einnews: err: Failed to retrieve date from article : \" + currentArticleLink\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(currentArticleLink))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        ArticleDates.append(currentArticleDate)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    articleText = \"\"\n",
    "                    for pitem in item.find_all(\"p\"):\n",
    "                        articleText += pitem.text\n",
    "                    ArticleBody.append(articleText.strip(\"\\n\"))\n",
    "\n",
    "                scrapedData[\"title\"] = titles\n",
    "                scrapedData[\"link\"] = links\n",
    "                scrapedData[\"publish_date\"] = ArticleDates\n",
    "                scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                scrapedData[\"text\"] = ArticleBody\n",
    "                # print(titles)\n",
    "                # print(links)\n",
    "                # print(ArticleDates)\n",
    "                # print(ArticleBody)\n",
    "\n",
    "            # DataFrame creation\n",
    "            einnewsDF = pd.DataFrame(scrapedData)\n",
    "            if einnewsDF.empty:\n",
    "                err = \"einnews: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(einnewsDF)\n",
    "            emptydataframe(\"Einnews\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"IPO Einnews\")\n",
    "            print(\"EINnews not working\")\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def rss():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            description=[]\n",
    "            scraped_date=[]\n",
    "            cleaned_description=[]\n",
    "            try:\n",
    "                print('9')\n",
    "                URL=\"https://ipo.einnews.com/rss/cpCdwuL2w4azHHGe\"\n",
    "                # logging.info(\"go_rss.com: invoking requests.url()=\" + URL)\n",
    "                page=requests.get(URL,verify = False)\n",
    "                # logging.info(\"go_rss.com: completed requests.url()=\" + URL)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "\n",
    "                import datetime  \n",
    "\n",
    "                # using now() to get current time  \n",
    "                current_time = datetime.datetime.now() \n",
    "                import html\n",
    "\n",
    "                for i in range(1,len(soup.find_all('title'))):\n",
    "                    pdd=soup.find_all('title')[i]\n",
    "                    if soup.find_all('title')[i].get_text() not in articles:\n",
    "                        articles.append(soup.find_all('title')[i].get_text())\n",
    "                        links.append(soup.find_all('link')[i].get_text())\n",
    "                        dates.append(soup.find_all('pubDate')[i-1].get_text())\n",
    "                        description.append(html.unescape(soup.find_all('description')[i].get_text()))\n",
    "\n",
    "                        scraped_date.append(current_time)\n",
    "\n",
    "                #cleaning description\n",
    "                \n",
    "                import re\n",
    "                for i in range(0,len(description)-1):\n",
    "                    art=description[i]\n",
    "                    art= art.lower().replace(\"don't\",\"do not\")\n",
    "                    art = art.replace('â€¦', '').replace('<span class=\"match\">', '').replace('</span>','').replace('\\n','').replace('     ','')\n",
    "                    art = art.replace(\",,\",\",\")\n",
    "                    cleaned_description.append(re.sub('\\n','',art))\n",
    "\n",
    "                cleaned_description\n",
    "\n",
    "            except:\n",
    "                print('Rss not working')\n",
    "                # not_working_functions.append('RSS')\n",
    "            df1 = pd.DataFrame(list(zip(articles,links,cleaned_description,dates,scraped_date)), \n",
    "                        columns =['title', 'link','text','publish_date','scraped_date'])\n",
    "\n",
    "            import html\n",
    "            for i in range(0,df1.shape[0]):\n",
    "                df1['title'][i]=html.unescape(df1['title'][i])\n",
    "\n",
    "\n",
    "            for i in range(0,df1.shape[0]):\n",
    "    #             print(df1['link'][i])\n",
    "                response=requests.get(df1['link'][i])\n",
    "                df1['link'][i]=response.url\n",
    "\n",
    "            Rss = FilterFunction(df1)\n",
    "            emptydataframe(\"RSS\",Rss)\n",
    "            # logging.info(\"Rss function ended\")\n",
    "            # Rss  = link_correction(Rss)\n",
    "            return Rss\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def GoogleAlert():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            try:\n",
    "                print('10')\n",
    "                import html\n",
    "                url=\"https://www.google.co.in/alerts/feeds/15296043414695393299/12391429027627390948\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                    pdd=soup.find_all('entry')[i]\n",
    "                    for title in pdd.find_all('title'):\n",
    "                        articles.append(html.unescape(title.text))\n",
    "                    for link in pdd.find_all('link'):\n",
    "                        links.append(html.unescape(link['href']))\n",
    "                    for date in pdd.find_all('published'):\n",
    "                        dates.append(date.text)\n",
    "                        scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert is not working')\n",
    "                not_working_functions.append('Google Alert')\n",
    "                \n",
    "            df_google = pd.DataFrame(list(zip(articles,articles,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            \n",
    "            df = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google alert\",df)\n",
    "            logging.info(\"GoogleAlert function ended\")\n",
    "            df_google  = link_correction(df_google)\n",
    "            return df_google\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "            \n",
    "\n",
    "    def live(keyword):\n",
    "        try:\n",
    "            print('11')\n",
    "            links = []\n",
    "            title = []\n",
    "            dates = []\n",
    "            s_date = []\n",
    "            text = []\n",
    "            try:\n",
    "                print('11')\n",
    "                url = f'https://www.livemint.com/Search/Link/Keyword/{keyword}'\n",
    "                url1 = 'https://www.livemint.com'\n",
    "                logging.info(\"live.com: invoking requests.url()=\" + url)\n",
    "                page = requests.get(url,verify = False)\n",
    "                logging.info(\"live.com: completed requests.url()=\" + url)\n",
    "\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                for a in soup.find_all('h2', {'class': 'headline'}):\n",
    "                    for i in a.find_all('a',href=True):\n",
    "                        #print(i['href'])\n",
    "                        links.append(url1 + i[\"href\"])\n",
    "\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"h1\", { \"class\" : \"headline\" }).text\n",
    "                    #print(x)\n",
    "                    title.append(x)\n",
    "                    y = sp.find(\"span\", { \"class\" : \"articleInfo pubtime\" }).text\n",
    "                    dates.append(y)\n",
    "                    z = sp.find(\"div\", { \"class\" : \"mainArea\" }).text\n",
    "                    #print(z)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "                    text.append(z)\n",
    "            except:\n",
    "                print('Live is not working')\n",
    "                not_working_functions.append('Live mint')\n",
    "\n",
    "            live = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                                columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            \n",
    "            df = FilterFunction(live)\n",
    "            emptydataframe(\"Livemint\",df)\n",
    "            logging.info(\"Live function ended\")\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def xinhuanet():\n",
    "        try:\n",
    "            try:\n",
    "                print('13')\n",
    "                url = \"http://www.xinhuanet.com/english/mobile/business.htm\"\n",
    "                logging.info(\"www.xinhuanet.com: invoking requests.url()=\" + url)\n",
    "                page=requests.get(url)\n",
    "                logging.info(\"www.xinhuanet.com: completed invoking requests.url()=\" + url)\n",
    "                soup=BeautifulSoup(page.content,'lxml')\n",
    "                lists = []\n",
    "                title = []\n",
    "                dates = []\n",
    "                text = []\n",
    "                s_date = []\n",
    "                for h in soup.findAll('li'):\n",
    "                    for k in h.findAll('a'):\n",
    "                        lists.append(k['href'])\n",
    "\n",
    "                links= [line for line in lists if 'c_' in line]\n",
    "\n",
    "                for link in links:\n",
    "                    fetch = requests.get(link)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"h1\", { \"class\" : \"Btitle\" })\n",
    "                    if x is not None:\n",
    "                        s = x.text\n",
    "                    else:\n",
    "                        s = None\n",
    "                    #print(s)\n",
    "                        #print(s)\n",
    "                    title.append(s)\n",
    "                    y = sp.find(\"i\", { \"class\" : \"time\" }).text\n",
    "                        #print(y)\n",
    "                    dates.append(y)\n",
    "                    z = sp.find(\"div\", { \"class\" : \"content\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    #print(n)\n",
    "                    #print(n)\n",
    "                    text.append(n)\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('xinhuanet is not working')\n",
    "                not_working_functions.append('Xinhuanet')\n",
    "            import pandas as pd\n",
    "\n",
    "\n",
    "            df2 = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text']) \n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Xinhuanet\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "\n",
    "    def kontan(keyword):\n",
    "        try:\n",
    "            links = []\n",
    "            title = []\n",
    "            dates = []\n",
    "            text = []\n",
    "            s_date = []\n",
    "            try:\n",
    "                print('14')\n",
    "                \n",
    "                url = f\"https://www.kontan.co.id/search/?search={keyword}&Button_search=\"\n",
    "                url1 = \"https:\"\n",
    "                logging.info(\"www.kontan.co.id: invoking requests.url()=\" + url)\n",
    "                page=requests.get(url)\n",
    "                logging.info(\"www.kontan.co.id: completed invoking requests.url()=\" + url)\n",
    "                soup=BeautifulSoup(page.content,'lxml')\n",
    "                for divtag in soup.find_all('div', {'class': 'sp-hl linkto-black'}):\n",
    "                        for a in divtag.find_all('a',href=True):\n",
    "                            links.append(url1 + a['href'])\n",
    "                for link in links:\n",
    "                    fetch = requests.get(link)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"h1\", { \"class\" : \"detail-desk\" })\n",
    "                    if x is not None:\n",
    "                        s = x.text\n",
    "                    else:\n",
    "                        s = None\n",
    "                    #print(s)\n",
    "                        #print(s)\n",
    "                    title.append(s)\n",
    "                    y = sp.find(\"div\", { \"class\" : \"fs14 ff-opensans font-gray\" })\n",
    "                    if y is not None:\n",
    "                        k = y.text\n",
    "                        k = k.replace('Mei','May')\n",
    "                        k = k.split(',')\n",
    "                        k = k[1]\n",
    "                    else:\n",
    "                        k = None\n",
    "                        #print(y)\n",
    "                    dates.append(k)\n",
    "                    z = sp.find(\"div\", { \"class\" : \"tmpt-desk-kon\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    #print(n)\n",
    "                    #print(n)\n",
    "                    text.append(n)\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('kontan not working')\n",
    "                not_working_functions.append(\"Kontan\")\n",
    "            import pandas as pd\n",
    "\n",
    "\n",
    "            df2 = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            \n",
    "            df = translate_dataframe(df2)\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Kontan\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    \n",
    "    def AZ(keyword):\n",
    "        try:\n",
    "            print(\"AZ\")\n",
    "            err_logs = []\n",
    "            url = f\"https://en.trend.az/search?query={keyword}\"\n",
    "            domain_url = \"https://en.trend.az/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            div_class = \"inlineSearchResults\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_div_class = \"top-part\"\n",
    "            date_span_class = \"date-time\"\n",
    "            para_div_class = \"article-content\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"trend.az\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"div\", {\"class\": title_div_class})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": date_span_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                try:\n",
    "                    para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"AZ\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"AZ is not working\")\n",
    "            not_working_functions.append(\"AZ\")\n",
    "    def German():\n",
    "        try :\n",
    "            print(\"German\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.tagesschau.de/suche2.html?query=IPO&sort_by=date\"\n",
    "            domain_url = \"https://www.tagesschau.de\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"German : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"h3\",{\"class\":\"headline\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"German : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"span\" , {\"class\" : \"seitenkopf__headline--text\"}).text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"metatextline\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"main\" , {\"class\" : \"content-wrapper content-wrapper--show-cuts\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\" , {\"class\" : \"m-ten  m-offset-one l-eight l-offset-two textabsatz columns twelve\"}):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"German : err: Empty datframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            # df = FilterFunction(df)\n",
    "            emptydataframe(\"German\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"German not working\")\n",
    "            not_working_functions.append('German')\n",
    "    def Japannews():\n",
    "        try:\n",
    "            title=[]\n",
    "            text=[]\n",
    "            dates = []\n",
    "            s_date = []\n",
    "            text = []\n",
    "            try:\n",
    "                print('Japannews')\n",
    "                url = \"https://www.japantimes.co.jp/tag/ipo/\"\n",
    "                logging.info(\"Japannews: invoking requests.url()=\" + url)\n",
    "                page=requests.get(url)\n",
    "                logging.info(\"Japannews: Completed invoking requests.url()=\" + url)\n",
    "                soup=BeautifulSoup(page.content,'html.parser')\n",
    "                #print(soup)\n",
    "                links=[]\n",
    "                for divtag in soup.find_all('div', {'class': 'main_content'}):\n",
    "                        for a in divtag.find_all('a',href=True):\n",
    "                            if a[\"href\"].startswith(\"http\"):\n",
    "    #                             print(a[\"href\"])\n",
    "                                links.append(a[\"href\"])\n",
    "\n",
    "\n",
    "                for l in links:\n",
    "                    try:\n",
    "                        fetch = requests.get(l)\n",
    "                        sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                        #print(sp)\n",
    "                        title.append(sp.find(\"h1\").text)\n",
    "\n",
    "                        t=sp.find(\"div\", {'class': 'entry'})\n",
    "                        x=t.find_all(\"p\")\n",
    "                        tet=[]\n",
    "                        for i in x:\n",
    "                            tet.append(i.text)\n",
    "                        text.append(''.join(tet))   \n",
    "\n",
    "                        j=sp.find('div',{'class': 'meta-right'})\n",
    "                        m=j.find_all('li')\n",
    "        #                 print(m[2].text)\n",
    "                        dates.append(m[2].text.strip())\n",
    "                        from datetime import datetime, date\n",
    "                        cur_date = str(datetime.today())\n",
    "                        s_date.append(cur_date)     \n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                print('japan not working')\n",
    "                not_working_functions.append(\"Japan\")\n",
    "\n",
    "            japanese = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                                columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            japanese['publish_date']  = pd.to_datetime(japanese['publish_date'],errors='coerce',utc=True).dt.strftime('%d-%b-%Y' \" \" \"%H:%M:%S\")\n",
    "            df = FilterFunction(japanese)\n",
    "            emptydataframe(\"Japan\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Romania():\n",
    "        try:\n",
    "            try:\n",
    "                print('Romania')\n",
    "                url = \"https://adevarul.ro/cauta/?terms=ofert%C4%83%20public%C4%83%20ini%C8%9Bial%C4%83&fromDate=2012-1-1&toDate=2021-3-10&tab=mrarticle&page=1&sortBy=cronologic\"\n",
    "                url1 = \"https://adevarul.ro\"\n",
    "                logging.info(\"Romania: invoking requests.url()=\" + url)\n",
    "                page=requests.get(url).content\n",
    "                logging.info(\"Romania: Completed invoking requests.url()=\" + url)\n",
    "                unicode_str = page.decode(\"utf-8\")\n",
    "                encoded_str = unicode_str.encode(\"ascii\",'ignore')\n",
    "                soup = BeautifulSoup(encoded_str, \"html.parser\")\n",
    "\n",
    "                links = []\n",
    "                dates = []\n",
    "                s_date = []\n",
    "                text = []\n",
    "                title = []\n",
    "\n",
    "                for divtag in soup.find_all('h2', {'class': 'defaultTitle'}):\n",
    "                    for a in divtag.find_all('a',href=True):\n",
    "                        links.append(url1 + a[\"href\"])\n",
    "\n",
    "                for link in links:\n",
    "                    fetch = requests.get(link)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find('h1').text\n",
    "                    title.append(x)\n",
    "\n",
    "                    y = sp.findAll(\"aside\", { \"class\" : \"tools clearfix\" })\n",
    "\n",
    "                    for i in y:\n",
    "                        date_time = i.time['datetime']\n",
    "\n",
    "                        dates.append(date_time)\n",
    "\n",
    "                    z = sp.find(\"div\", { \"class\" : \"article-body\" }).text\n",
    "            #         print(z)\n",
    "                    text.append(z)\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('romania is not working')\n",
    "                not_working_functions(\"Romania\")\n",
    "            romania = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            # romania['publish_date']  = pd.to_datetime(romania['publish_date'],errors='coerce',utc=True).dt.strftime('%d-%b-%Y' \" \" \"%H:%M:%S\")\n",
    "            # df = FilterFunction(romania)\n",
    "            romania = translate_dataframe(romania)\n",
    "            romania = FilterFunction(romania)\n",
    "            emptydataframe(\"Romania\",romania)\n",
    "            #nonenglish\n",
    "            romania  = link_correction(romania)\n",
    "            return romania\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Swedish():\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            try:\n",
    "                print('Swedish')\n",
    "                url = \"https://www.svd.se/sok?q=ipo\"\n",
    "                url1 = \"https://www.svd.se\"\n",
    "                logging.info(\"Swedish: invoking requests.url()=\" + url)\n",
    "                page=requests.get(url).content\n",
    "                logging.info(\"Swedish: Completed invoking requests.url()=\" + url)\n",
    "                unicode_str = page.decode(\"utf-8\")\n",
    "                encoded_str = unicode_str.encode(\"ascii\",'ignore')\n",
    "                soup = BeautifulSoup(encoded_str, \"html.parser\")\n",
    "\n",
    "                links = []\n",
    "                dates = []\n",
    "                s_date = []\n",
    "                text = []\n",
    "                title = []\n",
    "\n",
    "                for divtag in soup.find_all('a', {'class': 'Teaser-link'}):\n",
    "                    links.append(url1 + divtag['href'])\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        fetch = requests.get(link)\n",
    "                    except requests.exceptions.ConnectionError:\n",
    "                        requests.status_code = \"Connection refused\"\n",
    "\n",
    "\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find('h1', {'class': 'ArticleHead-heading'})\n",
    "                    if x is not None:\n",
    "                            ementa = x.text\n",
    "                    else:\n",
    "                            ementa = None\n",
    "                    title.append(ementa)\n",
    "\n",
    "                    y = sp.findAll(\"div\", { \"class\" : \"Meta-part Meta-part--published\" })\n",
    "                    for x in y:\n",
    "                        x = x.get_text()\n",
    "                        x = re.sub('[^0-9-:.]+', ' ', x)\n",
    "                        dates.append(x)\n",
    "\n",
    "                    z = sp.find(\"div\", { \"class\" : \"Body\"})\n",
    "                    if z is not None:\n",
    "                            n = z.text\n",
    "                    else:\n",
    "                            n = None\n",
    "                    text.append(n)\n",
    "                    s_date.append(cur_date)\n",
    "\n",
    "            except:\n",
    "                print('swedish is not working')\n",
    "                not_working_functions.append(\"Swedish\")\n",
    "            swedish = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            # swedish['publish_date']  = pd.to_datetime(swedish['publish_date'],errors='coerce',utc=True).dt.strftime('%d-%b-%Y' \" \" \"%H:%M:%S\")\n",
    "            df = FilterFunction(swedish)\n",
    "            # emptydataframe(\"Swedish\",df)\n",
    "            swedish  = link_correction(swedish)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Spanish():\n",
    "        try:\n",
    "            try:\n",
    "                print('Spanish')\n",
    "                urls=[\n",
    "                    \"https://www.abc.es/rss/feeds/abc_EspanaEspana.xml\",\n",
    "                    \"http://ep01.epimg.net/rss/elpais/inenglish.xml\",\n",
    "                    \"https://feeds.thelocal.com/rss/es\",\n",
    "                    \"http://www.tenerifenews.com/feed/\"\n",
    "                ]\n",
    "                # logging.info(\"www.fr.de: invoking requests.url()=\" + url)\n",
    "                page=requests.get(urls[0])\n",
    "                soup=BeautifulSoup(page.content,features='lxml')\n",
    "                soup.find_all()\n",
    "\n",
    "\n",
    "\n",
    "                title=[]\n",
    "                link=[]\n",
    "                publish_date=[]\n",
    "                scraped_date=[]\n",
    "                #finding date for the above script\n",
    "                import datetime\n",
    "                x=datetime.datetime.now()\n",
    "                x=x.date()\n",
    "                x=str(x)\n",
    "                x=x.split('-')\n",
    "                date=x[2]\n",
    "                #print(date)\n",
    "                month=x[1]\n",
    "                month=str(month)\n",
    "                #print(month)\n",
    "                if (month=='01'):\n",
    "                    mon='Jan'\n",
    "                elif (month=='02'):\n",
    "                    mon='Feb'\n",
    "                elif (month=='03'):\n",
    "                    mon='Mar'\n",
    "                elif (month=='04'):\n",
    "                    mon='Apr'\n",
    "                elif (month=='05'):\n",
    "                    mon='May'\n",
    "                elif (month=='06'):\n",
    "                    mon='Jun'\n",
    "                elif (month=='07'):\n",
    "                    mon='Jul'\n",
    "                elif (month=='08'):\n",
    "                    mon='Aug'\n",
    "                elif (month=='09'):\n",
    "                    mon='Sep'\n",
    "                elif (month=='10'):\n",
    "                    mon=='Oct'\n",
    "                elif (month=='11'):\n",
    "                    mon='Nov'\n",
    "                else:\n",
    "                    mon='Dec'\n",
    "                date=str(date)\n",
    "    \n",
    "                date=int(date)\n",
    "                date_int=date-1\n",
    "                date_int=str(date_int)\n",
    "                if(len(date_int)==1):\n",
    "                    date_int='0'+date_int\n",
    "                date_int=str(date_int)\n",
    "                date=str(date)\n",
    "                date_int=str(date_int)\n",
    "    \n",
    "\n",
    "                for i in range(0,len(soup.find_all('title'))):\n",
    "                    pub_date=soup.find_all('pubdate')[i-1].text\n",
    "                #print(pub_date)\n",
    "                    pub_date=pub_date.split(' ')\n",
    "                    pub_date = str(pub_date)\n",
    "                    pub_date = re.sub('[^A-Za-z0-9%:]+', ' ', pub_date)\n",
    "                    act_date=pub_date[1]\n",
    "                    act_mon=pub_date[2]\n",
    "                    #print(act_date,act_mon)\n",
    "                    act_date=str(act_date)\n",
    "                    act_mon=str(act_mon)\n",
    "                    #print(act_mon,mon)\n",
    "\n",
    "                    if(act_mon==mon):\n",
    "                        if(act_date==date or act_date==date_int):\n",
    "                            title.append(soup.find_all('title')[i].text)\n",
    "                            link.append(soup.find_all('guid')[i-1].text)\n",
    "                            scraped_date.append(pub_date)\n",
    "                            publish_date.append(pub_date)\n",
    "\n",
    "            except:\n",
    "                print('Spanish not working')\n",
    "                not_working_functions(\"Spanish\")\n",
    "\n",
    "            import pandas as pd \n",
    "            df2 = pd.DataFrame(list(zip(title,link,publish_date,scraped_date)), \n",
    "                        columns =['title','link','publish_date','scraped_date'])\n",
    "            df2\n",
    "\n",
    "\n",
    "            # In[175]:\n",
    "\n",
    "\n",
    "            title=[]\n",
    "            link=[]\n",
    "            publish_date=[]\n",
    "            scraped_date=[]\n",
    "\n",
    "\n",
    "            try:\n",
    "                page=requests.get(urls[2])\n",
    "                soup=BeautifulSoup(page.content,features='lxml')\n",
    "                soup.find_all()\n",
    "\n",
    "\n",
    "                # In[176]:\n",
    "\n",
    "\n",
    "                for i in range(0,len(soup.find_all('title'))):\n",
    "                        pub_date=soup.find_all('pubdate')[i-1].text\n",
    "                        #print(pub_date)\n",
    "                        pub_date=pub_date.split(' ')\n",
    "\n",
    "                        act_date=pub_date[1]\n",
    "                        act_mon=pub_date[2]\n",
    "                        #print(act_date,act_mon)\n",
    "                        act_date=str(act_date)\n",
    "                        act_mon=str(act_mon)\n",
    "                        #print(act_mon,mon)\n",
    "\n",
    "                        if(act_mon==mon):\n",
    "                            if(act_date==date or act_date==date_int):\n",
    "                                title.append(soup.find_all('title')[i].text)\n",
    "                                link.append(soup.find_all('guid')[i-1].text)\n",
    "                                scraped_date.append(pub_date)\n",
    "                                publish_date.append(pub_date)\n",
    "\n",
    "\n",
    "                # In[177]:\n",
    "            except:\n",
    "                print('Spanish not working')\n",
    "                not_working_functions.append(\"Spanish\")\n",
    "\n",
    "            import pandas as pd \n",
    "            df3 = pd.DataFrame(list(zip(title,link,publish_date,scraped_date)), \n",
    "                        columns =['title','link','publish_date','scraped_date'])\n",
    "            df3\n",
    "\n",
    "\n",
    "            # In[178]:\n",
    "\n",
    "\n",
    "            title=[]\n",
    "            link=[]\n",
    "            publish_date=[]\n",
    "            scraped_date=[]\n",
    "\n",
    "            try:\n",
    "                page=requests.get(urls[3])\n",
    "                soup=BeautifulSoup(page.content,features='lxml')\n",
    "                soup.find_all()\n",
    "\n",
    "\n",
    "                # In[179]:\n",
    "\n",
    "\n",
    "                for i in range(0,len(soup.find_all('lastBuildDate'))):\n",
    "                        pub_date=soup.find_all('pubdate')[i-1].text\n",
    "                #print(pub_date)\n",
    "                        pub_date=pub_date.split(' ')\n",
    "                        act_date=pub_date[1]\n",
    "                        act_mon=pub_date[2]\n",
    "                #print(act_date,act_mon)\n",
    "                        act_date=str(act_date)\n",
    "                        act_mon=str(act_mon)\n",
    "                #print(act_mon,mon)\n",
    "\n",
    "                        if(act_mon==mon):\n",
    "                            if(act_date==date or act_date==date_int):\n",
    "                                title.append(soup.find_all('title')[i].text)\n",
    "                                link.append(soup.find_all('guid')[i-1].text)\n",
    "                                scraped_date.append(pub_date)\n",
    "                                publish_date.append(pub_date)\n",
    "\n",
    "\n",
    "                # In[180]:\n",
    "            except:\n",
    "                print('Spanish not working')\n",
    "                not_working_functions.append((\"Spanish\"))\n",
    "\n",
    "            import pandas as pd \n",
    "            df4 = pd.DataFrame(list(zip(title,link,publish_date,scraped_date)), \n",
    "                        columns =['title','link','publish_date','scraped_date'])\n",
    "\n",
    "            frames = [df2,df3,df4]\n",
    "            fin=pd.concat(frames)\n",
    "            fin=fin.reset_index()\n",
    "\n",
    "            fin=fin.drop(['index'],axis=1)\n",
    "\n",
    "    #         get_ipython().system('pip install googletrans==3.1.0a0')\n",
    "            import sys\n",
    "\n",
    "\n",
    "            from googletrans import Translator\n",
    "            import googletrans\n",
    "\n",
    "\n",
    "            translator = Translator()\n",
    "            #translated=translator.translate(listtt[0][:2000],dest='en')\n",
    "\n",
    "\n",
    "            title=[]\n",
    "            link=[]\n",
    "            scraped_date=[]\n",
    "            publish_date=[]\n",
    "            for i in range(0,fin.shape[0]):\n",
    "                    translations=translator.translate(fin['title'][i])\n",
    "                    title.append(translations.text)\n",
    "                    link.append(fin['link'][i])\n",
    "                    scraped_date.append(fin['scraped_date'][i])\n",
    "                    publish_date.append(fin['publish_date'][i])\n",
    "            fin = pd.DataFrame(list(zip(title,link,publish_date,scraped_date)), \n",
    "                        columns =['title','link','publish_date','scraped_date'])\n",
    "            fin['publish_date']  = pd.to_datetime(fin['publish_date'],errors='coerce',utc=True).dt.strftime('%d-%b-%Y' \" \" \"%H:%M:%S\")\n",
    "            Fin = FilterFunction(fin)\n",
    "            emptydataframe(\"Spanish\",Fin)\n",
    "            Fin  = link_correction(Fin)\n",
    "            return Fin\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Russian():\n",
    "        try :\n",
    "            print(\"Russian\")\n",
    "            err_logs = []\n",
    "            url = \"https://ipo.einnews.com/search/IPO/?search%5B%5D=news&search%5B%5D=press&order=relevance\"\n",
    "            domain_url = \"https://ipo.einnews.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Russian : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"article-content\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.h3.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = Article(link)\n",
    "                    page.download()\n",
    "                    page.parse()\n",
    "                    title.append(page.title)  \n",
    "                  \n",
    "                    pub_date.append(page.publish_date)\n",
    "                    \n",
    "                    text.append(page.text)\n",
    "                    # print(text)\n",
    "\n",
    "                    #Scrapped date \n",
    "                    scraped_date.append(str(today))\n",
    "\n",
    "                    #Working links\n",
    "                    final_links.append(link)\n",
    "                except:\n",
    "                  continue\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Russian : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Russian\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Russian not working\")\n",
    "            not_working_functions.append('Russian')\n",
    "\n",
    "\n",
    "    def GoogleAlert1():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            text = []\n",
    "            try:\n",
    "                print('GoogleAlert1')\n",
    "                import html\n",
    "                url=\"https://www.google.com/alerts/feeds/01154643345605641334/12910713483086187784\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                        pdd=soup.find_all('entry')[i]\n",
    "                        for title in pdd.find_all('title'):\n",
    "                            articles.append(html.unescape(title.text))\n",
    "                        for content in pdd.find_all('content'):\n",
    "                            text.append(html.unescape(content.text))\n",
    "                        for link in pdd.find_all('link'):\n",
    "                            links.append(html.unescape(link['href']))\n",
    "                        for date in pdd.find_all('published'):\n",
    "                            dates.append(date.text)\n",
    "                            scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert is not working')\n",
    "                not_working_functions.append(\"Google alert 1\")\n",
    "            df_google = pd.DataFrame(list(zip(articles,text,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            Df2 = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google alert1\",Df2)\n",
    "            Df2  = link_correction(Df2)\n",
    "            return Df2\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def GoogleAlert2():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            text = []\n",
    "            try:\n",
    "                print('GoogleAlert2')\n",
    "                import html\n",
    "                url=\"https://www.google.com/alerts/feeds/01154643345605641334/6308064720496016673\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                        pdd=soup.find_all('entry')[i]\n",
    "                        for title in pdd.find_all('title'):\n",
    "                            articles.append(html.unescape(title.text))\n",
    "                        for content in pdd.find_all('content'):\n",
    "                            text.append(html.unescape(content.text))\n",
    "                        for link in pdd.find_all('link'):\n",
    "                            links.append(html.unescape(link['href']))\n",
    "                        for date in pdd.find_all('published'):\n",
    "                            dates.append(date.text)\n",
    "                            scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert2 is not working')\n",
    "                not_working_functions.append(\"Google Alert2\")\n",
    "            df_google = pd.DataFrame(list(zip(articles,text,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            Df2 = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google Alert 2\",Df2)\n",
    "            Df2  = link_correction(Df2)\n",
    "            return Df2\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def GoogleAlert3():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            text = []\n",
    "            try:\n",
    "                print('GoogleAlert3')\n",
    "                import html\n",
    "                url=\"https://www.google.com/alerts/feeds/01154643345605641334/13304767747222280933\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                        pdd=soup.find_all('entry')[i]\n",
    "                        for title in pdd.find_all('title'):\n",
    "                            articles.append(html.unescape(title.text))\n",
    "                        for content in pdd.find_all('content'):\n",
    "                            text.append(html.unescape(content.text))\n",
    "                        for link in pdd.find_all('link'):\n",
    "                            links.append(html.unescape(link['href']))\n",
    "                        for date in pdd.find_all('published'):\n",
    "                            dates.append(date.text)\n",
    "                            scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert 3 is not working')\n",
    "                not_working_functions.append(\"Google Alert 3\")\n",
    "            df_google = pd.DataFrame(list(zip(articles,text,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            Df2 = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google Alert 3\",Df2)\n",
    "            Df2  = link_correction(Df2)\n",
    "            return Df2\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def GoogleAlert4():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            text = []\n",
    "            try:\n",
    "                print('GoogleAlert4')\n",
    "                import html\n",
    "                url=\"https://www.google.com/alerts/feeds/01154643345605641334/3217985541207435755\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                        pdd=soup.find_all('entry')[i]\n",
    "                        for title in pdd.find_all('title'):\n",
    "                            articles.append(html.unescape(title.text))\n",
    "                        for content in pdd.find_all('content'):\n",
    "                            text.append(html.unescape(content.text))\n",
    "                        for link in pdd.find_all('link'):\n",
    "                            links.append(html.unescape(link['href']))\n",
    "                        for date in pdd.find_all('published'):\n",
    "                            dates.append(date.text)\n",
    "                            scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert4 is not working')\n",
    "                not_working_functions(\"Google Alert 4\")\n",
    "            df_google = pd.DataFrame(list(zip(articles,text,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            Df2 = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google Alert 4\",Df2)\n",
    "            Df2  = link_correction(Df2)\n",
    "            return Df2\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def GoogleAlert5():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            text = []\n",
    "            try:\n",
    "                print('GoogleAlert5')\n",
    "                import html\n",
    "                url=\"https://www.google.com/alerts/feeds/01154643345605641334/6882160862931884057\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                        pdd=soup.find_all('entry')[i]\n",
    "                        for title in pdd.find_all('title'):\n",
    "                            articles.append(html.unescape(title.text))\n",
    "                        for content in pdd.find_all('content'):\n",
    "                            text.append(html.unescape(content.text))\n",
    "                        for link in pdd.find_all('link'):\n",
    "                            links.append(html.unescape(link['href']))\n",
    "                        for date in pdd.find_all('published'):\n",
    "                            dates.append(date.text)\n",
    "                            scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert5 is not working')\n",
    "                not_working_functions.append(\"Google alert 5\")\n",
    "            df_google = pd.DataFrame(list(zip(articles,text,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            Df2 = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google Alert 5\",Df2)\n",
    "            df_google  = link_correction(df_google)\n",
    "            return df_google\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def IPOMonitor():\n",
    "        try:\n",
    "            links = []\n",
    "            title = []\n",
    "            dates = []\n",
    "            s_date = []\n",
    "            text = []\n",
    "            try:\n",
    "                print('IPOmonitor')\n",
    "                url = \"https://www.ipomonitor.com/pages/ipo-news.html\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,'html.parser')\n",
    "                for divtag in soup.find_all('span'):\n",
    "                    for a in divtag.find_all('a',href=True):\n",
    "\n",
    "                        links.append(a[\"href\"])\n",
    "                        title.append(a.text)\n",
    "                for i in soup.find_all('dd'):\n",
    "                    for k in i.find_all('span'):\n",
    "                        s = k.text\n",
    "                        dates.append(s)\n",
    "                        s_date.append(cur_date)\n",
    "                        text.append(' ')\n",
    "            except:\n",
    "                print('Ipo monitor not working')\n",
    "                not_working_functions.append(\"IPOMonitor\")\n",
    "            \n",
    "            df2 = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"IPO Monitor\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def Seenews():\n",
    "        try:\n",
    "            links = [] \n",
    "            title = []\n",
    "            dates = []\n",
    "            text = []\n",
    "            s_date = []\n",
    "            try:\n",
    "                print('Seenews')\n",
    "                urls = \"https://seenews.com/news/search_results/?keywords=ipo&order_by=name&order=asc&optradio=on&company_id=&company_owner=&capital_from=&capital_to=&total_assets_from=&total_assets_to=&total_revenue_from=&total_revenue_to=&number_of_employees_from=&number_of_employees_to=&net_profit_from=&net_profit_to=&net_loss_from=&net_loss_to=&seeci_from=&seeci_to=&ebitda_from=&ebitda_to=&year=&statement_type=\"\n",
    "                url1 = \"https://seenews.com\"\n",
    "                page=requests.get(urls)\n",
    "                soup=BeautifulSoup(page.content,'html')\n",
    "                divs = soup.find_all('dt',{'class':'search-result-title'})\n",
    "                for i in divs:\n",
    "                    links.append(url1 + i.a[\"href\"])\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"div\", { \"class\" : \"heading--content f-java\" })\n",
    "                    if x is not None:\n",
    "                        ementa = x.text\n",
    "                    else:\n",
    "                        ementa = None\n",
    "                    title.append(ementa)\n",
    "                    y=sp.find(\"div\", { \"class\" : \"post-date\" })\n",
    "                    if y is not None:\n",
    "                        d = y.text\n",
    "                        d = d.split()\n",
    "                        d = d[1:5]\n",
    "                        d = ' '.join(d)\n",
    "                    else:\n",
    "                        d = None\n",
    "                    dates.append(d)\n",
    "                    z=sp.find(\"div\", { \"class\" : \"content-description\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    text.append(n)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('Seenews not working')\n",
    "                not_working_functions.append(\"Seenews\")\n",
    "            df2 = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                    columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Seenews\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Bisnis():\n",
    "        try :\n",
    "            print(\"Bisnis\")\n",
    "            err_logs = []\n",
    "            url = \"https://search.bisnis.com/?q=IPO\"\n",
    "            domain_url = \"https://www.reuters.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Bisnis : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"col-sm-8\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Bisnis : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "            \n",
    "                #Title of article \n",
    "                if(soup.find(\"h1\" , {\"class\" : \"title-only\"}) == None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"title-only\"}).text)  \n",
    "                # print(title)\n",
    "\n",
    "                if(soup.find(\"div\" , {\"class\" :\"author\"}) == None):\n",
    "                    continue \n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"author\"}).span)\n",
    "                #Text of article\n",
    "                if(soup.find(\"div\" , {\"class\" : \"col-sm-10\"}) == None ) :\n",
    "                    continue \n",
    "                div = soup.find(\"div\" , {\"class\" : \"col-sm-10\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                \n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Bisnis : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df.head(10)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Bisnis\",df)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Bisnis not working\")\n",
    "            not_working_functions.append('bisnis')\n",
    "\n",
    "    \n",
    "    def RomaniaNew():\n",
    "        try:\n",
    "            try:\n",
    "                print('RomaniaNew')\n",
    "                urls = \"https://www.romania-insider.com/search/node?keys=ipo\"\n",
    "                links = [] \n",
    "                title = []\n",
    "                dates= []\n",
    "                s_date = []\n",
    "                text = []\n",
    "                page=requests.get(urls)\n",
    "                soup=BeautifulSoup(page.content,'html')\n",
    "                h = soup.find_all('h3',{'class':'search-result__title'})\n",
    "                for i in h:\n",
    "                    links.append(i.a[\"href\"])\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"h1\", {\"class\" : \"field field--name-field-title field--type-string field--label-hidden field__item\" })\n",
    "                    if x is not None:\n",
    "                        ementa = x.text\n",
    "                    else:\n",
    "                        ementa = None\n",
    "                    title.append(ementa)\n",
    "                    y=sp.find(\"div\", { \"class\" : \"field field--name-field-date field--type-datetime field--label-hidden field__item\" })\n",
    "                    if y is not None:\n",
    "                        d = y.text\n",
    "                    else:\n",
    "                        d = None\n",
    "                    dates.append(d)\n",
    "                    z=sp.find(\"div\", { \"class\" : \"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    text.append(n)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "                \n",
    "                df2 = pd.DataFrame({\"text\":text,\"link\":links,\"publish_date\":dates,\"scraped_date\":s_date,\"title\":title})\n",
    "                df2 = FilterFunction(df2)\n",
    "                emptydataframe(\"Romania New \",df2)\n",
    "                df2  = link_correction(df2)\n",
    "                return df2\n",
    "            except:\n",
    "                print('RomaniaNew not working')\n",
    "                not_working_functions.append(\"Romania New\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1   \n",
    "\n",
    "    def romania_insider():\n",
    "        try:\n",
    "            print(\"Romania Insider Dominician Republic\")\n",
    "            err_logs = []\n",
    "            err_index = []\n",
    "            baseSearchUrl = \"https://www.romania-insider.com/search/node?keys=ipo\"\n",
    "            domainUrl = \"https://www.romania-insider.com\"\n",
    "\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            queryUrl = baseSearchUrl\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                pageSource = requests.get(queryUrl, headers=headers)\n",
    "            except:\n",
    "                err = \"romania_insider: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "            # with open(\"response.html\", \"wb\") as f:\n",
    "            #     f.write(pageSource.content)\n",
    "            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "            for item in parsedSource.find(\"ol\", class_=\"search-results node_search-results\").find_all(\"li\"):\n",
    "                requiredTag = item.find(\"h3\").find(\"a\")\n",
    "                currentArticleTitle = requiredTag.text\n",
    "                # print(currentArticleTitle)\n",
    "                currentArticleLink = requiredTag[\"href\"]\n",
    "                # print(currentArticleLink)\n",
    "                if currentArticleLink[0] == \"/\":\n",
    "                    links.append(domainUrl + currentArticleLink)\n",
    "                else:\n",
    "                    links.append(currentArticleLink)\n",
    "                titles.append(currentArticleTitle)\n",
    "\n",
    "            scrapedData[\"title\"] = titles\n",
    "            scrapedData[\"link\"] = links\n",
    "            # print(titles)\n",
    "            # print(links)\n",
    "\n",
    "            # Article's date and description scraping\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            ArticleBody = []\n",
    "            for link in links:\n",
    "                articleText = \"\"\n",
    "                try:\n",
    "                    pageSource = requests.get(link, headers=headers)\n",
    "                    parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                except:\n",
    "                    err = \"romania_insider: err: Failed to access article link : \" + link\n",
    "                    ArticleDates.append(\"Error\")\n",
    "                    err_logs.append(err)\n",
    "                    err_index.append(link)\n",
    "                    continue\n",
    "                # with open(\"response.html\", \"wb\") as f:\n",
    "                #     f.write(pageSource.content)\n",
    "                # break\n",
    "                if parsedSource.find(\"div\", class_=\"field field--name-field-date field--type-datetime field--label-hidden field__item\"):\n",
    "                    sourceDateTimeTag = parsedSource.find(\"div\", class_=\"field field--name-field-date field--type-datetime field--label-hidden field__item\")\n",
    "                else:\n",
    "                    err = \"romania_insider: err: Failed to retrieve date from article : \" + link\n",
    "                    ArticleDates.append(\"Error\")\n",
    "                    err_logs.append(err)\n",
    "                    err_index.append(link)\n",
    "                    continue\n",
    "                sourceDateTime = datetime.strptime(sourceDateTimeTag.text, \"%d %B %Y\").strftime(\"%d-%m-%Y\")\n",
    "                # print(sourceDateTime)\n",
    "                ArticleDates.append(sourceDateTime)\n",
    "                ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                # print(ArticleDates)\n",
    "                # if parsedSource.find(\"div\", class_=\"entry-content\"):\n",
    "                #     textBodyDiv = parsedSource.find(\"div\", class_=\"entry-content\")\n",
    "                # else:\n",
    "                #     err = \"romania_insider: err: Failed to retrieve article text: \" + link\n",
    "                #     ArticleBody.append(\"Error\")\n",
    "                #     err_logs.append(err)\n",
    "                #     err_index.append(link)\n",
    "                #     continue\n",
    "                for item in parsedSource.find(\"div\", class_=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\").find_all(\"p\"):\n",
    "                    articleText += item.text.strip()\n",
    "                print(articleText)\n",
    "                ArticleBody.append(articleText)\n",
    "            scrapedData[\"publish_date\"] = ArticleDates\n",
    "            scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "            scrapedData[\"text\"] = ArticleBody\n",
    "            # print(ArticleBody)\n",
    "\n",
    "            # Clean and Normalize links\n",
    "            if len(err_index) != 0:\n",
    "                for e in err_index:\n",
    "                    idx = scrapedData[\"link\"].index(e)\n",
    "                    scrapedData[\"link\"].pop(idx)\n",
    "                    scrapedData[\"title\"].pop(idx)\n",
    "                    scrapedData[\"publish_date\"].pop(idx)\n",
    "\n",
    "            # DataFrame creation\n",
    "            romania_insiderDF = pd.DataFrame(scrapedData)\n",
    "            romania_insiderDF = romania_insiderDF.drop_duplicates(subset=[\"link\"])\n",
    "            if romania_insiderDF.empty:\n",
    "                err = \"romania_insider: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(romania_insiderDF)\n",
    "            emptydataframe(\"Romania Insider\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Romania Insider Dominic Republic not working\")\n",
    "            not_working_functions.append(\"Romania Insider Dominic Rep\")\n",
    "    def cnbc1():\n",
    "        try:\n",
    "            print(\"CNBC Barbados\")\n",
    "            err_logs = []\n",
    "            baseSearchUrl = \"https://www.cnbc.com/id/10000666/device/rss\"\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            err_index = []\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            ArticleBody = []\n",
    "            queryUrl = baseSearchUrl\n",
    "            try:\n",
    "                pageSource = requests.get(baseSearchUrl)\n",
    "            except:\n",
    "                err = \"cnbc: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                exit()\n",
    "            # with open(\"response.xml\", \"wb\") as f:\n",
    "            #     f.write(pageSource.content)\n",
    "            # break\n",
    "            parsedSource = BeautifulSoup(pageSource.content, \"xml\")\n",
    "            for eachItem in parsedSource.find_all(\"item\"):\n",
    "                currentArticleTitle = eachItem.find(\"title\").text\n",
    "                currentArticleLink = eachItem.find(\"link\").text\n",
    "                currentArticleDate = datetime.strptime(str(eachItem.find(\"pubDate\").text).split(\"GMT\", maxsplit=2)[0].strip(),\n",
    "                                                    \"%a, %d %b %Y %H:%M\").strftime(\"%d-%m-%Y\")\n",
    "                # articleText = str(eachItem.find(\"description\").text).split(\"CDATA[ \", maxsplit=2)[1].rstrip(\" ]]\")\n",
    "                articleText = str(eachItem.find(\"description\").text).split(\"CDATA[ \", maxsplit=2)[0].replace(\"#039;\", \"\")\n",
    "                ArticleBody.append(articleText)\n",
    "\n",
    "                titles.append(currentArticleTitle)\n",
    "                links.append(currentArticleLink)\n",
    "                ArticleDates.append(currentArticleDate)\n",
    "                ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "            scrapedData[\"title\"] = titles\n",
    "            scrapedData[\"link\"] = links\n",
    "            scrapedData[\"publish_date\"] = ArticleDates\n",
    "            scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "            scrapedData[\"text\"] = ArticleBody\n",
    "\n",
    "            # DataFrame creation\n",
    "            cnbcDF = pd.DataFrame(scrapedData)\n",
    "            cnbcDF = cnbcDF.drop_duplicates(subset=[\"link\"])\n",
    "            if cnbcDF.empty:\n",
    "                err = \"cnbc: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(cnbcDF)\n",
    "            emptydataframe(\"Cnbc barbados\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return cnbcDF        \n",
    "        except:\n",
    "            not_working_functions.append(\"CNBC Barbados\")\n",
    "            print(\"CNBC Barbados not working\")\n",
    "    def RomaniaInsider():\n",
    "        try:\n",
    "            try:\n",
    "                print('RomaniaInsider')\n",
    "                urls = \"https://www.romania-insider.com/index.php/daily-news/capital-markets?page=0\"\n",
    "                url1 = \"https://www.romania-insider.com\"\n",
    "\n",
    "                links = [] \n",
    "                title = []\n",
    "                dates= []\n",
    "                s_date = []\n",
    "                text = []\n",
    "                page=requests.get(urls)\n",
    "                soup=BeautifulSoup(page.content,'html')\n",
    "                h = soup.find_all('div',{'class':'field field--name-field-title field--type-string field--label-hidden field__item'})\n",
    "                for i in h:\n",
    "                    links.append(url1 + i.a[\"href\"])\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "\n",
    "                    x=sp.find(\"h1\", {\"class\" : \"field field--name-field-title field--type-string field--label-hidden field__item\" })\n",
    "                    if x is not None:\n",
    "                        ementa = x.text\n",
    "                    else:\n",
    "                        ementa = None\n",
    "                    title.append(ementa)\n",
    "                    y=sp.find(\"div\", { \"class\" : \"field field--name-field-date field--type-datetime field--label-hidden field__item\" })\n",
    "                    if y is not None:\n",
    "                        d = y.text\n",
    "                    else:\n",
    "                        d = None\n",
    "                    dates.append(d)\n",
    "                    z=sp.find(\"div\", { \"class\" : \"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    text.append(n)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('RomaniaInsider not working')\n",
    "                not_working_functions.append(\"RomaniaInsider\")\n",
    "\n",
    "            df2 = pd.DataFrame({\"text\":text,\"link\":links,\"publish_date\":dates,\"scraped_date\":s_date,\"title\":title})\n",
    "            df2 = FilterFunction(df2)\n",
    "            emptydataframe(\"RomaniaInsider\",df2)\n",
    "            df2  = link_correction(df2)\n",
    "            return df2\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def SpaceMoney():\n",
    "        try:\n",
    "            try:\n",
    "                print('spacemoney')\n",
    "                urls = \"https://www.spacemoney.com.br/noticias/ipos/\"\n",
    "                links = [] \n",
    "                title = []\n",
    "                dates = []\n",
    "                s_date = []\n",
    "                text = []\n",
    "                page=requests.get(urls)\n",
    "                soup=BeautifulSoup(page.content,'html')\n",
    "                divs = soup.find_all('div',{'class':'linkNoticia crop'})\n",
    "                for i in divs:\n",
    "                    links.append(i.a[\"href\"])\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "\n",
    "                    x=sp.find(\"h1\")\n",
    "                    if x is not None:\n",
    "                        ementa = x.text\n",
    "                    else:\n",
    "                        ementa = None\n",
    "                    title.append(ementa)\n",
    "                    y=sp.find(\"section\", { \"class\" : \"dataAutor\" })\n",
    "                    if y is not None:\n",
    "                        d = y.text\n",
    "                    else:\n",
    "                        d = None\n",
    "                    dates.append(d)\n",
    "                    z=sp.find(\"article\", { \"class\" : \"grid_8 alpha\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    text.append(n)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('Spacemoney not working')\n",
    "                not_working_functions.append(\"Spacemoney\")\n",
    "\n",
    "            df2 = pd.DataFrame({\"text\":text,\"link\":links,\"publish_date\":dates,\"scraped_date\":s_date,\"title\":title})\n",
    "            # df2 = FilterFunction(df2)\n",
    "            df = translate_dataframe(df2)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Spacemoney\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Carteira():\n",
    "        try:\n",
    "            # print(\"Carteira\")\n",
    "            body = [] \n",
    "            link = []\n",
    "            date = []\n",
    "            title=[]\n",
    "            # link=[]\n",
    "            pub_date=[]\n",
    "            scraped_date=[]\n",
    "            try:\n",
    "                print('Carteira')\n",
    "                urls = \"https://carteirasa.com.br/?s=Ipo\"\n",
    "                def get_links(urls):\n",
    "\n",
    "                        links = [] \n",
    "                        page=requests.get(urls)\n",
    "                        soup=BeautifulSoup(page.content,'html')\n",
    "                        divs = soup.find_all('div',{'class':'item-mid'})\n",
    "                        for i in divs:\n",
    "                            links.append(i.a[\"href\"])\n",
    "                        return links\n",
    "                \n",
    "                links1 = get_links(urls)\n",
    "                \n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(current_time)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "\n",
    "                df = pd.DataFrame({\"Article\":body,\"Link\":link,\"Publish Date\":date,\"Scrape Date\":scraped_date,\"Title\":title})\n",
    "\n",
    "                links1 = get_links(urls)\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(current_time)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "\n",
    "                df = pd.DataFrame({\"Article\":body,\"Link\":link,\"Publish Date\":date,\"Scrape Date\":scraped_date,\"Title\":title})\n",
    "\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(current_time)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "            except:\n",
    "                print('Carteira is not working')\n",
    "                not_working_functions.append(\"Carteira\")\n",
    "            dic = {\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title}\n",
    "\n",
    "            df = pd.DataFrame.from_dict(dic,orient='index')\n",
    "            df = df.transpose()\n",
    "            df.dropna(inplace=True)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Carteira\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def Kontan1():\n",
    "        try :\n",
    "            print(\"Kontan1\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.kontan.co.id/search/?search=ipo&Button_search=\"\n",
    "            domain_url = \"https://www.kontan.co.id\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Kontan1 : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"sp-hl linkto-black\"})\n",
    "            for div in all_divs:\n",
    "                links.append(\"https:\"+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Kontan1 : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                if(soup.find(\"h1\" , {\"class\" : \"detail-desk\"})== None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"detail-desk\"}).text)\n",
    "                print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"fs14 ff-opensans font-gray\"}))\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"tmpt-desk-kon\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":scraped_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Kontan1 : err: Empty datKontan1ame\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Kontan1\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Kontan1 not working\")\n",
    "            not_working_functions.append('Kontan1')\n",
    "    def euronews():\n",
    "        try:\n",
    "            print(\"Euronews\")\n",
    "            try:\n",
    "                urls = \"https://www.euronews.com/search?query=ipo\"\n",
    "                def get_links(urls):\n",
    "                    links = [] \n",
    "                    page=requests.get(urls)\n",
    "                    soup=BeautifulSoup(page.content,'html')\n",
    "                    divs = soup.find_all('h3',{'class':'m-object__title qa-article-title'})\n",
    "                    for i in divs:\n",
    "                        links.append(i.a[\"href\"])\n",
    "                    return links\n",
    "                title=[]\n",
    "                pub_date=[]\n",
    "                scraped_date=[]\n",
    "                links1 = get_links(urls)\n",
    "                body = [] \n",
    "                link = []\n",
    "                date = []\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                for ur in links1:\n",
    "                #   print(ur)\n",
    "                        ur='https://www.euronews.com'+ur\n",
    "                        try:\n",
    "                            article = Article(ur)\n",
    "                            article.download()\n",
    "                            article.parse()\n",
    "                            # print(article.text)\n",
    "\n",
    "                            link.append(ur)\n",
    "                            scraped_date.append(now)\n",
    "                            body.append(article.text)\n",
    "                            title.append(article.title)\n",
    "                            date.append(article.publish_date)\n",
    "\n",
    "                        except:\n",
    "                            continue\n",
    "                        time.sleep(0.5)\n",
    "            except:\n",
    "                    print(\"Euronews not working\")\n",
    "                    not_working_functions.append(\"Euronews\")\n",
    "            df2 = pd.DataFrame({\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            \n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Euronews\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def franchdailynews():\n",
    "        try:\n",
    "            print(\"Franchdailynews\")\n",
    "            try:\n",
    "                urls = \"https://frenchdailynews.com/?s=ipo\"\n",
    "                def get_links(urls):\n",
    "                    links = [] \n",
    "                    page=requests.get(urls)\n",
    "                    print(page)\n",
    "                    soup=BeautifulSoup(page.content,'html')\n",
    "                    divs = soup.find_all('h2',{'class':'entry-title'})\n",
    "                    for i in divs:\n",
    "            #         print('hi')\n",
    "                        links.append(i.a[\"href\"])\n",
    "                    return links\n",
    "                title=[]\n",
    "                link=[]\n",
    "                pub_date=[]\n",
    "                scraped_date=[]\n",
    "                links1 = get_links(urls)\n",
    "                body = [] \n",
    "                link = []\n",
    "                date = []\n",
    "                from datetime import datetime\n",
    "                now = datetime.now()\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "            #       print(ur)\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(now)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "            except:\n",
    "                    print(\"franchdailynew not working\")\n",
    "                    not_working_functions.append(\"French Daily news\")\n",
    "            df2 = pd.DataFrame({\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"French daily news \" ,df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "   \n",
    "    def norway():\n",
    "        try:\n",
    "            print(\"Norway\")\n",
    "            try:\n",
    "                urls = \"https://www.thelocal.no/?s=ipo\"\n",
    "                def get_links(urls):\n",
    "\n",
    "                    links = [] \n",
    "                    page=requests.get(urls)\n",
    "            #       print(page)\n",
    "                    soup=BeautifulSoup(page.content,'html')\n",
    "                    divs = soup.find_all('div',{'class':'article-search-title'})\n",
    "                    for i in divs:\n",
    "                #         print('hi')\n",
    "                        links.append(i.a[\"href\"])\n",
    "                    return links\n",
    "                title=[]\n",
    "                link=[]\n",
    "                pub_date=[]\n",
    "                scraped_date=[]\n",
    "                links1 = get_links(urls)\n",
    "                body = [] \n",
    "                link = []\n",
    "                date = []\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "            #       print(ur)\n",
    "\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(current_time)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "            except:\n",
    "                print(\"Normay is not working\")\n",
    "                not_working_functions.append(\"Norway\")\n",
    "            df2 = pd.DataFrame({\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Norway\",df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def localde():\n",
    "        try:\n",
    "            print(\"localde\")\n",
    "            try:\n",
    "                urls = \"https://www.thelocal.de/?s=ipo\"\n",
    "                def get_links(urls):\n",
    "\n",
    "                    links = [] \n",
    "                    page=requests.get(urls)\n",
    "                #       print(page)\n",
    "                    soup=BeautifulSoup(page.content,'html')\n",
    "                    divs = soup.find_all('div',{'class':'article-search-title'})\n",
    "                    for i in divs:\n",
    "                #         print('hi')\n",
    "                        links.append(i.a[\"href\"])\n",
    "                    return links\n",
    "                title=[]\n",
    "                link=[]\n",
    "                pub_date=[]\n",
    "                scraped_date=[]\n",
    "                links1 = get_links(urls)\n",
    "                body = [] \n",
    "                link = []\n",
    "                date = []\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "            #       print(ur)\n",
    "\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(now)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "            except:\n",
    "                    print(\"localde not working\")\n",
    "                    not_working_functions.append(\"localde\")\n",
    "            df2 = pd.DataFrame({\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"localde\",df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def chinatoday():\n",
    "        try:\n",
    "            print(\"chinatoday\")\n",
    "            try:\n",
    "                urls = \"http://www.chinatoday.com/inv/a.htm\"\n",
    "                def get_links(urls):\n",
    "\n",
    "                    links = [] \n",
    "                    page=requests.get(urls)\n",
    "                    #   print(page)\n",
    "                    soup=BeautifulSoup(page.content,'html')\n",
    "                    divs = soup.find_all('li')\n",
    "                    for i in divs:\n",
    "                    #     print('hi')\n",
    "                        try:\n",
    "                            links.append(i.a[\"href\"])\n",
    "                        except:\n",
    "                            pass\n",
    "                    return links\n",
    "                title=[]\n",
    "                link=[]\n",
    "                pub_date=[]\n",
    "                scraped_date=[]\n",
    "                links1 = get_links(urls)\n",
    "                body = [] \n",
    "                link = []\n",
    "                date = []\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "            #       print(ur)\n",
    "\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(now)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "            except:\n",
    "                    print(\"chinatoday not working\")\n",
    "                    not_working_functions.append(\"Chinatoday\")\n",
    "            df2 = pd.DataFrame({\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Chinatoday\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def Koreatimes():\n",
    "        try :\n",
    "            print(\"Koreatimes\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.koreatimes.co.kr/www2/common/search.asp?kwd=IPO\"\n",
    "            domain_url = \"https://www.kontan.co.id\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Koreatimes : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"list_article_headline HD\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Koreatimes : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                if (soup.find(\"div\" , {\"class\" : \"view_headline HD\"}) == None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"view_headline HD\"}).text)  \n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find_all(\"div\" , {\"class\" :\"view_date\"})[0].text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Title of article \n",
    "                \n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"view_article\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"span\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            \n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Koreatimes : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Koreatimes\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Koreatimes not working\")\n",
    "            not_working_functions.append('Koreatimes')\n",
    "\n",
    " \n",
    "        \n",
    "  \n",
    "  # TODO: commented out for testing.  Uncomment later\n",
    "  # CombineFunction()\n",
    "\n",
    "\n",
    "  \n",
    "    def zdnet():\n",
    "        try :\n",
    "            print(\"zdnet\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.zdnet.com/search/?q=IPO\"\n",
    "            domain_url = \"https://www.zdnet.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"zdnet : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"article\",{\"class\":\"item\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"zdnet : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"header\" , {\"class\" : \"storyHeader precap-variation article\"}).h1.text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"byline-details\"}).time.text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"storyBody\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"zdnet : err: Empty datframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"zdnet\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"zdnet not working\")\n",
    "            not_working_functions.append('zdnet')\n",
    "\n",
    "    def arabNews():\n",
    "        try:\n",
    "            print(\"Arabnews\")\n",
    "            err_logs = []\n",
    "            err_index = []\n",
    "            baseSearchUrl = \"https://www.arabnews.com/search/site/\"\n",
    "            domainUrl = \"https://www.arabnews.com\"\n",
    "            keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "            # use this for faster testing\n",
    "            tkeywords = [\"IPO\"]\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            for keyword in tkeywords:\n",
    "                queryUrl = baseSearchUrl + keyword\n",
    "                try:\n",
    "                    headers = {\n",
    "                        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                    pageSource = requests.get(queryUrl, headers=headers)\n",
    "                    parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                except:\n",
    "                    err = \"arabNews: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                # with open(\"response.html\", \"wb\") as f:\n",
    "                #     f.write(pageSource.content)\n",
    "                # break\n",
    "                for item in parsedSource.find_all(\"div\", class_=\"article-item-title\"):\n",
    "                    requiredTag = item.find(\"h4\").find(\"a\")\n",
    "                    currentArticleTitle = requiredTag.text\n",
    "                    # print(currentArticleTitle)\n",
    "                    currentArticleLink = requiredTag[\"href\"]\n",
    "                    # print(currentArticleLink)\n",
    "                    if currentArticleLink[0] == \"/\":\n",
    "                        links.append(domainUrl + currentArticleLink)\n",
    "                    else:\n",
    "                        links.append(currentArticleLink)\n",
    "                    titles.append(currentArticleTitle)\n",
    "\n",
    "                scrapedData[\"title\"] = titles\n",
    "                scrapedData[\"link\"] = links\n",
    "                # print(titles)\n",
    "                # print(links)\n",
    "                #\n",
    "                # Article's date and description scraping\n",
    "                ArticleDates = []\n",
    "                ScrapeDates = []\n",
    "                ArticleBody = []\n",
    "                for link in links:\n",
    "                    articleText = \"\"\n",
    "                    try:\n",
    "                        pageSource = requests.get(link, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                    except:\n",
    "                        err = \"arabNews: err: Failed to access article link : \" + link\n",
    "                        ArticleDates.append(\"Error\")\n",
    "                        err_logs.append(err)\n",
    "                        err_index.append(link)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "                    if parsedSource.find(\"div\", class_=\"entry-date\"):\n",
    "                        sourceDateTimeTag = parsedSource.find(\"div\", class_=\"entry-date\").find(\"time\")\n",
    "                    else:\n",
    "                        err = \"arabNews: err: Failed to retrieve date from article : \" + link\n",
    "                        ArticleDates.append(\"Error\")\n",
    "                        err_logs.append(err)\n",
    "                        err_index.append(link)\n",
    "                        continue\n",
    "                    sourceDateTime = datetime.strptime(sourceDateTimeTag.text, \"%B %d, %Y %H:%M\").strftime(\"%d-%m-%Y\")\n",
    "                    # print(sourceDateTime)\n",
    "                    ArticleDates.append(sourceDateTime)\n",
    "                    ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    # print(ArticleDates)\n",
    "                    if parsedSource.find(\"div\", class_=\"entry-content\"):\n",
    "                        textBodyDiv = parsedSource.find(\"div\", class_=\"entry-content\")\n",
    "                    else:\n",
    "                        err = \"arabNews: err: Failed to retrieve article text: \" + link\n",
    "                        ArticleBody.append(\"Error\")\n",
    "                        err_logs.append(err)\n",
    "                        err_index.append(link)\n",
    "                        continue\n",
    "                    for item in textBodyDiv.find_all(\"p\"):\n",
    "                        articleText += item.text.strip()\n",
    "                    ArticleBody.append(articleText)\n",
    "                scrapedData[\"publish_date\"] = ArticleDates\n",
    "                scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                scrapedData[\"text\"] = ArticleBody\n",
    "                # print(ArticleBody)\n",
    "\n",
    "            # Clean and Normalize links\n",
    "            if len(err_index) != 0:\n",
    "                for e in err_index:\n",
    "                    idx = scrapedData[\"link\"].index(e)\n",
    "                    scrapedData[\"link\"].pop(idx)\n",
    "                    scrapedData[\"title\"].pop(idx)\n",
    "                    scrapedData[\"publish_date\"].pop(idx)\n",
    "\n",
    "            # DataFrame creation\n",
    "            arabNewsDF = pd.DataFrame(scrapedData)\n",
    "            arabNewsDF = arabNewsDF.drop_duplicates(subset=[\"link\"])\n",
    "            if arabNewsDF.empty:\n",
    "                err = \"arabNews: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(arabNewsDF)\n",
    "            emptydataframe(\"Arabnews\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Arabnews not working\")\n",
    "            not_working_functions.append(\"Arabnews\")\n",
    "    def chosun():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Chosun\")\n",
    "                err_logs = []\n",
    "\n",
    "                # baseSearchUrl = f\"http://english.chosun.com/svc/list_in/search.html?query={keyword}&sort=1&catid=\"\n",
    "                domainUrl = \"https://english.chosun.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = f\"http://english.chosun.com/svc/list_in/search.html?query={keyword}&sort=1&catid=\"\n",
    "                    try:\n",
    "                        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                        pageSource = requests.get(queryUrl, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                    except:\n",
    "                        err = \"chosun: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "                    for item in parsedSource.find_all(\"dl\", class_=\"list_item\"):\n",
    "                        requiredTag = item.find(\"a\")\n",
    "                        currentArticleTitle = requiredTag.text\n",
    "                        currentArticleLink = requiredTag[\"href\"]\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                # print(titles)\n",
    "                # print(links)\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleDates = []\n",
    "                    ScrapeDates = []\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                        except:\n",
    "                            err = \"chosun: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        # with open(\"response.html\", \"wb\") as f:\n",
    "                        #     f.write(pageSource.content)\n",
    "                        # break\n",
    "                        try:\n",
    "                            sourceDateTimeTag = parsedSource.find(\"p\", id=\"date_text\")\n",
    "                        except:\n",
    "                            err = \"chosun: err: Failed to retrieve date from article : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        sourceDateTime = sourceDateTimeTag.text.strip()\n",
    "                        sourceDateTime = datetime.strptime(sourceDateTime, \"%B %d, %Y %H:%M\").strftime(\"%d-%m-%Y\")\n",
    "                        ArticleDates.append(sourceDateTime)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    # print(ArticleDates)\n",
    "                        try:\n",
    "                            textBodyDiv = parsedSource.find(\"div\", class_=\"par\")\n",
    "                        except:\n",
    "                            err = \"chosun: err: Failed to retrieve article text: \" + link\n",
    "                            ArticleBody.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        for item in textBodyDiv.find_all(\"p\"):\n",
    "                            articleText = textBodyDiv.text.replace(item.text, \"\")\n",
    "                        # print(articleText)\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "                    # print(ArticleBody)\n",
    "\n",
    "                # Clean data\n",
    "                if len(err_index) != 0:\n",
    "                    for index in err_index:\n",
    "                        del scrapedData[\"title\"][index]\n",
    "                        del scrapedData[\"link\"][index]\n",
    "                        if scrapedData[\"publish_date\"][index] == \"Error\":\n",
    "                            del scrapedData[\"publish_date\"][index]\n",
    "                        if scrapedData[\"article\"][index] == \"Error\":\n",
    "                            del scrapedData[\"article\"][index]\n",
    "\n",
    "                # DataFrame creation\n",
    "                chosunDF = pd.DataFrame(scrapedData)\n",
    "                if chosunDF.empty:\n",
    "                    err = \"Chosun news : err : Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(chosunDF)\n",
    "                emptydataframe(\"Chosun\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Chosun not working\")\n",
    "                not_working_functions.append(\"Chosun\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def Forbes():\n",
    "        try :\n",
    "            print(\"Forbes\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.forbes.com/search/?q=IPO&sh=8e278c4279f4\"\n",
    "            domain_url = \"https://www.forbes.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Forbes : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"stream-item__title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Forbes : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "\n",
    "                if(soup.find(\"div\" , {\"class\" :\"metrics-channel light-text with-border\"}) == None):\n",
    "                    continue\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"metrics-channel light-text with-border\"}).time.text)\n",
    "            \n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"fs-headline speakable-headline font-base font-size should-redesign\"}))  \n",
    "                # print(title)\n",
    "            \n",
    "            \n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"article-body fs-article fs-responsive-text current-article\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            # print(len(text) , len(final_links))\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Forbes : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Forbes\",df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Forbes not working\")\n",
    "            not_working_functions.append('Forbes')\n",
    "    def kedg():\n",
    "        try :\n",
    "            print(\"kedg\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.kedglobal.com/newsSearch?keyword=IPO\"\n",
    "            domain_url = \"https://www.kedglobal.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"kedg : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"box\"})\n",
    "            for div in all_divs:\n",
    "                try :\n",
    "                    links.append(domain_url+div.a[\"href\"])\n",
    "                except : \n",
    "                    continue\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"kedg : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                try:\n",
    "                    if not (soup.find(\"p\" , {\"class\" :\"update_time\"})):\n",
    "                        continue\n",
    "                    if not (soup.find(\"h1\" , {\"class\" : \"tit\"})):\n",
    "                        continue\n",
    "                    if not (soup.find(\"div\" , {\"class\" : \"cont\"})):\n",
    "                        continue\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "\n",
    "                #Working links\n",
    "                except :\n",
    "                    continue \n",
    "                pub_date.append(soup.find(\"p\" , {\"class\" :\"update_time\"}).text)\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"tit\"}).text)\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"cont\"}).text)\n",
    "                scraped_date.append(str(today))\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"kedg : err: Empty datframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"kedg\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"kedg not working\")\n",
    "            not_working_functions.append('kedg')\n",
    "    def kngnet():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"kngnet\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.koreanewsgazette.com/?s=\"\n",
    "                domainUrl = \"https://www.koreanewsgazette.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        pageSource = requests.get(queryUrl)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\")\n",
    "                    except:\n",
    "                        err = \"kng: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    for item in parsedSource.find_all(\"h2\", class_=\"entry-title\"):\n",
    "                        # requiredTag = item.find(\"h2\")\n",
    "                        currentArticleLink = item.a[\"href\"]\n",
    "                        currentArticleTitle = item.text\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                    print(links)\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleDates = []\n",
    "                    ScrapeDates = []\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                      page = Article(link)\n",
    "                      page.download()\n",
    "                      page.parse()\n",
    "                      ArticleDates.append(page.publish_date)\n",
    "                      ArticleBody.append(page.text)\n",
    "                      ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "\n",
    "                # Clean data\n",
    "                if len(err_index) != 0:\n",
    "                    for index in err_index:\n",
    "                        del scrapedData[\"title\"][index]\n",
    "                        del scrapedData[\"link\"][index]\n",
    "                        if scrapedData[\"publish_date\"][index] == \"Error\":\n",
    "                            del scrapedData[\"publish_date\"][index]\n",
    "                        if scrapedData[\"article\"][index] == \"Error\":\n",
    "                            del scrapedData[\"article\"][index]\n",
    "\n",
    "                # DataFrame creation\n",
    "                kngDF = pd.DataFrame(scrapedData)\n",
    "                if kngDF.empty:\n",
    "                    err = \"Kngnet news : err : Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(kngDF)\n",
    "                emptydataframe(\"Kngnet news\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Kngnet news is not working\")\n",
    "                not_working_functions.append(\"Kngnet news\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def pymnts():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Pymnts\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.pymnts.com/?s=\"\n",
    "                domainUrl = \"https://www.pymnts.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                        pageSource = requests.get(queryUrl, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\")\n",
    "                    except:\n",
    "                        err = \"pymnts: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    for item in parsedSource.find_all(\"li\", class_=\"infinite-post\"):\n",
    "                        requiredTag = item.find(\"a\")\n",
    "                        currentArticleTitle = requiredTag[\"title\"]\n",
    "                        currentArticleLink = requiredTag[\"href\"]\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleDates = []\n",
    "                    ScrapeDates = []\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\")\n",
    "                        except:\n",
    "                            err = \"pymnts: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        try:\n",
    "                            sourceDateTime = parsedSource.find(\"time\", class_=\"post-date updated\")\n",
    "                        except:\n",
    "                            err = \"pymnts: err: Failed to retrieve date from article : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        sourceDateTime = datetime.strptime(sourceDateTime[\"datetime\"], \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "                        ArticleDates.append(sourceDateTime)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                        try:\n",
    "                            textBodyDiv = parsedSource.find(\"div\", id=re.compile(\"pymnts-content\"))\n",
    "                        except:\n",
    "                            err = \"pymnts: err: Failed to retrieve article text: \" + link\n",
    "                            ArticleBody.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        for item in textBodyDiv.find_all(\"p\"):\n",
    "                            articleText += item.text\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "\n",
    "                # Clean data\n",
    "                if len(err_index) != 0:\n",
    "                    for index in err_index:\n",
    "                        del scrapedData[\"title\"][index]\n",
    "                        del scrapedData[\"link\"][index]\n",
    "                        if scrapedData[\"publish_date\"][index] == \"Error\":\n",
    "                            del scrapedData[\"publish_date\"][index]\n",
    "                        if scrapedData[\"article\"][index] == \"Error\":\n",
    "                            del scrapedData[\"article\"][index]\n",
    "\n",
    "                # DataFrame creation\n",
    "                pymntsDF = pd.DataFrame(scrapedData)\n",
    "                df = FilterFunction(pymntsDF)\n",
    "                emptydataframe(\"Pymnts\",df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Pymnts not working\")\n",
    "                not_working_functions.append(\"Pymnts\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def toi():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Toi\")\n",
    "                err_logs = []  # Access to view error logs\n",
    "                url = \"https://timesofindia.indiatimes.com/topic/pre-ipo/news\"\n",
    "                domain_url = \"https://timesofindia.indiatimes.com\"\n",
    "\n",
    "                try:\n",
    "                    page = requests.get(url)\n",
    "                    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                except:\n",
    "                    err = \"toi: err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    return None\n",
    "\n",
    "                # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "                div_class = \"Mc7GB\"\n",
    "                h1_class = \"_1Y-96\"\n",
    "                date_div_class = \"yYIu- byline\"\n",
    "                para_div_class = \"_3YYSt\"\n",
    "\n",
    "                links = []\n",
    "\n",
    "                for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                    for a in divtag.find_all(\"a\", href=True):\n",
    "                        link = a[\"href\"]  # Gets the link\n",
    "\n",
    "                    # Checking the link if it is a relative link\n",
    "                        if link[0] == '/':\n",
    "                                link = domain_url + link\n",
    "\n",
    "                        links.append(link)\n",
    "\n",
    "                collection = []\n",
    "\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        l_page = requests.get(link)\n",
    "                        l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                    except:\n",
    "                        err = \"toi: err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    data = []\n",
    "                    # Scraping the heading\n",
    "                    h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                    try:\n",
    "                        data.append(h1_ele.text)\n",
    "                    except:\n",
    "                        err = \"toi: err: Failed to find title in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding the link to data\n",
    "                    data.append(link)\n",
    "\n",
    "                    # Scraping the published date\n",
    "                    date_ele = l_soup.find(\"div\", {\"class\": date_div_class})\n",
    "                    try:\n",
    "                        date_text = date_ele.text\n",
    "                        date_text = (date_text.split('/'))[-1]\n",
    "                        date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                        data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "                    except:\n",
    "                        err = \"toi: err: Failed to find date in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding the scraped date to data\n",
    "                    cur_date = str(datetime.today())\n",
    "                    data.append(cur_date)\n",
    "\n",
    "                    # Scraping the paragraph\n",
    "                    para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "                    try:\n",
    "                        data.append(para_ele.text)  # Need to make this better\n",
    "                    except:\n",
    "                        err = \"toi: err: Failed to find paragraph in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding data to the collection\n",
    "                    collection.append(data)\n",
    "\n",
    "                df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "                if df.empty:\n",
    "                        err = \"Arab news : err : Empty dataframe\"\n",
    "                        err_logs.append(err)\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(df)\n",
    "                emptydataframe(\"Toi\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Toi not working \")\n",
    "                not_working_functions.append(\"Toi\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def wealthx():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Wealthx\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.wealthx.com/?s=\"\n",
    "                domainUrl = \"https://www.wealthx.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                        pageSource = requests.get(queryUrl, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                    except:\n",
    "                        err = \"wealthx: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "                    for item in parsedSource.find_all(\"article\", class_=\"result\"):\n",
    "                        requiredTag = item.find(\"h2\", class_=\"title\").find(\"a\")\n",
    "                        currentArticleTitle = requiredTag.text\n",
    "                        currentArticleLink = requiredTag[\"href\"]\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                # print(titles)\n",
    "                # print(links)\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleDates = []\n",
    "                    ScrapeDates = []\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                        except:\n",
    "                            err = \"wealthx: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        # with open(\"response.html\", \"wb\") as f:\n",
    "                        #     f.write(pageSource.content)\n",
    "                        # break\n",
    "                        try:\n",
    "                            sourceDateTimeTag = parsedSource.find(\"span\", class_=\"meta-date date updated\")\n",
    "                        except:\n",
    "                            err = \"wealthx: err: Failed to retrieve date from article : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        sourceDateTime = datetime.strptime(sourceDateTimeTag.text, \"%B %d, %Y\").strftime(\"%d-%m-%Y\")\n",
    "                        ArticleDates.append(sourceDateTime)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    # print(ArticleDates)\n",
    "                        try:\n",
    "                            textBodyDiv = parsedSource.find(\"div\", class_=\"content-inner\")\n",
    "                        except:\n",
    "                            err = \"wealthx: err: Failed to retrieve article text: \" + link\n",
    "                            ArticleBody.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        for item in textBodyDiv.find_all(\"p\"):\n",
    "                            articleText += item.text.strip()\n",
    "                        # print(articleText)\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "                    # print(ArticleBody)\n",
    "\n",
    "                # Clean data\n",
    "                if len(err_index) != 0:\n",
    "                    for index in err_index:\n",
    "                        del scrapedData[\"title\"][index]\n",
    "                        del scrapedData[\"link\"][index]\n",
    "                        if scrapedData[\"publish_date\"][index] == \"Error\":\n",
    "                            del scrapedData[\"publish_date\"][index]\n",
    "                        if scrapedData[\"article\"][index] == \"Error\":\n",
    "                            del scrapedData[\"article\"][index]\n",
    "\n",
    "                # DataFrame creation\n",
    "                wealthxDF = pd.DataFrame(scrapedData)\n",
    "                if wealthxDF.empty:\n",
    "                    err = \"Wealthx news : err : Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(wealthxDF)\n",
    "                emptydataframe(\"WealthX\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Wealth x not working\")\n",
    "                not_working_functions.append(\"Wealth x\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def AFR():\n",
    "        try :\n",
    "            print(\"AFR\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.afr.com/search?text=ipo\"\n",
    "            domain_url = \"https://www.afr.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"AFR : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"_20-Rx\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"AFR : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                # if(soup.find(\"h1\" , {\"class\" : \"primary-font__PrimaryFontStyles-o56yd5-0 dEdODy headline\"}) == None):\n",
    "                #   continue\n",
    "                if(soup.find(\"h1\" , {\"class\" : \"_3lFzE\"})== None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"_3lFzE\"}))\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"_2cdD4\"}).time.text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"tl7wu\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"AFR : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            # df = FilterFunction(df)\n",
    "            # emptydataframe(\"AFR\",df)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"AFR not working\")\n",
    "            not_working_functions.append('AFR')\n",
    "    def indonesia():\n",
    "\n",
    "        try :\n",
    "            print(\"Antara\")\n",
    "            err_logs = []\n",
    "            url = \"https://en.antaranews.com/search?q=ipo\"\n",
    "            domain_url = \"https://en.antaranews.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Indonesia : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"article\",{\"class\":\"simple-post simple-big clearfix\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Indonesia : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "\n",
    "                # if(soup.find(\"div\" , {\"class\" :\"metrics-channel light-text with-border\"}) == None):\n",
    "                #   continue\n",
    "                pub_date.append(soup.find(\"i\" , {\"class\" :\"fa fa-clock-o\"}).text)\n",
    "            \n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"post-title\"}).text)  \n",
    "                # print(title)\n",
    "            \n",
    "            \n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"post-content clearfix\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"div\"):\n",
    "                # if(i != None):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                \n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            # print(len(text) , len(final_links))\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Indonesia : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"indonesia\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Indonesia not working\")\n",
    "            not_working_functions.append('Indonesia')\n",
    "            \n",
    "    def asiainsurancereview():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Asia Insurance\")\n",
    "                err_logs = []  # Access to view error logs\n",
    "                url = \"https://www.asiainsurancereview.com/Search?search_key=IPO\"\n",
    "                domain_url = \"https://www.asiainsurancereview.com\"\n",
    "\n",
    "                try:\n",
    "                    page = requests.get(url)\n",
    "                    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                except:\n",
    "                    err = \"asiainsurancereview: err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    return None\n",
    "\n",
    "                # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "                div_class = \"items\"\n",
    "                h1_class = \"main-title\"\n",
    "                date_div_class = \"title-right\"\n",
    "                para_div_class = \"article-wrap\"\n",
    "\n",
    "                links = []\n",
    "\n",
    "                for divtag in soup.find_all(\"li\", {\"class\": div_class}):\n",
    "                    for a in divtag.find_all(\"a\", href=True):\n",
    "                        link = a[\"href\"]  # Gets the link\n",
    "\n",
    "                        # Checking the link if it is a relative link\n",
    "                        if link[0] == '/':\n",
    "                            link = domain_url + link\n",
    "\n",
    "                        links.append(link)\n",
    "\n",
    "                collection = []\n",
    "\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        l_page = requests.get(link)\n",
    "                        l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                    except:\n",
    "                        err = \"asiainsurancereview: err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    data = []\n",
    "                    # Scraping the heading\n",
    "                    h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                    try:\n",
    "                        data.append(h1_ele.text)\n",
    "                    except:\n",
    "                        err = \"asiainsurancereview: err: Failed to find title in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding the link to data\n",
    "                    data.append(link)\n",
    "\n",
    "                    # Scraping the published date\n",
    "                    date_ele = l_soup.find(\"span\", {\"class\": date_div_class})\n",
    "                    try:\n",
    "                        date_text = date_ele.text\n",
    "                        date_text = (date_text.split('/'))[-1]\n",
    "                        date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                        data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "                    except:\n",
    "                        err = \"asiainsurancereview: err: Failed to find date in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding the scraped date to data\n",
    "                    cur_date = str(datetime.today())\n",
    "                    data.append(cur_date)\n",
    "\n",
    "                    # Scraping the paragraph\n",
    "                    para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "                    try:\n",
    "                        data.append(para_ele.text)  # Need to make this better\n",
    "                    except:\n",
    "                        err = \"asiainsurancereview: err: Failed to find paragraph in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding data to the collection\n",
    "                    collection.append(data)\n",
    "\n",
    "                df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "                if df.empty:\n",
    "                    print(\"Antara indonesia review : err : Empty dataframe\")\n",
    "                #     err_logs.append(err)\n",
    "                # log_errors(err_logs)\n",
    "                df = FilterFunction(df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Asia insurance not working\")\n",
    "                not_working_functions.append(\"Asia Insurance\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def economic_times():\n",
    "        try :\n",
    "            print(\"economic_times\")\n",
    "            err_logs = []\n",
    "            url = \"https://economictimes.indiatimes.com/markets/ipo\"\n",
    "            domain_url = \"https://economictimes.indiatimes.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"economic_times : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"wrapLines l1\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"economic_times : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"artTitle font_faus\"}).text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\" , {\"class\" :\"jsdtTime\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                # div = soup.find(\"div\" , {\"class\" : \"storyBody\"})\n",
    "                # t = \"\"\n",
    "                # for i in div.find_all(\"p\"):\n",
    "                #   t += i.text + \" \"\n",
    "                # text.append(t)\n",
    "                text.append(soup.find(\"div\" , {\"class\" :\"pageContent flt\"}).text)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"economic_times : err: Empty datframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"economic_times\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"economic_times not working\")\n",
    "            not_working_functions.append('economic_times')\n",
    "    def prnewswire():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"prnnewswire\")\n",
    "                err_logs = []\n",
    "                url = \"https://www.prnewswire.com/search/news/?keyword=pre%20ipo\"\n",
    "                domain_url = \"https://www.prnewswire.com/\"\n",
    "\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "                # Class names of the elements to be scraped\n",
    "                div_class = \"card\"\n",
    "                #h1_class = \"_1Y-96\"\n",
    "                h1_div_class = \"col-xs-12\"\n",
    "                date_div_class = \"mb-no\"\n",
    "                para_div_class = \"col-sm-10\"\n",
    "\n",
    "                links = []\n",
    "\n",
    "                for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                    for a in divtag.find_all(\"a\", href=True):\n",
    "                        link = a[\"href\"]  # Gets the link\n",
    "\n",
    "                        # Checking the link if it is a relative link\n",
    "                        if link[0] == '/':\n",
    "                            link = domain_url + link\n",
    "\n",
    "                        link_start = \"https://www.prnewswire.com//news-releases\"\n",
    "                        if link.startswith(link_start):\n",
    "                            links.append(link)\n",
    "                # Remove duplicates\n",
    "                links = list(set(links))\n",
    "\n",
    "                collection = []\n",
    "\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        l_page = requests.get(link)\n",
    "                        l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                    except:\n",
    "                        err = \"prnewswire: err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                    try:\n",
    "                        div_ele = l_soup.find(\"div\", {\"class\": h1_div_class})\n",
    "                        h1_ele = div_ele.find(\"h1\")\n",
    "                        data.append(h1_ele.text)\n",
    "                    except:\n",
    "                        err = \"prnewswire: err: Failed to find title in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                    data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                    date_ele = l_soup.find(\"p\", {\"class\": date_div_class})\n",
    "                    try:\n",
    "                        date_text = date_ele.text\n",
    "                        date_text = (date_text.split('/'))[-1]\n",
    "                        date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                        data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                    except:\n",
    "                        err = \"prnewswire: err: Failed to find date in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                    cur_date = str(datetime.today())\n",
    "                    data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                    para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                    try:\n",
    "                        data.append(para_ele.text)  # Need to make this better\n",
    "                    except:\n",
    "                        err = \"prnewswire: err: Failed to find paragraph in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                    collection.append(data)\n",
    "\n",
    "                df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "                if df.empty:\n",
    "                    err = \"prnewswire: err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = FilterFunction(df)\n",
    "                emptydataframe(\"Prnewswire\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Prnewswire not working\")\n",
    "                not_working_functions.append(\"Prnewswire\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def arabfinance():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Arab Finanace\")\n",
    "                err_logs = []\n",
    "                url = \"https://www.albawaba.com/search?keyword=IPO&sort_by=created\"\n",
    "                domain_url = \"https://www.albawaba.com\"\n",
    "                title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "                try:\n",
    "                    page = requests.get(url)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                except:\n",
    "                    err = \"Arab Finance : err : Couldn't fetch \" + url \n",
    "                    err_logs.append(err)\n",
    "                    return \n",
    "                # Fetching all links \n",
    "                all_divs = soup.find_all(\"div\",{\"class\":\"field-content title\"})\n",
    "                for div in all_divs:\n",
    "                    links.append(domain_url + div.a[\"href\"])\n",
    "                #Fetch all the necessary data \n",
    "                final_links = []\n",
    "                today = date.today()\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    except:\n",
    "                        err = \"Arab Finance : err : Couldn't fetch url \" + link \n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    #get date of the article \n",
    "                    pub_date.append(\"Date\")\n",
    "                    #get title of the article \n",
    "                    title.append(soup.find(\"h1\",{\"class\":\"page-header\"}).text)\n",
    "                    #get text of the article \n",
    "                    # print(soup.find(\"div\",{\"class\":\"field field--name-body field--type-text-with-summary field--label-hidden field--item\"}).text)\n",
    "                    text.append(soup.find(\"div\",{\"class\":\"field field--name-body field--type-text-with-summary field--label-hidden field--item\"}).text)\n",
    "                    #get today's date i.r scrape date \n",
    "                    scraped_date.append(str(today))\n",
    "                    # get links that works \n",
    "                    final_links.append(link)\n",
    "                df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "                if df.empty:\n",
    "                    err = \"Arab Finance : err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = df.drop_duplicates(subset=[\"link\"])\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(df)\n",
    "                emptydataframe(\"Arab finance\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Arab Finanace is not working\")\n",
    "                not_working_functions.append(\"Arab Finance\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def star():\n",
    "        try:\n",
    "            print(\"Star\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.thestar.com.my/news/latest?tag=Business\"\n",
    "            domain_url = \"https://mb.com.ph\"\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "    #             print(page.content)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "            except:\n",
    "                err = \"The star : err : Couldn't fetch the url \"+ url\n",
    "                err_logs.append(err)\n",
    "                return\n",
    "            pub_date,scraped_date,links,final_links,title,text = [],[],[],[],[],[]\n",
    "            #get all links \n",
    "            for h4 in soup.find_all(\"h2\",{\"class\":\"f18\"}):\n",
    "                links.append(h4.a[\"href\"])\n",
    "            # Fetch all the required data for the dataframe \n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                except:\n",
    "                    err = \"The star : err : Couldn't fetch the url \"+ link\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                # Fetch all other data \n",
    "                title.append(soup.find(\"div\",{\"class\":\"headline story-pg\"}).h1.text)\n",
    "                pub_date.append(soup.find(\"p\",{\"class\":\"date\"}).text)\n",
    "                text.append(soup.find(\"div\",{\"class\":\"story bot-15 relative\"}).text)\n",
    "                scraped_date.append(today)\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"the star: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            log_errors(err_logs)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Star\",df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Star is not working\")\n",
    "            not_working_functions.append(\"Star\")\n",
    "    \n",
    "    def interfax():\n",
    "        try:\n",
    "            err_logs = []  # Access to view error logs\n",
    "            url = \"https://en.interfax.com.ua/news/search.html?q=ipo\"\n",
    "            domain_url = \"https://en.interfax.com.ua/\"\n",
    "\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            except:\n",
    "                err = \"toi: err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                return None\n",
    "\n",
    "            # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "            div_class = \"col-57\"\n",
    "            h1_class = \"article-content-title\"\n",
    "            date_div_class = \"col-18 article-time\"\n",
    "            para_div_class = \"article-content\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "\n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "\n",
    "                    links.append(link)\n",
    "\n",
    "            collection = []\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = \"err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                try:\n",
    "                    data.append(h1_ele.text)\n",
    "                except:\n",
    "                    err = \"err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"div\", {\"class\": date_div_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    date_text = (date_text.split('/'))[-1]\n",
    "                    date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "                except:\n",
    "                    err = \"err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = \"toi: err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding data to the collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = \"err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Interfax Ukraine\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Interfax_Ukraine not working\")\n",
    "            not_working_functions.append(\"Interfax_ukraine\")\n",
    "    def vccircle():\n",
    "        try :\n",
    "            print(\"Vccircle\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.vccircle.com/search/result/ipo/all\"\n",
    "            domain_url = \"https://www.vccircle.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Vccircle : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Vccircle : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"publish-time\"})[\"content\"])\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"premium-txt-container\"}).text)  \n",
    "                # print(title)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"col-sm-9 mid-content\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Vccircle : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Vccircle\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Vccircle not working\")\n",
    "            not_working_functions.append('Vccircle')\n",
    "\n",
    "    def allafrica():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Allafrica\")\n",
    "                err_logs = []\n",
    "                url = \"https://allafrica.com/search/?search_string=pre+ipo&search_submit=Search\"\n",
    "                domain_url = \"https://allafrica.com/\"\n",
    "\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "                # Class names of the elements to be scraped\n",
    "                div_class = \"search-results\"  # Class name of div containing the a tag\n",
    "                #h1_class = \"_1Y-96\"\n",
    "                #h1_div_class = \"col-xs-12\"\n",
    "                h2_class = \"headline\"\n",
    "                date_div_class = \"publication-date\"\n",
    "                para_div_class = \"story-body\"\n",
    "\n",
    "                links = []\n",
    "\n",
    "                for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                    for a in divtag.find_all(\"a\", href=True):\n",
    "                        link = a[\"href\"]  # Gets the link\n",
    "                        \n",
    "                        # Checking the link if it is a relative link\n",
    "                        if link[0] == '/':\n",
    "                            link = domain_url + link\n",
    "                        \n",
    "                        # Filtering advertaisment links\n",
    "                        link_start = domain_url \n",
    "                        if link.startswith(link_start):\n",
    "                            links.append(link)\n",
    "                # Remove duplicates\n",
    "                links = list(set(links))\n",
    "                #links # Debugging - if link array is generated\n",
    "\n",
    "                collection = []\n",
    "                scrapper_name = \"allafrica\"\n",
    "\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        l_page = requests.get(link)\n",
    "                        l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                    except:\n",
    "                        err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    data = []\n",
    "                    # Scraping the heading\n",
    "                    #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                    \n",
    "                    try:\n",
    "                        h2_ele = l_soup.find(\"h2\", {\"class\": h2_class})\n",
    "                        data.append(h2_ele.text)\n",
    "                    except:\n",
    "                        err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                    # Adding the link to data\n",
    "                    data.append(link)\n",
    "\n",
    "                    # Scraping the published date\n",
    "                    date_ele = l_soup.find(\"div\", {\"class\": date_div_class})\n",
    "                    try:\n",
    "                        date_text = date_ele.text\n",
    "                        #date_text = (date_text.split('/'))[-1]\n",
    "                        #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                        data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                    except:\n",
    "                        err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "                    \n",
    "                    # Adding the scraped date to data\n",
    "                    cur_date = str(datetime.today())\n",
    "                    data.append(cur_date)\n",
    "                    \n",
    "\n",
    "                    # Scraping the paragraph\n",
    "                    para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                    try:\n",
    "                        data.append(para_ele.text)  # Need to make this better\n",
    "                    except:\n",
    "                        err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "                    # Adding data to a collection\n",
    "                    collection.append(data)\n",
    "\n",
    "                df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "                if df.empty:\n",
    "                    err = scrapper_name + \": err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = FilterFunction(df)\n",
    "                # emptydataframe(\"Allafrica\",df)\n",
    "                # df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "              # pass\n",
    "                not_working_functions.append(\"Allafrica\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def zawya():\n",
    "        try :\n",
    "            print(\"zawya\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.zawya.com/en/search?q=pre+ipo\"\n",
    "            domain_url = \"https://www.zawya.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"zawya : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"h2\",{\"class\":\"teaser-title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"zawya : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"article-title\"}).text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"article-date\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"article-body\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":scraped_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"zawya : err: Empty datzawyaame\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"zawya\",df)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"zawya not working\")\n",
    "            not_working_functions.append('zawya')\n",
    "    def phnompenhpost():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Phnompenhpost\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.phnompenhpost.com/search/node/\"\n",
    "                domainUrl = \"https://www.phnompenhpost.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                ArticleDates = []\n",
    "                ScrapeDates = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        session = HTMLSession()\n",
    "                        resp = session.post(queryUrl)\n",
    "                        resp.html.render()\n",
    "                        pageSource = resp.html.html\n",
    "                        parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "                    except:\n",
    "                        err = \"phnompenhpost: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"w\") as f:\n",
    "                    #     f.write(pageSource)\n",
    "                    # break\n",
    "                    for item in parsedSource.find_all(\"li\", class_=\"search-result\"):\n",
    "                        requiredTag = item.find(\"h3\", class_=\"title\")\n",
    "                        currentArticleTitle = str(requiredTag.text).strip()\n",
    "                        # print(currentArticleTitle)\n",
    "                        currentArticleLink = requiredTag.find(\"a\")[\"href\"]\n",
    "                        # print(currentArticleLink)\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "                        currentArticleDateText = str(item.find(\"div\", class_=\"posted-date\").find(\"span\").text).split(\"by\")[0].strip()\n",
    "                        try:\n",
    "                            currentArticleDate = datetime.strptime(currentArticleDateText,\n",
    "                                                                \"%d %b %Y\").strftime(\"%d-%m-%Y\")\n",
    "                        except:\n",
    "                            err = \"phnompenhpost: err: Failed to retrieve date from article : \" + currentArticleLink\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(currentArticleLink))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        ArticleDates.append(currentArticleDate)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    # print(titles)\n",
    "                    # print(links)\n",
    "                    # print(ArticleDates)\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            # session = HTMLSession()\n",
    "                            # resp = session.get(link)\n",
    "                            # resp.html.render()\n",
    "                            # pageSource = resp.html.html\n",
    "                            # parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "                            headers = {\n",
    "                                'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                        except:\n",
    "                            err = \"phnompenhpost: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        # with open(\"response.html\", \"wb\") as f:\n",
    "                        #     f.write(pageSource.content)\n",
    "                        # break\n",
    "                        divTag = parsedSource.find(\"div\", id=\"ArticleBody\")\n",
    "                        for item in divTag.find_all(\"p\"):\n",
    "                            articleText += item.text\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "                    # print(ArticleBody)\n",
    "\n",
    "                # DataFrame creation\n",
    "                phnompenhpostDF = pd.DataFrame(scrapedData)\n",
    "                if phnompenhpostDF.empty:\n",
    "                    err = \"phnompenhpost: err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = FilterFunction(phnompenhpostDF)\n",
    "                emptydataframe(df)\n",
    "                return df\n",
    "            except:\n",
    "                not_working_functions.append(\"Phnompenhpost\")\n",
    "                print(\"Phnompenhpost not working\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def scmp():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Scmp\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.scmp.com/search/\"\n",
    "                domainUrl = \"https://www.scmp.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                ArticleDates = []\n",
    "                ScrapeDates = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        session = HTMLSession()\n",
    "                        resp = session.get(queryUrl)\n",
    "                        resp.html.render()\n",
    "                        pageSource = resp.html.html\n",
    "                        parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "                    except:\n",
    "                        err = \"scmp: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "                    for item in parsedSource.find_all(\"li\", class_=\"search-results__item item\"):\n",
    "                        requiredTag = item.find(\"span\", class_=\"content-link__title\")\n",
    "                        currentArticleTitle = requiredTag.text\n",
    "                        # print(currentArticleTitle)\n",
    "                        currentArticleLink = item.find(\"a\", class_=\"content__content-link content-link\")[\"href\"]\n",
    "                        # print(currentArticleLink)\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "                        try:\n",
    "                            currentArticleDate = datetime.strptime(item.find(\"div\", class_=\"wrapper__published-date\").text,\n",
    "                                                                \"%d %b %Y - %H:%M%p\").strftime(\"%d-%m-%Y\")\n",
    "                        except:\n",
    "                            err = \"scmp: err: Failed to retrieve date from article : \" + currentArticleLink\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(currentArticleLink))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        ArticleDates.append(currentArticleDate)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    # print(titles)\n",
    "                    # print(links)\n",
    "                    # print(ArticleDates)\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            # session = HTMLSession()\n",
    "                            # resp = session.get(link)\n",
    "                            # resp.html.render()\n",
    "                            # pageSource = resp.html.html\n",
    "                            # parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "                            headers = {\n",
    "                                'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                        except:\n",
    "                            err = \"scmp: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        # with open(\"response.html\", \"w\") as f:\n",
    "                        #     f.write(pageSource)\n",
    "                        # break\n",
    "                        for item in parsedSource.find_all(\"script\", type=\"application/ld+json\"):\n",
    "                            tempDict = json.loads(item.text)\n",
    "                            try:\n",
    "                                ArticleText = tempDict[\"articleBody\"]\n",
    "                            except:\n",
    "                                continue\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "                    # print(ArticleBody)\n",
    "\n",
    "                # DataFrame creation\n",
    "                scmpDF = pd.DataFrame(scrapedData)\n",
    "                if scmpDF.empty:\n",
    "                    err = \"scmp: err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df =FilterFunction(scmpDF)\n",
    "                emptydataframe(\"Scmp\",df)\n",
    "                return scmpDF\n",
    "            except:\n",
    "                print(\"Scmp not working\")\n",
    "                not_working_functions.append(\"Scmp\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    # def theoutreach():\n",
    "    #     try:\n",
    "    #         try:\n",
    "    #             print(\"The out reach\")\n",
    "    #             err_logs = []\n",
    "\n",
    "    #             baseSearchUrl = \"https://theoutreach.in/?s=\"\n",
    "    #             domainUrl = \"hhttps://theoutreach.in\"\n",
    "    #             keywords = ['IPO', 'initial public offering','Pre IPO']\n",
    "\n",
    "    #             # use this for faster testing\n",
    "    #             tkeywords = [\"IPO\"]\n",
    "    #             scrapedData = {}\n",
    "    #             links = []\n",
    "    #             titles = []\n",
    "    #             err_index = []\n",
    "    #             ArticleDates = []\n",
    "    #             ScrapeDates = []\n",
    "    #             for keyword in tkeywords:\n",
    "    #                 queryUrl = baseSearchUrl + keyword\n",
    "    #                 try:\n",
    "    #                     session = HTMLSession()\n",
    "    #                     resp = session.post(queryUrl)\n",
    "    #                     resp.html.render()\n",
    "    #                     pageSource = resp.html.html\n",
    "    #                     parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "    #                 except:\n",
    "    #                     err = \"theoutreach: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "    #                     err_logs.append(err)\n",
    "    #                     continue\n",
    "    #                 # with open(\"response.html\", \"w\") as f:\n",
    "    #                 #     f.write(pageSource)\n",
    "    #                 # break\n",
    "    #                 for item in parsedSource.find(\"div\", class_=\"article-container\").find_all(\"article\"):\n",
    "    #                     requiredTag = item.find(\"h2\", class_=\"entry-title\")\n",
    "    #                     currentArticleTitle = str(requiredTag.find(\"a\")[\"title\"]).strip()\n",
    "    #                     # print(currentArticleTitle)\n",
    "    #                     currentArticleLink = requiredTag.find(\"a\")[\"href\"]\n",
    "    #                     # print(currentArticleLink)\n",
    "    #                     if currentArticleLink[0] == \"/\":\n",
    "    #                         links.append(domainUrl + currentArticleLink)\n",
    "    #                     else:\n",
    "    #                         links.append(currentArticleLink)\n",
    "    #                     titles.append(currentArticleTitle)\n",
    "    #                     currentArticleDateText = item.find(\"time\", class_=\"entry-date published updated\").text\n",
    "    #                     try:\n",
    "    #                         currentArticleDate = datetime.strptime(currentArticleDateText,\n",
    "    #                                                             \"%B %d, %Y\").strftime(\"%d-%m-%Y\")\n",
    "    #                     except:\n",
    "    #                         err = \"theoutreach: err: Failed to retrieve date from article : \" + currentArticleLink\n",
    "    #                         ArticleDates.append(\"Error\")\n",
    "    #                         err_index.append(links.append(currentArticleLink))\n",
    "    #                         err_logs.append(err)\n",
    "    #                         continue\n",
    "    #                     ArticleDates.append(currentArticleDate)\n",
    "    #                     ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "    #                 scrapedData[\"title\"] = titles\n",
    "    #                 scrapedData[\"link\"] = links\n",
    "    #                 scrapedData[\"publish_date\"] = ArticleDates\n",
    "    #                 scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "    #                 # print(titles)\n",
    "    #                 # print(links)\n",
    "    #                 # print(ArticleDates)\n",
    "\n",
    "    #                 # Article's date and description scraping\n",
    "    #                 ArticleBody = []\n",
    "    #                 for link in links:\n",
    "    #                     articleText = \"\"\n",
    "    #                     try:\n",
    "    #                         session = HTMLSession()\n",
    "    #                         resp = session.get(link)\n",
    "    #                         resp.html.render()\n",
    "    #                         pageSource = resp.html.html\n",
    "    #                         parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "    #                         # headers = {\n",
    "    #                         #     'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "    #                         # pageSource = requests.get(link, headers=headers)\n",
    "    #                         # parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "    #                     except:\n",
    "    #                         err = \"theoutreach: err: Failed to access article link : \" + link\n",
    "    #                         ArticleDates.append(\"Error\")\n",
    "    #                         err_index.append(links.append(link))\n",
    "    #                         err_logs.append(err)\n",
    "    #                         continue\n",
    "    #                     # with open(\"response.html\", \"w\") as f:\n",
    "    #                     #     f.write(pageSource)\n",
    "    #                     # break\n",
    "    #                     divTag = parsedSource.find(\"div\", class_=\"entry-content clearfix\")\n",
    "    #                     for item in divTag.find_all(\"p\"):\n",
    "    #                         articleText += item.text\n",
    "    #                     ArticleBody.append(articleText)\n",
    "    #                 scrapedData[\"text\"] = ArticleBody\n",
    "    #                 # print(ArticleBody)\n",
    "\n",
    "    #             # DataFrame creation\n",
    "    #             theoutreachDF = pd.DataFrame(scrapedData)\n",
    "    #             if theoutreachDF.empty:\n",
    "    #                 err = \"theoutreach: err: Empty dataframe\"\n",
    "    #                 err_logs.append(err)\n",
    "    #             df = FilterFunction(theoutreachDF)\n",
    "    #             emptydataframe(\"The out reach\",df)\n",
    "    #             return df\n",
    "    #         except:\n",
    "    #             print(\"The outreach is not working\")\n",
    "    #             not_working_functions(\"The out reach\")\n",
    "    #     except:\n",
    "    #         df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "    #         return df1\n",
    "    # def dealstreetasia():\n",
    "    #     try:\n",
    "    #         try:\n",
    "    #             print(\"Dealstreetasia\")\n",
    "    #             err_logs = []  # Access to view error logs\n",
    "    #             url = \"https://www.dealstreetasia.com/?s=ipo\"\n",
    "    #             domain_url = \"https://www.dealstreetasia.com/\"\n",
    "\n",
    "    #             try:\n",
    "    #                 page = requests.get(url)\n",
    "    #                 soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    #             except:\n",
    "    #                 err = \"err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "    #                 err_logs.append(err)\n",
    "    #                 return None\n",
    "\n",
    "    #             # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "    #             div_class = \"category-link\"\n",
    "    #             h1_class = \"col-xl-8 col-lg-10 col-main\"\n",
    "    #             date_div_class = \"published-date\"\n",
    "    #             para_div_class = \"content-section\"\n",
    "\n",
    "    #             links = []\n",
    "\n",
    "    #             for divtag in soup.find_all(\"p\", {\"class\": div_class}):\n",
    "    #                 for a in divtag.find_all(\"a\", href=True):\n",
    "    #                     link = a[\"href\"]  # Gets the link\n",
    "\n",
    "    #                     # Checking the link if it is a relative link\n",
    "    #                     if link[0] == '/':\n",
    "    #                         link = domain_url + link\n",
    "\n",
    "    #                     links.append(link)\n",
    "\n",
    "\n",
    "\n",
    "    #             collection = []\n",
    "\n",
    "    #             for link in links:\n",
    "    #                 try:\n",
    "    #                     l_page = requests.get(link)\n",
    "    #                     l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "    #                 except:\n",
    "    #                     err = \"err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "    #                     err_logs.append(err)\n",
    "    #                     continue\n",
    "\n",
    "    #                 data = []\n",
    "    #                 # Scraping the heading\n",
    "    #                 h1_ele = l_soup.find(\"div\", {\"class\": h1_class})\n",
    "    #                 try:\n",
    "    #                     data.append(h1_ele.text)\n",
    "    #                 except:\n",
    "    #                     err = \"err: Failed to find title in page. Link: \" + link\n",
    "    #                     err_logs.append(err)\n",
    "    #                     continue  # drops the complete data if there is an error\n",
    "\n",
    "    #                 # Adding the link to data\n",
    "    #                 data.append(link)\n",
    "\n",
    "    #                 # Scraping the published date\n",
    "    #                 date_ele = l_soup.find(\"p\", {\"class\": date_div_class})\n",
    "    #                 try:\n",
    "    #                     date_text = date_ele.text\n",
    "    #                     date_text = (date_text.split('/'))[-1]\n",
    "    #                     date_text = date_text.replace(\" Updated: \", \"\")\n",
    "    #                     data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "    #                 except:\n",
    "    #                     err = \"err: Failed to find date in page. Link: \" + link\n",
    "    #                     err_logs.append(err)\n",
    "    #                     continue  # drops the complete data if there is an error\n",
    "\n",
    "    #                 # Adding the scraped date to data\n",
    "    #                 cur_date = str(datetime.today())\n",
    "    #                 data.append(cur_date)\n",
    "\n",
    "    #                 # Scraping the paragraph\n",
    "    #                 para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "    #                 try:\n",
    "    #                     data.append(para_ele.text)  # Need to make this better\n",
    "    #                 except:\n",
    "    #                     err = \"err: Failed to find paragraph in page. Link: \" + link\n",
    "    #                     err_logs.append(err)\n",
    "    #                     continue  # drops the complete data if there is an error\n",
    "\n",
    "    #                 # Adding data to the collection\n",
    "    #                 collection.append(data)\n",
    "\n",
    "    #             df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "    #             if df.empty:\n",
    "    #                 err = \"err: Empty dataframe\"\n",
    "    #                 err_logs.append(err)\n",
    "    #             df =FilterFunction(df)\n",
    "    #             emptydataframe(\"Dealstreetasia\",df)\n",
    "    #             log_errors(err_logs)\n",
    "    #             return df\n",
    "    #         except:\n",
    "    #             print(\"Dealstreetasia not working\")\n",
    "    #             not_working_functions.append(\"Dealstreetasia\")\n",
    "    #     except:\n",
    "    #         df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "    #         return df1\n",
    "    def livemint():\n",
    "        try:\n",
    "            print(\"Livemint India\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.livemint.com/Search/Link/Keyword/ipo\"\n",
    "            domain_url = \"https://www.livemint.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            h2_class = \"headline\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_class = \"headline\"\n",
    "            date_span_class = \"pubtime\"\n",
    "            para_ul_class = \"highlights\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"h2\", {\"class\": h2_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"livemint\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": date_span_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    date_text = ''.join((date_text.split(':'))[1:])\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                try:\n",
    "                    para_ele = (l_soup.findAll(\"ul\", {\"class\": para_ul_class}))[-1]\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    new_para_p_class = \"summary\"\n",
    "                    try:\n",
    "                        para_ele = (l_soup.findAll(\"p\", {\"class\": new_para_p_class}))[-1]\n",
    "                        data.append(para_ele.text)  # Need to make this better\n",
    "                    except:\n",
    "                        err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Livemint India \",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"Livemint India\")\n",
    "            print(\"Livemint not working\")\n",
    "    def guardian():\n",
    "        try:\n",
    "            print(\"Bahamas\")\n",
    "            err_logs = []  # Access to view error logs\n",
    "            url = \"https://bahamaspress.com/\"\n",
    "            domain_url = \"https://bahamaspress.com/?s=ipo\"\n",
    "\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            except:\n",
    "                err = \"err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                return None\n",
    "\n",
    "            # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "            div_class = \"item-details\"\n",
    "            h1_class = \"entry-title\"\n",
    "            date_div_class = \"td-post-date\"\n",
    "            para_div_class = \"td-post-content\"\n",
    "\n",
    "            links = []\n",
    "            for h3 in soup.find_all(\"h3\",{\"class\":\"entry-title td-module-title\"}):\n",
    "                link = h3.a[\"href\"]\n",
    "                if link[0] == '/':\n",
    "                    link = domain_url + link\n",
    "                links.append(link)\n",
    "            collection = []\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = \"err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                try:\n",
    "                    data.append(h1_ele.text)\n",
    "                except:\n",
    "                    err = \"err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": date_div_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "                except:\n",
    "                    err = \"err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = \"err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding data to the collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = \"err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Guardian Bahamas\",df)\n",
    "            log_errors(err_logs)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"Guardian bahamas\")\n",
    "            print(\"Guardian bahamas not working\")\n",
    "    def azernews():\n",
    "        try:\n",
    "            print(\"Azernews\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.azernews.az/search.php?query=ipo\"\n",
    "            domain_url = \"https://www.azernews.az/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            links = []\n",
    "\n",
    "            for a in soup.find_all(\"a\",{\"class\":\"news-item shadow\"}, href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "            print(links)\n",
    "            collection = []\n",
    "            scrapper_name = \"azernews\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"div\", {\"class\": \"article-content-wrapper\"}).h2\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": \"me-3\"})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = l_soup.find(\"div\", {\"class\": \"article-content\"})\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "            # print(err_logs)\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            # print(df) # For debugging. To check if df is created\n",
    "            # print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Azernews\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"Azernews\")\n",
    "            print(\"Azernews is not working\")\n",
    "    def chosenilboenglish():\n",
    "        try:\n",
    "            print(\"chosenilboenglish\")\n",
    "            err_logs = []\n",
    "            url = \"https://english.chosun.com/svc/list_in/search.html?query=ipo&pageconf=total\"\n",
    "            domain_url = \"http://english.chosun.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            dl_class = \"list_item\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_id = \"news_title_text_id\"\n",
    "            date_p_id = \"date_text\"\n",
    "            para_div_class = \"par\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            # for divtag in soup.find_all(\"dl\", {\"class\": dl_class}):\n",
    "            for a in soup.find_all(\"dl\",{\"class\":\"list_item\"}):\n",
    "                    link = a.dt.a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "            print(links)\n",
    "            collection = []\n",
    "            scrapper_name = \"azernews\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"h1\", {\"id\": title_h1_id})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"p\", {\"id\": date_p_id})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"chosenilboenglish\",df)\n",
    "            log_errors(err_logs)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"chosenilboenglish\")\n",
    "            print(\"chosenilboenglish not working\")\n",
    "    def otempo():\n",
    "        try:\n",
    "            print(\"Brazil\")\n",
    "\n",
    "            err_logs = []\n",
    "            url = \"https://www.otempo.com.br/busca-portal-o-tempo-7.6253516?q=IPO\"\n",
    "            domain_url = \"https://www.otempo.com.br/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Otempo : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"h2\",{\"class\":\"titulo\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            # print(links)\n",
    "            for link in links:\n",
    "                try:\n",
    "                    try:\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    # print(soup)\n",
    "                    # print(link)\n",
    "                    except:\n",
    "                        err = \"Otempo : err : Couldn't fetch url \" + link \n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    #Published Date\n",
    "                    pub_date.append(soup.find(\"div\" , {\"class\" : \"data-publicacao\"}).text)\n",
    "                    \n",
    "\n",
    "                    #Title of article \n",
    "                    title.append(soup.find(\"div\" , {\"class\" : \"cell titulo\"}).h1.text)       \n",
    "\n",
    "                    #Text of article\n",
    "                    text.append(soup.find(\"div\" , {\"class\" : \"cell chamada\"}).h2.text)\n",
    "\n",
    "                    #Scrapped date \n",
    "                    scraped_date.append(str(today))\n",
    "\n",
    "                    #Working links\n",
    "                    final_links.append(link)\n",
    "                except:\n",
    "                    continue\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"otempo : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Otempo brazil\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Otempo brazil not working\")\n",
    "            not_working_functions.append(\"Otempo Brazil\")\n",
    "    def elicudadona():\n",
    "        try:\n",
    "            print(\"Chile\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.elciudadano.com/?s=oferta+p%C3%BAblica+inicial\"\n",
    "            domain_url = \"https://www.elciudadano.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Elicudadona : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"h3\",{\"class\":\"mb-3\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Elicudadona : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\",{\"class\":\"time-ago time-now\"})[\"data-date\"])\n",
    "                # print(pub_date) \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"mb-4 the_title\"}).text)      \n",
    "                # print(title) \n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"the-excerpt-\"}).text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Folha : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df =FilterFunction(df)\n",
    "            emptydataframe(\"Elicudadona Chile\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"elicudadona chile\")\n",
    "            print(\"elicudadona not working\")\n",
    "    \n",
    "    def standartnews(keyword):\n",
    "        try:\n",
    "            print(\"standartnews bulgaria\")\n",
    "            err_logs = []\n",
    "            url = f\"https://www.standartnews.com/articles/search.html?keywords={keyword}&author=&category=-1&date_from=&date_to=\"\n",
    "            domain_url = \"https://www.standartnews.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Standartnews : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"news-general-link\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    # print(soup)\n",
    "                    # print(link)\n",
    "                except:\n",
    "                    err = \"Standartnews : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\")[\"datetime\"])\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"title-cont\"}).h1.text)       \n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"content\"}).p.text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Folha : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"standartnews bulgaria\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"standartnews bulgaria not working\")\n",
    "            not_working_functions.append(\"standartnews bulgaria\")\n",
    "    def lastampa():\n",
    "        try:\n",
    "            print(\"lastampa Italy\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.lastampa.it/ricerca?query=IPO&tracking=LSHHD-S\"\n",
    "            domain_url = \"\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Lastampa : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"entry__content__top\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"lastampa : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                if(soup.find(\"div\" , {\"class\" : \"story__content\"}).p == None or len(soup.find(\"div\" , {\"class\" : \"story__content\"}).p) == 0 ):\n",
    "                    continue\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"story__content\"}).p)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\",{\"class\":\"story__date\"})[\"datetime\"])\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"story__title\"}).text)       \n",
    "                # print(title)\n",
    "                #Text of article\n",
    "                \n",
    "            \n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\" : text , \"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"lastampa : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Lastampa Italy\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Lastampa Italy not working\")\n",
    "            not_working_functions.append(\"Lastampa Italy\")\n",
    "    def liputan6():\n",
    "        try:\n",
    "            print(\"Liputan Indo\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.liputan6.com/search?q=IPO\"\n",
    "            domain_url = \"https://www.liputan6.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Liputan6 : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"ui--a articles--iridescent-list--text-item__title-link\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Liputan6 : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\",{\"class\":\"read-page--header--author__datetime updated\"})[\"datetime\"])\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"read-page--header--title entry-title\"}).text)       \n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"article-content-body__item-content\"}).p.text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Folha : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Liputan\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Liputan indo not working\")\n",
    "            not_working_functions.append(\"Liputan indo\")\n",
    "    def milenio():\n",
    "        try:\n",
    "            print(\"Mexico\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.milenio.com/buscador?text=oferta+p%C3%BAblica+inicial\"\n",
    "            domain_url = \"https://www.milenio.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Milenio : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url + div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Milenio : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" : \"content-date\"}).time.text)\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"title\"}).text)       \n",
    "            \n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"media-container news\"}).p.text)\n",
    "            \n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Milenio : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Millenio Mexico\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Mexico not working\")\n",
    "            not_working_functions.append(\"Millenio Mexico\")\n",
    "    def scoop():\n",
    "        try:\n",
    "            print(\"New Zealand\")\n",
    "            err_logs = []\n",
    "            url = \"https://search.scoop.co.nz/search?q=IPO&submit=\"\n",
    "            domain_url = \"https://search.scoop.co.nz/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Scoop : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"result\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Scoop : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" : \"byline\"}).b.text)\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"story-top\"}).h1.text)     \n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"id\" : \"article\"}).p.text)\n",
    "                \n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Scoop : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"New Zealand\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"New Zealand\")\n",
    "            print(\"New Zealand not working\")\n",
    "    def globallegalchronicle():\n",
    "        try:\n",
    "            print(\"Globallegal\")\n",
    "            err_logs = []\n",
    "            url = \"https://globallegalchronicle.com/?s=ipo\"\n",
    "            domain_url = \"https://globallegalchronicle.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            h3_class = \"entry-title\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_class = \"entry-title\"\n",
    "            date_time_class = \"entry-date\"\n",
    "            para_div_class = \"entry-content\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"h3\", {\"class\": h3_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"globallegalchronicle\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link, headers=headers)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"time\", {\"class\": date_time_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"globalillegalchronicle\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Globallegal not working\")\n",
    "            not_working_functions.append(\"Global legal\")\n",
    "    \n",
    "    def supchina():\n",
    "        try:\n",
    "            print(\"supchina\")\n",
    "            err_logs = []\n",
    "            url = \"https://supchina.com/?s=ipo\"\n",
    "            domain_url = \"https://supchina.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            h3_class = \"card__title\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_class = \"h1\"\n",
    "            date_time_class = \"post__date\"\n",
    "            para_div_class = \"post__chunk\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"h3\", {\"class\": h3_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"supchina\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link, headers=headers)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"time\", {\"class\": date_time_class})\n",
    "                try:\n",
    "                    date_text = \"\".join(((\" \".join((date_ele.text.split(\" \"))[1:])).split(\"\\t\"))[0])\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Supchina\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Supchina not working\")\n",
    "            not_working_functions.append(\"Supchina\")\n",
    "    def aljazeera():\n",
    "        try:\n",
    "            print(\"Al jazeera Qatar\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.aljazeera.com/search/IPO\"\n",
    "            domain_url = \"https://www.aljazeera.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Aljazeera : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"h3\",{\"class\":\"gc__title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Aljazeera : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" ,{\"class\" : \"date-simple css-1yjq2zp\"}).text)\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"header\" , {\"class\" : \"article-header\"}).h1.text)       \n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"wysiwyg wysiwyg--all-content css-1ck9wyi\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "            \n",
    "            \n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Aljazeera: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Aljazeera Qatar\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Al Jazeera not working\")\n",
    "            not_working_functions.append(\"Al Jazeera Qatar\")\n",
    "    def aif():\n",
    "            try:\n",
    "                print(\"AIF Russia\")\n",
    "                err_logs = []\n",
    "                url = \"https://aif.ru/search?text=IPO\"\n",
    "                domain_url = \"https://aif.ru/\"\n",
    "                title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "                try:\n",
    "                    page = requests.get(url)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    # print(soup)\n",
    "                    \n",
    "                except:\n",
    "                    err = \"Aif : err : Couldn't fetch \" + url \n",
    "                    err_logs.append(err)\n",
    "                    return \n",
    "\n",
    "                all_divs = soup.find_all(\"div\",{\"class\":\"text_box\"})\n",
    "                for div in all_divs:\n",
    "                    links.append(div.a[\"href\"])\n",
    "                #Fetch all the necessary data \n",
    "                # print(links)\n",
    "                final_links = []\n",
    "                today = date.today()\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                        # print(soup)\n",
    "                    # print(link)\n",
    "                    except:\n",
    "                        err = \"Aif : err : Couldn't fetch url \" + link \n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    div = soup.find(\"div\" , {\"class\" : \"article_text\"})\n",
    "                    t = \"\"\n",
    "                    for i in div.find_all(\"p\"):\n",
    "                        t += i.text + \" \"\n",
    "                    text.append(t) \n",
    "                    # print(text)\n",
    "                    #Published Date\n",
    "                    pub_date.append(soup.find(\"div\" , {\"class\" : \"date\"}).text)\n",
    "                    \n",
    "\n",
    "                    #Title of article \n",
    "                    title.append(soup.find(\"h1\" , {\"itemprop\" : \"headline\"}).text)       \n",
    "\n",
    "                    #Text of article\n",
    "                \n",
    "                \n",
    "                    #Scrapped date \n",
    "                    scraped_date.append(str(today))\n",
    "\n",
    "                    #Working links\n",
    "                    final_links.append(link)\n",
    "                df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "                if df.empty:\n",
    "                    err = \"Aif : err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = df.drop_duplicates(subset=[\"link\"])\n",
    "                # df = FilterFunction(df)\n",
    "                emptydataframe(\"Aif Russia\",df)\n",
    "                log_errors(err_logs)\n",
    "                df = df[df[\"text\"] != \"\"]\n",
    "                df = translate_dataframe(df)\n",
    "                # df = FilterFunction(df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"AIF Russia not working\")\n",
    "                not_working_functions.append(\"AIF\")\n",
    "    def monitor():\n",
    "        try:\n",
    "            print(\"Monitor uganda\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.monitor.co.ug/service/search/uganda/1822068?pageNum=0&query=IPO%20offering&sortByDate=true&channelId=1448278\"\n",
    "            domain_url = \"https://www.monitor.co.ug/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Monitor : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"li\",{\"class\":\"search-result\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    try:\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                  # print(soup)\n",
    "                  # print(link)\n",
    "                    except:\n",
    "                        err = \"Monitor : err : Couldn't fetch url \" + link \n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                  #Published Date\n",
    "                    pub_dat = soup.find(\"time\" , {\"class\" : \"date\"}).text\n",
    "                  \n",
    "\n",
    "                  #Title of article \n",
    "                    titl  = soup.find(\"h1\" , {\"class\" : \"title-medium\"}).text      \n",
    "\n",
    "                  #Text of article\n",
    "                    div = soup.find(\"div\" , {\"class\" : \"paragraph-wrapper\"})\n",
    "                    t = \"\"\n",
    "                    for i in div.find_all(\"p\"):\n",
    "                        t += i.text + \" \"\n",
    "                    text.append(t)\n",
    "                  #Scrapped date \n",
    "                    scraped_date.append(str(today))\n",
    "                    pub_date.append(pub_dat)\n",
    "                    title.append(titl)\n",
    "                  #Working links\n",
    "                    final_links.append(link)\n",
    "                except:\n",
    "                    continue\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Monitor : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Monitor Uganda\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Monitor Uganda not working\")\n",
    "            not_working_functions.append(\"Uganda Monitor\")\n",
    "    def thesun():\n",
    "        try:\n",
    "            print(\"the sun UK\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.thesun.co.uk/?s=IPO\"\n",
    "            domain_url = \"https://www.thesun.co.uk/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"The sun  : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"teaser-anchor teaser-anchor--search\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"The sun : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"article__timestamp\"}).text)\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"article__headline\"}).text)       \n",
    "\n",
    "                #Text of article     \n",
    "                div = soup.find(\"div\" , {\"class\" : \"article__content\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "            \n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"The sun  : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"The sun UK\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"The sun uk not working\")\n",
    "            not_working_functions.append(\"The sun UK\")\n",
    "\n",
    "\n",
    "\n",
    "    def koreannewsgazette():\n",
    "        try :\n",
    "            print(\"Koreannewsgazette\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.koreanewsgazette.com/?s=IPO\"\n",
    "            domain_url = \"https://www.koreanewsgazette.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"koreannewsgazette : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"header\",{\"class\":\"entry-header\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"koreannewsgazette : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\" , {\"class\" :\"entry-date published updated\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"entry-title\"}).text)  \n",
    "                # print(title)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"entry-content clearfix\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"koreannewsgazette : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"koreannewsgazette\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"koreannewsgazette not working\")\n",
    "            not_working_functions.append('koreannewsgazette')\n",
    "    \n",
    "    def parool():\n",
    "        try:\n",
    "            print(\"Parool Netherlands\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.parool.nl/search?query=beursgang\"\n",
    "            domain_url = \"https://www.parool.nl\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Parool : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"article\",{\"class\":\"fjs-teaser-compact teaser--compact\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Parool : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(str(soup.find(\"time\" , {\"class\" : \"artstyle__production__datetime\"})[\"datetime\"]))\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"artstyle__header-title\"}).text)       \n",
    "            \n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"section\" , {\"class\" : \"artstyle__main\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\" , {\"class\" : \"artstyle__paragraph \"}):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                \n",
    "                \n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Parool : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Parool Netherlands\")\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Netherlands is not working\")\n",
    "            not_working_functions.append(\"Netherlands parool\")\n",
    "    def shabiba():\n",
    "        try:\n",
    "            print(\"Oman shabiba\")\n",
    "            err_logs = []\n",
    "            url = \"https://shabiba.com/search?search=%D8%A7%D9%84%D8%B7%D8%B1%D8%AD+%D8%A7%D9%84%D8%B9%D8%A7%D9%85+%D8%A7%D9%84%D8%A3%D9%88%D9%84%D9%8A\"\n",
    "            domain_url = \"https://shabiba.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "              # print(soup)\n",
    "\n",
    "            except:\n",
    "                err = \"Shabiba : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"h2\",{\"class\":\"post-title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "          #Fetch all the necessary data \n",
    "          # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "              # print(soup)\n",
    "              # print(link)\n",
    "                except:\n",
    "                    err = \"Shabiba : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "              #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" : \"text-muted\"}).text)\n",
    "\n",
    "\n",
    "              #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"my-3\"}).text)       \n",
    "\n",
    "              #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"mb-5 post-details\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "              # print(t)\n",
    "\n",
    "              #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "              #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Shabiba : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Oman\",df)\n",
    "            log_errors(err_logs)\n",
    "        #   df = translate_dataframe(df)\n",
    "        #   df  = link_correction(df)\n",
    "        \n",
    "            return df\n",
    "        except:\n",
    "            print(\"Oman not working\")\n",
    "            not_working_functions.append(\"Oman shabiba\")\n",
    "    def sabah():\n",
    "        try :\n",
    "            print(\"Sabah\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.sabah.com.tr/arama?query=IPO\"\n",
    "            domain_url = \"https://www.sabah.com.tr\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Sabah : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"figure\",{\"class\":\"multiple boxShadowSet\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Sabah : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                if(soup.find(\"h1\" , {\"class\" : \"pageTitle\"}) == None or soup.find(\"span\" , {\"class\" :\"textInfo align-center\"}) == None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"pageTitle\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"textInfo align-center\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"newsBox\"}).text)\n",
    "            \n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Sabah : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Sabah\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Sabah not working\")\n",
    "            not_working_functions.append('Sabah')\n",
    "    def swissinfo():\n",
    "        try :\n",
    "            print(\"Swissinfo\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.swissinfo.ch/service/search/eng/45808844?query=IPO\"\n",
    "            domain_url = \"https://www.swissinfo.ch\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Swissinfo : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"si-teaser__link\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"koreannewsgazette : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"si-detail__title\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\" , {\"class\" :\"si-detail__date\"})[\"datetime\"])\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"section\" , {\"class\" : \"si-detail__content\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Swissinfo : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Swissinfo\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Swissinfo not working\")\n",
    "            not_working_functions.append('Swissinfo')\n",
    "    def dziennik():\n",
    "        try :\n",
    "            print(\"Dziennik\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.dziennik.pl/szukaj?c=1&b=1&o=1&s=0&search_term=&q=IPO\"\n",
    "            domain_url = \"https://www.thelocal.se\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Dziennik : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"resultContent\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Dziennik : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"mainTitle\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"datePublished\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"detail intext articleBody\"}) \n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\" , {\"class\" : \"hyphenate\"}):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Dziennik : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Dziennik\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Dziennik not working\")\n",
    "            not_working_functions.append('dziennik')\n",
    "    def aljarida():\n",
    "        try :\n",
    "            print(\"Aljarida\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.aljarida.com/search/%D8%A7%D9%84%D8%B7%D8%B1%D8%AD%20%D8%A7%D9%84%D8%B9%D8%A7%D9%85%20%D8%A7%D9%84%D8%A3%D9%88%D9%84%D9%8A/\"\n",
    "            domain_url = \"https://www.aljarida.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Aljarida : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"text\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Aljarida : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"title\"}).h1.text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"date\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"section\" , {\"id\" : \"main\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\" , {\"class\" : \"ar\"}):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Aljarida : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Aljarida\",df)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"aljarida not working\")\n",
    "            not_working_functions.append('aljarida')\n",
    "    def hungary():\n",
    "        try :\n",
    "            print(\"Hungary\")\n",
    "            err_logs = []\n",
    "            url = \"https://444.hu/kereses?q=IPO\"\n",
    "            domain_url = \"https://444.hu/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Hungary : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"h1\",{\"class\":\"kJ eR eF eT eU eV eW kI eZ\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Hungary : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"kH eR eF eT eU eV eW fa eZ kG\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"eE f5 eT eU eV eW fa fh\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"eA f- fG eu eT he hf eW hg lp\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Hungary : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Hungary\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Hungary not working\")\n",
    "            not_working_functions.append('Hungary')\n",
    "    def jauns():\n",
    "        try :\n",
    "            print(\"jauns\")\n",
    "            err_logs = []\n",
    "            url = \"https://jauns.lv/meklet?q=IPO\"\n",
    "            domain_url = \"https://jauns.lv\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"jauns : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"article-small__link\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Jaus : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"span\" , {\"class\" : \"heading__text\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"meta-info__publish-date-full\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"data-io\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text ,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"jauns : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"jauns\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"jauns not working\")\n",
    "            not_working_functions.append('jauns')\n",
    "    def pulse():\n",
    "        try :\n",
    "            print(\"Pulse\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.pulse.ng/search?q=IPO\"\n",
    "            domain_url = \"https://www.pulse.ng\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Pulse : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"gradient-overlay\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Pulse : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"article-headline \"}).span.text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\" , {\"class\" :\"detail-article-date date-type-publicationDate\"})[\"datetime\"])\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"article-content \"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Pulse : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Pulse\",df)\n",
    "            df = df[df[\"text\"] != \"\"]\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Pulse not working\")\n",
    "            not_working_functions.append('Pulse')\n",
    "    def vnexpress():\n",
    "        try :\n",
    "            print(\"vnexpress\")\n",
    "            err_logs = []\n",
    "            url = \"https://timkiem.vnexpress.net/?q=IPO\"\n",
    "            domain_url = \"https://timkiem.vnexpress.net\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"vnexpress : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"h3\",{\"class\":\"title-news\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"vnexpress : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                if(soup.find(\"h1\" , {\"class\" : \"title-detail\"})== None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"title-detail\"}))\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"date\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"article\" , {\"class\" : \"fck_detail \"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text ,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"vnexpress : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"vnexpress\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"vnexpress not working\")\n",
    "            not_working_functions.append('vnexpress')\n",
    "    def jamaicaobserver():\n",
    "        try :\n",
    "            print(\"jamaicaobserver\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.jamaicaobserver.com/search/?q=IPO\"\n",
    "            domain_url = \"https://www.jamaicaobserver.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"jamaicaobserver : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"entry-title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Jamaice Observer : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"headline col-12\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"article-pubdate\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"article-restofcontent\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\" , {\"class\" : \"article__body\"}):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text ,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"jamaicaobserver : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"jamaicaobserver\",df)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"jamaicaobserver not working\")\n",
    "            not_working_functions.append('jamaicaobserver') \n",
    "    def independent():\n",
    "        try :\n",
    "            print(\"independent\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.independent.ie/search/?q=IPO\"\n",
    "            domain_url = \"https://www.independent.ie\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"independent : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"c-card1-textlink -d:b -as:1\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    try:\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    # print(soup)\n",
    "                    # print(link)\n",
    "                    except:\n",
    "                        err = \"independent : err : Couldn't fetch url \" + link \n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    #Title of article\n",
    "                    title.append(soup.find(\"h1\" , {\"class\" : \"title1-main\"}).text)\n",
    "                    #Published Date\n",
    "                    pub_date.append(soup.find(\"time\" , {\"class\" :\"time1\"}).text)\n",
    "                    # print(pub_date)\n",
    "\n",
    "                    #Text of article\n",
    "                    div = soup.find(\"div\" , {\"class\" : \"n-body1\"})\n",
    "                    t = \"\"\n",
    "                    for i in div.find_all(\"p\"):\n",
    "                        t += i.text + \" \"\n",
    "                    text.append(t)\n",
    "                    # print(text)\n",
    "\n",
    "                    #Scrapped date \n",
    "                    scraped_date.append(str(today))\n",
    "\n",
    "                    #Working links\n",
    "                    final_links.append(link)\n",
    "                except:\n",
    "                    continue\n",
    "            df = pd.DataFrame({\"text\":text ,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"independent : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"independent\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"independent not working\")\n",
    "            not_working_functions.append('independent')\n",
    "    def albaniandailynews():\n",
    "        try:\n",
    "            print(\"Albania\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.albaniandailynews.com/search.php?s=ipo\"\n",
    "            domain_url = \"https://albaniandailynews.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\"\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            h3_class = \"alith_post_title\"  # Class name of h3 containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            h3_class_title = \"alith_post_title\"\n",
    "            date_span_class = \"meta_date\"\n",
    "            para_div_class = \"column-1\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"h3\", {\"class\": h3_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"albaniandailynews\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link, headers=headers)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    h2_ele = l_soup.find(\"h3\", {\"class\": h3_class_title})\n",
    "                    data.append(h2_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": date_span_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Albania daily news\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Albaniadailynews not working\")\n",
    "            not_working_functions.append(\"Albania\")\n",
    "    def ewn():\n",
    "        try:\n",
    "            print(\"EWN\")\n",
    "            err_logs = []  # Access to view error logs\n",
    "            url = \"https://ewnews.com/?s=ipo\"\n",
    "            domain_url = \"https://ewnews.com\"\n",
    "\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            except:\n",
    "                err = \"err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                return None\n",
    "\n",
    "            # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "            div_class = \"entry-title\"\n",
    "            h1_class = \"entry-title\"\n",
    "            date_div_class = \"entry-date published\"\n",
    "            para_div_class = \"entry-content col-md-12\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for h3 in soup.find_all(\"h3\",{\"class\":\"entry-title\"}):\n",
    "                link = h3.a[\"href\"]\n",
    "                if link[0] == '/':\n",
    "                    link = domain_url + link\n",
    "                links.append(link)\n",
    "\n",
    "            collection = []\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = \"err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                try:\n",
    "                    data.append(h1_ele.text)\n",
    "                except:\n",
    "                    err = \"err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": date_div_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "                except:\n",
    "                    err = \"err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = \"err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding data to the collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = \"err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"EWN\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"EWN not working\")\n",
    "            not_working_functions.append(\"Ewn\")\n",
    "    def bloombergquint():\n",
    "        try:\n",
    "            print(\"Bloombergquint\")\n",
    "            err_logs = []  # Access to view error logs\n",
    "            keywords = [\"ipo\",\"pre-ipo\",\"pre ipo\",\"listing\"]\n",
    "            url = \"https://www.bloombergquint.com/search?q=\"\n",
    "            domain_url = \"https://www.bloombergquint.com\"\n",
    "            links = []\n",
    "            pub_date = []\n",
    "            scraped_date = []\n",
    "            title = []\n",
    "            text = []\n",
    "            for keyword in keywords:\n",
    "                try:\n",
    "                    page = requests.get(url+keyword)\n",
    "                    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                except:\n",
    "                    err = \"Bllomberg quint: err: Failed to access main url: \" + url + keyword + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    return None\n",
    "                for a in soup.find_all(\"a\",{\"class\":\"list-story-m__item__link__2mfId\"}):\n",
    "                    links.append(a[\"href\"])\n",
    "            today = date.today()\n",
    "            final_links = []\n",
    "            for link in links:\n",
    "                try:\n",
    "                    # if(link.startswith(\"/\")):\n",
    "                    link = domain_url + link\n",
    "                    try:\n",
    "                        print(link)\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    except:\n",
    "                        err = \"Bloomberg Quint: err: Failed to access main url: \" + link + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                    # return None\n",
    "                    pub_date.append(soup.find(\"time\",{\"class\":\"desktop-only published-info-module__updated-on__2JWey\"}).string)\n",
    "                    title.append(soup.find(\"h1\",{\"class\":\"story-base-template-m__story-headline__2cNwS\"}).text)\n",
    "                    t = [ i.text for i in soup.find_all(\"div\",{\"class\":\"story-element story-element-text\"})]\n",
    "                    text.append(\" \".join(t))\n",
    "                    scraped_date.append(today)\n",
    "                    final_links.append(link)\n",
    "                except:\n",
    "                    err = \"Bloomberg Quint : err: None type object found for \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "\n",
    "            # print(len(pub_date),len(scraped_date),len(title),len(links),len(text))\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Bloomberg Quint: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Bloombergquint not working\")\n",
    "            not_working_functions.append(\"Bloombergquint\")\n",
    "    def ecns():\n",
    "        try:\n",
    "            print(\"ECNS\")\n",
    "            err_logs = []\n",
    "            baseSearchUrl = \"http://www.ecns.cn/rss/rss.xml\"\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            err_index = []\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            ArticleBody = []\n",
    "            queryUrl = baseSearchUrl\n",
    "            try:\n",
    "                pageSource = requests.get(baseSearchUrl)\n",
    "            except:\n",
    "                err = \"ecns: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                exit()\n",
    "            # with open(\"response.xml\", \"wb\") as f:\n",
    "            #     f.write(pageSource.content)\n",
    "            # break\n",
    "            parsedSource = BeautifulSoup(pageSource.content, \"xml\")\n",
    "            for eachItem in parsedSource.find_all(\"item\"):\n",
    "                currentArticleTitle = eachItem.find(\"title\").text\n",
    "                currentArticleLink = eachItem.find(\"link\").text\n",
    "                currentArticleDate = datetime.strptime(str(eachItem.find(\"pubDate\").text).split(\"GMT\", maxsplit=2)[0].strip(),\n",
    "                                                    \"%Y-%m-%d %H:%M:%S\").strftime(\"%d-%m-%Y\")\n",
    "                # articleText = str(eachItem.find(\"description\").text).split(\"CDATA[ \", maxsplit=2)[1].rstrip(\" ]]\")\n",
    "                articleText = str(eachItem.find(\"description\").text).split(\"CDATA[ \", maxsplit=2)[0].replace(\"#039;\", \"\")\n",
    "                ArticleBody.append(articleText)\n",
    "\n",
    "                titles.append(currentArticleTitle)\n",
    "                links.append(currentArticleLink)\n",
    "                ArticleDates.append(currentArticleDate)\n",
    "                ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "            scrapedData[\"title\"] = titles\n",
    "            scrapedData[\"link\"] = links\n",
    "            scrapedData[\"publish_date\"] = ArticleDates\n",
    "            scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "            scrapedData[\"text\"] = ArticleBody\n",
    "            print(scrapedData)\n",
    "            # DataFrame creation\n",
    "            ecnsDF = pd.DataFrame(scrapedData)\n",
    "            ecnsDF = ecnsDF.drop_duplicates(subset=[\"link\"])\n",
    "            if ecnsDF.empty:\n",
    "                err = \"ecns: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(ecnsDF)\n",
    "            # emptydataframe(\"ECNS Canada\",df)\n",
    "            # log_errors(err_logs)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"ECNS canada not working\")\n",
    "            not_working_functions.append(\"ECNS\")\n",
    "    def energy_voice():\n",
    "        try :\n",
    "            print(\"energy voice\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.energyvoice.com/?s=ipo\"\n",
    "            domain_url = \"https://www.energyvoice.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"energy voice : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"h2\",{\"class\":\"title title--sm\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"energy voice : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"title entry-title\"}).text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"post-timestamp__published\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"cms clearfix\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"energy voice : err: Empty datenergy voiceame\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"energy voice\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"energy voice not working\")\n",
    "            not_working_functions.append('energy voice')\n",
    "    def euroNews():\n",
    "        try:\n",
    "            print(\"Euronews\")\n",
    "            err_logs = []\n",
    "            err_index = []\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            ArticleBody = []\n",
    "            baseSearchUrl = \"https://www.euronews.com/search?query=ipo\"\n",
    "            domainUrl = \"https://www.euronews.com\"\n",
    "\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            queryUrl = baseSearchUrl\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                pageSource = requests.get(queryUrl, headers=headers)\n",
    "            except:\n",
    "                err = \"euroNews: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "            # with open(\"response.html\", \"wb\") as f:\n",
    "            #     f.write(pageSource.content)\n",
    "            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "            # if parsedSource.find(\"div\", class_=\"fpj_bignews\"):\n",
    "            #     bigNews = parsedSource.find(\"div\", class_=\"fpj_bignews\")\n",
    "            #     currentArticleTitle = bigNews.find(\"h3\").text\n",
    "            #     titles.append(currentArticleTitle)\n",
    "            #     currentArticleLink = bigNews.find(\"a\")[\"href\"]\n",
    "            #     links.append(currentArticleLink)\n",
    "            #     sourceDateTime = datetime.strptime(bigNews.find(\"span\", class_=\"dateTime\").text, \"%d %B %Y, %I:%M %p\").strftime(\n",
    "            #         \"%d-%m-%Y\")\n",
    "            #     ArticleDates.append(sourceDateTime)\n",
    "            #     ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "            for item in parsedSource.find(\"div\", class_=\"o-block-listing__content\").find_all(\"article\"):\n",
    "                requiredTag = item.find(\"h3\").find(\"a\")\n",
    "                currentArticleTitle = requiredTag[\"title\"]\n",
    "                # print(currentArticleTitle)\n",
    "                currentArticleLink = requiredTag[\"href\"]\n",
    "                # print(currentArticleLink)\n",
    "                if currentArticleLink[0] == \"/\":\n",
    "                    links.append(domainUrl + currentArticleLink)\n",
    "                else:\n",
    "                    links.append(currentArticleLink)\n",
    "                titles.append(currentArticleTitle)\n",
    "                sourceDateTime = datetime.strptime(item.find(\"time\", class_=\"m-object__date u-margin-top-2\").text.strip(), \"%d/%m/%Y\").strftime(\n",
    "                    \"%d-%m-%Y\")\n",
    "                ArticleDates.append(sourceDateTime)\n",
    "                ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "            scrapedData[\"title\"] = titles\n",
    "            scrapedData[\"link\"] = links\n",
    "            scrapedData[\"publish_date\"] = ArticleDates\n",
    "            scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "            # print(len(titles), titles)\n",
    "            # print(len(links), links)\n",
    "            # print(len(ArticleDates), ArticleDates)\n",
    "\n",
    "            # Article's date and description scraping\n",
    "            for link in links:\n",
    "                articleText = \"\"\n",
    "                try:\n",
    "                    pageSource = requests.get(link, headers=headers)\n",
    "                    parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                except:\n",
    "                    err = \"euroNews: err: Failed to access article link : \" + link\n",
    "                    ArticleDates.append(\"Error\")\n",
    "                    err_logs.append(err)\n",
    "                    err_index.append(link)\n",
    "                    continue\n",
    "                # with open(\"response.html\", \"wb\") as f:\n",
    "                #     f.write(pageSource.content)\n",
    "                # break\n",
    "                requiredTag = parsedSource.find(\"div\", class_=\"c-article__full_article\")\n",
    "                if requiredTag:\n",
    "                    for item in requiredTag.find_all(\"p\"):\n",
    "                        articleText += item.text.strip()\n",
    "                    # print(articleText)\n",
    "                    ArticleBody.append(articleText)\n",
    "                else:\n",
    "                    articleText = titles[links.index(link)]\n",
    "                    ArticleBody.append(articleText)\n",
    "\n",
    "            scrapedData[\"text\"] = ArticleBody\n",
    "            # print(len(ArticleBody), ArticleBody)\n",
    "\n",
    "            # Clean and Normalize links\n",
    "            if len(err_index) != 0:\n",
    "                for e in err_index:\n",
    "                    idx = scrapedData[\"link\"].index(e)\n",
    "                    scrapedData[\"link\"].pop(idx)\n",
    "                    scrapedData[\"title\"].pop(idx)\n",
    "                    scrapedData[\"publish_date\"].pop(idx)\n",
    "\n",
    "            # DataFrame creation\n",
    "            euroNewsDF = pd.DataFrame(scrapedData)\n",
    "            euroNewsDF = euroNewsDF.drop_duplicates(subset=[\"link\"])\n",
    "            if euroNewsDF.empty:\n",
    "                err = \"euroNews: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(euroNewsDF)\n",
    "            emptydataframe(\"Euro news\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Euronews not working\")\n",
    "            not_working_functions.append(\"Euro news europe\")\n",
    "    \n",
    "    import requests\n",
    "\n",
    "    def theFreePressJournal():\n",
    "            try:\n",
    "                print(\"The free press journal\")\n",
    "                err_logs = []\n",
    "                err_index = []\n",
    "                ArticleDates = []\n",
    "                ScrapeDates = []\n",
    "                ArticleBody = []\n",
    "                baseSearchUrl = \"https://www.freepressjournal.in/search?q=ipo\"\n",
    "                domainUrl = \"https://www.freepressjournal.in\"\n",
    "\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                queryUrl = baseSearchUrl\n",
    "                try:\n",
    "                    headers = {\n",
    "                        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                    pageSource = requests.get(queryUrl, headers=headers)\n",
    "                except:\n",
    "                    err = \"theFreePressJournal: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                # with open(\"response.html\", \"wb\") as f:\n",
    "                #     f.write(pageSource.content)\n",
    "                parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                if parsedSource.find(\"div\", class_=\"fpj_bignews\"):\n",
    "                    bigNews = parsedSource.find(\"div\", class_=\"fpj_bignews\")\n",
    "                    currentArticleTitle = bigNews.find(\"h3\").text\n",
    "                    titles.append(currentArticleTitle)\n",
    "                    currentArticleLink = bigNews.find(\"a\")[\"href\"]\n",
    "                    links.append(currentArticleLink)\n",
    "                    sourceDateTime = datetime.strptime(bigNews.find(\"span\", class_=\"dateTime\").text, \"%d %B %Y, %I:%M %p\").strftime(\n",
    "                        \"%d-%m-%Y\")\n",
    "                    ArticleDates.append(sourceDateTime)\n",
    "                    ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "                for item in parsedSource.find(\"div\", class_=\"fpj_newList\").find_all(\"li\"):\n",
    "                    requiredTag = item.find(\"span\", class_=\"fpj_title\")\n",
    "                    currentArticleTitle = requiredTag.text\n",
    "                    # print(currentArticleTitle)\n",
    "                    currentArticleLink = item.find(\"a\")[\"href\"]\n",
    "                    # print(currentArticleLink)\n",
    "                    if currentArticleLink[0] == \"/\":\n",
    "                        links.append(domainUrl + currentArticleLink)\n",
    "                    else:\n",
    "                        links.append(currentArticleLink)\n",
    "                    titles.append(currentArticleTitle)\n",
    "                    sourceDateTime = datetime.strptime(item.find(\"i\", class_=\"dateTime\").text, \"%d %B %Y, %I:%M %p\").strftime(\n",
    "                        \"%d-%m-%Y\")\n",
    "                    ArticleDates.append(sourceDateTime)\n",
    "                    ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "                for item in parsedSource.find(\"div\", class_=\"fpj_lsh\").find_all(\"li\"):\n",
    "                    requiredTag = item.find(\"h3\")\n",
    "                    currentArticleTitle = requiredTag.text\n",
    "                    # print(currentArticleTitle)\n",
    "                    currentArticleLink = item.find(\"a\")[\"href\"]\n",
    "                    # print(currentArticleLink)\n",
    "                    if currentArticleLink[0] == \"/\":\n",
    "                        links.append(domainUrl + currentArticleLink)\n",
    "                    else:\n",
    "                        links.append(currentArticleLink)\n",
    "                    titles.append(currentArticleTitle)\n",
    "                    sourceDateTime = datetime.strptime(item.find(\"span\", class_=\"dateTime\").text, \"%d %B %Y, %I:%M %p\").strftime(\n",
    "                        \"%d-%m-%Y\")\n",
    "                    ArticleDates.append(sourceDateTime)\n",
    "                    ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "                scrapedData[\"title\"] = titles\n",
    "                scrapedData[\"link\"] = links\n",
    "                scrapedData[\"publish_date\"] = ArticleDates\n",
    "                scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                # print(len(titles), titles)\n",
    "                # print(len(links), links)\n",
    "                # print(len(ArticleDates), ArticleDates)\n",
    "\n",
    "                # Article's date and description scraping\n",
    "                for link in links:\n",
    "                    articleText = \"\"\n",
    "                    try:\n",
    "                        pageSource = requests.get(link, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                    except:\n",
    "                        err = \"theFreePressJournal: err: Failed to access article link : \" + link\n",
    "                        ArticleDates.append(\"Error\")\n",
    "                        err_logs.append(err)\n",
    "                        err_index.append(link)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "\n",
    "                    for item in parsedSource.find(\"article\",{\"id\":\"fjp-article\"}).find_all(\"p\"):\n",
    "                        articleText += item.text.strip()\n",
    "                    # print(articleText)\n",
    "                    ArticleBody.append(articleText)\n",
    "\n",
    "                scrapedData[\"text\"] = ArticleBody\n",
    "                # print(len(ArticleBody), ArticleBody)\n",
    "\n",
    "                # Clean and Normalize links\n",
    "                if len(err_index) != 0:\n",
    "                    for e in err_index:\n",
    "                        idx = scrapedData[\"link\"].index(e)\n",
    "                        scrapedData[\"link\"].pop(idx)\n",
    "                        scrapedData[\"title\"].pop(idx)\n",
    "                        scrapedData[\"publish_date\"].pop(idx)\n",
    "\n",
    "                # DataFrame creation\n",
    "                theFreePressJournalDF = pd.DataFrame(scrapedData)\n",
    "                theFreePressJournalDF = theFreePressJournalDF.drop_duplicates(subset=[\"link\"])\n",
    "                if theFreePressJournalDF.empty:\n",
    "                    err = \"theFreePressJournal: err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = FilterFunction(theFreePressJournalDF)\n",
    "                emptydataframe(\"The free press journal\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"thefreepressjournal not working\")\n",
    "                not_working_functions.append(\"Thefreepressjournal\")\n",
    "    def aylien():\n",
    "\n",
    "        import time\n",
    "        import aylien_news_api\n",
    "        from aylien_news_api.rest import ApiException\n",
    "        from datetime import datetime,date\n",
    "        from pprint import pprint\n",
    "        import json\n",
    "        import pandas as pd\n",
    "        def setup_alyien_api():\n",
    "            configuration = aylien_news_api.Configuration()\n",
    "            # Configure API key authorization: app_id\n",
    "            configuration.api_key['X-AYLIEN-NewsAPI-Application-ID'] = '6104f1f4'\n",
    "            # Configure API key authorization: app_key\n",
    "            configuration.api_key['X-AYLIEN-NewsAPI-Application-Key'] = 'f5ddb80c14739c18cc5e40cd260ba9b1'\n",
    "            # Defining host is optional and default to https://api.aylien.com/news\n",
    "            configuration.host = \"https://api.aylien.com/news\"\n",
    "            # Create an instance of the API class\n",
    "            api_instance = aylien_news_api.DefaultApi(aylien_news_api.ApiClient(configuration))\n",
    "            return api_instance\n",
    "        def convert_to_dict(stories):\n",
    "            for index,value in enumerate(stories):\n",
    "                stories[index] = stories[index].to_dict()\n",
    "            return stories\n",
    "        def fetch_news_stories(api_instance, params={}):\n",
    "            fetched_strories = []\n",
    "            stories = None \n",
    "            while (stories is None or len(stories) > 0) and len(fetched_strories)<1300:\n",
    "                try:\n",
    "                    response = api_instance.list_stories(**params)\n",
    "                except ApiException as e:\n",
    "                    print(\"Exception when calling DefaultApi->list_stories: %s\\n\" % e)\n",
    "                stories = response.stories\n",
    "                stories = convert_to_dict(stories)\n",
    "                params['cursor'] = response.next_page_cursor\n",
    "                fetched_strories += stories\n",
    "                print(\"Fetched %d stories.Total story count so far : %d\"%(len(stories),len(fetched_strories)))\n",
    "                return fetched_strories\n",
    "        # # Current date with correct format for the API option \n",
    "        # today = datetime.now()\n",
    "        # time = str(today.strftime(\"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "\n",
    "\n",
    "        params = {\n",
    "            'text': 'IPO',\n",
    "            'published_at_start': '2022-05-12T00:00:00Z',\n",
    "            'published_at_end': '2022-05-13T23:59:00Z',\n",
    "            'cursor':'*',\n",
    "            'per_page':50\n",
    "        }\n",
    "        api_instance = setup_alyien_api()\n",
    "        today = datetime.now()\n",
    "        day = today.strftime(\"%d\")\n",
    "        month = today.strftime(\"%m\")\n",
    "        year = today.strftime(\"%y\")\n",
    "        stories = fetch_news_stories(api_instance,params)\n",
    "        text = []\n",
    "        link = []\n",
    "        pub_date = []\n",
    "        title = []\n",
    "        scraped_date = []\n",
    "        today = str(datetime.now())\n",
    "        for i in stories:\n",
    "            text.append(i[\"body\"])\n",
    "            title.append(i[\"title\"])\n",
    "            pub_date.append(\"-\".join(str(i[\"published_at\"]).strip().split(\" \")[0].split(\"-\")[::-1]))\n",
    "            link.append(i[\"links\"][\"permalink\"])\n",
    "            scraped_date.append(today)\n",
    "\n",
    "        df = pd.DataFrame({\"text\":text,\"link\":link,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "        return df\n",
    "    def dw():\n",
    "        try:\n",
    "            print(\"DW\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.dw.com/search/en?searchNavigationId=9097&languageCode=en&origin=gN&item=ipo\"\n",
    "            domain_url = \"https://www.dw.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            div_class = \"searchResult\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_class = \"\" # There exists no title here\n",
    "            date_ul_class = \"smallList\"\n",
    "            para_p_class = \"intro\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link[1:]\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"dw\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link, headers=headers)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    # title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                    title_ele = l_soup.find(\"h1\") # There exists only 1 h1 ele in the page\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                try:\n",
    "                    date_ele_container = l_soup.find(\"ul\", {\"class\": date_ul_class})\n",
    "                    date_text =  date_ele_container.text.split(\"\\n\")[2]\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text.replace(\".\",\"-\").strip())  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                try:\n",
    "                    para_ele = (l_soup.findAll(\"p\", {\"class\": para_p_class}))[-1]\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"DW\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"DW\")\n",
    "            print(\"DW not working\")\n",
    "    def thehindubusinessline():\n",
    "        err_logs = []\n",
    "        url = \"https://www.thehindubusinessline.com/search/?q=ipo&pd=pastweek&type=story&type=storyline&sort=newest\"\n",
    "        domain_url = \"https://www.thehindubusinessline.com/\"\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "            'sec-fetch-site': 'none',\n",
    "            'sec-fetch-mode': 'navigate',\n",
    "            'sec-fetch-user': '?1',\n",
    "            'sec-fetch-dest': 'document',\n",
    "            'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "        }\n",
    "        page = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "        # Class names of the elements to be scraped\n",
    "        div_class = \"searchPage\"  # Class name of div containing the a tag\n",
    "        #h1_class = \"_1Y-96\"\n",
    "        #h1_div_class = \"col-xs-12\"\n",
    "        title_h1_class = \"tp-title-inf\"\n",
    "        date_span_class = \"update-time\"\n",
    "        para_div_class = \"contentbody\"\n",
    "\n",
    "        links = []\n",
    "\n",
    "        for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "            for a in divtag.find_all(\"a\", href=True):\n",
    "                link = a[\"href\"]  # Gets the link\n",
    "\n",
    "                # Checking the link if it is a relative link\n",
    "                if link[0] == '/':\n",
    "                    link = domain_url + link\n",
    "\n",
    "                # Filtering advertaisment links\n",
    "                link_start = domain_url \n",
    "                if link.startswith(link_start):\n",
    "                    links.append(link)\n",
    "        # Remove duplicates\n",
    "        links = list(set(links))\n",
    "        #links # Debugging - if link array is generated\n",
    "\n",
    "        collection = []\n",
    "        scrapper_name = \"thehindubusinessline\"\n",
    "\n",
    "        for link in links:\n",
    "            try:\n",
    "                l_page = requests.get(link)\n",
    "                l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "            except:\n",
    "                err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                continue\n",
    "\n",
    "            data = []\n",
    "            # Scraping the heading\n",
    "            #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "\n",
    "            try:\n",
    "                title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                data.append(title_ele.text)\n",
    "            except:\n",
    "                err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                err_logs.append(err)\n",
    "                continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "            # Adding the link to data\n",
    "            data.append(link)\n",
    "\n",
    "            # Scraping the published date\n",
    "            date_ele = l_soup.find(\"span\", {\"class\": date_span_class})\n",
    "            try:\n",
    "                date_text = date_ele.text\n",
    "                date_text = (date_text.split('\\n'))[-2]\n",
    "                #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "            except:\n",
    "                err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                err_logs.append(err)\n",
    "                continue  # drops the complete data if there is an error\n",
    "\n",
    "            # Adding the scraped date to data\n",
    "            cur_date = str(datetime.today())\n",
    "            data.append(cur_date)\n",
    "\n",
    "\n",
    "            # Scraping the paragraph\n",
    "            try:\n",
    "                para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                # para_ele = para_ele.strip(\"\\n\")\n",
    "                data.append(para_ele.text)  # Need to make this better\n",
    "            except:\n",
    "                err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                err_logs.append(err)\n",
    "                continue  # drops the complete data if there is an error\n",
    "            # Adding data to a collection\n",
    "            collection.append(data)\n",
    "\n",
    "        df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "        if df.empty:\n",
    "            err = scrapper_name + \": err: Empty dataframe\"\n",
    "            err_logs.append(err)\n",
    "        #print(df) # For debugging. To check if df is created\n",
    "        #print(err_logs) # For debugging - to check if any errors occoured\n",
    "        df = FilterFunction(df)\n",
    "        return df\n",
    "    df1 = korea()\n",
    "    df2 = proactive(\"ipo\")\n",
    "    df3 = Reuters(\"ipo\")\n",
    "    df4 = TradingChart()\n",
    "    df5 = GoogleAlert()\n",
    "    df6 = live(\"ipo\")\n",
    "    df7 = standartnews(\"ipo\")\n",
    "    df8 = kontan(\"ipo\")\n",
    "    df9 = AZ(\"ipo\")\n",
    "    df10 = xinhuanet()\n",
    "    df11 = AFR()\n",
    "    df12 = MoneyControl()\n",
    "    df13 = Cnbc_Seeking()\n",
    "    df14 = toi()\n",
    "    df15 = German()\n",
    "    # df16 = Italian()\n",
    "    df17 = Japannews()\n",
    "    df18 = Romania()\n",
    "    df19 = Russian()\n",
    "    df20 = Swedish()\n",
    "    df21 = GoogleAlert1()\n",
    "    df22 = GoogleAlert2()\n",
    "    df23 = GoogleAlert3()\n",
    "    df24 = GoogleAlert4()\n",
    "    df25 = GoogleAlert5()\n",
    "    df26 = IPOMonitor()\n",
    "    df27 = globallegalchronicle()\n",
    "    df28 = Seenews()\n",
    "    df29 = Bisnis()\n",
    "    df30 = RomaniaNew()\n",
    "    df31 = RomaniaInsider()\n",
    "    df32 = SpaceMoney()\n",
    "    # df33 = Carteira()\n",
    "    df34 = Kontan1() ##\n",
    "    df35 = euronews()\n",
    "    # df36 = franchdailynews()\n",
    "    # df37 = norway()\n",
    "    # df38 = localde()\n",
    "    df39 = chinatoday()\n",
    "    df40 = aljazeera()\n",
    "    df41 = Koreatimes()\n",
    "    df42 = zdnet()\n",
    "    df43 = arabNews()\n",
    "    df44 = chosun()\n",
    "    # df45 = Forbes()\n",
    "    df46 = kngnet()\n",
    "    df47 = kedg()\n",
    "    df48 = wealthx()\n",
    "    df49 = indonesia()\n",
    "    df50 = asiainsurancereview()\n",
    "    df51 = economic_times()\n",
    "    df52 = prnewswire()\n",
    "    df53 = arabfinance()\n",
    "    df54 = interfax()\n",
    "    df55 = vccircle()\n",
    "    df56 = allafrica()\n",
    "    df57 = zawya()\n",
    "    df58 = aljarida()\n",
    "    df59 = dziennik()\n",
    "    df60 = swissinfo()\n",
    "    df61 = einnews()\n",
    "    df62 = sabah()\n",
    "    df63 = livemint()\n",
    "    df64 = guardian()\n",
    "    df65 = azernews()\n",
    "    df66 = chosenilboenglish()\n",
    "    df67 = otempo()\n",
    "    df68 = elicudadona()\n",
    "    df69 = lastampa()\n",
    "    df70 = liputan6()\n",
    "    df71 = milenio()\n",
    "    df72 = scoop()\n",
    "    df73 = supchina()\n",
    "    df74 = romania_insider()\n",
    "    df75 = cnbc1()\n",
    "    df76 = aif()\n",
    "    df77 = monitor()\n",
    "    df78 = thesun()\n",
    "    df79 = parool()\n",
    "    df80 = shabiba()\n",
    "    df81 = koreannewsgazette()\n",
    "    df82 = hungary()\n",
    "    df83 = jauns()\n",
    "    df84 = pulse()\n",
    "    df85 = vnexpress()\n",
    "    df86 = jamaicaobserver()\n",
    "    df87 = independent()\n",
    "    df88 = albaniandailynews()\n",
    "    df89 = ewn()\n",
    "    df90 = bloombergquint()\n",
    "    df91 = ecns()\n",
    "    df92 = energy_voice()\n",
    "    df93 = euroNews()\n",
    "    df94 = theFreePressJournal()\n",
    "    # df95 = aylien()\n",
    "    df96 = dw()\n",
    "    df97 = star()\n",
    "    df98 = Reuters(\"pre ipo\")\n",
    "    df99 = Reuters(\"Initial Public Offering\")\n",
    "    df100 = rss()\n",
    "    df101 = thehindubusinessline()\n",
    "    # df67 = scmp()\n",
    "    # df66 = phnompenhpost()\n",
    "    df_final_1 = [df101,df100,df46,df19,df99,df98,df97,df96,df94,df93,df92,df91,df90,df89,df88,df87,df86,df85,df84,df83,df81,df80,df79, df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11, df12,df13,df14,df15, df17,df18,df21,df22 ,df23,df24,df25,df26, df27, df28, df29,df30,df31,df32, df34, df35,df39, df40, df41,df42,df43,df44,df47,df48,df49,df50,df52,df53,df54,df55,df57, df58, df59, df60,df61,  df62,df63,df64,df65, df66,df67, df68, df69, df70, df71, df72,df73,  df74, df75, df76, df77,df78]\n",
    "    # df_final_1 = [df85,df83,df80,df79,df7,df8,df18,df29,df32,df34,df58,df59,df62,df67, df68, df69, df70, df71,df76]\n",
    "    df_final = pd.concat(df_final_1)\n",
    "#     df_final = FilterFunction(df_fin)\n",
    "    # TODO: commented out for testing as it takes too long.  uncomment later\n",
    "    \n",
    "    \n",
    "\n",
    "#     dff = FilterFunction(df_final)\n",
    "    # df_final = remove_navigablestring(df_final)\n",
    "    todays_report_filename = os.path.join(output_dir, 'todays_report.csv')\n",
    "    todays_report_filename1 = os.path.join(output_dir, 'todays_report1.csv')\n",
    "    # df_final.to_csv(todays_report_filename1,index=False)\n",
    "    final = correct_navigable_string(df_final)\n",
    "    final.to_csv(todays_report_filename1,index=False)\n",
    "    # final = translate_dataframe(final)\n",
    "    #evening \n",
    "#     final =df_final.loc[public_date == scrap_date]\n",
    "#     dff = FilterFunction(final)\n",
    "    # from googletrans import Translator\n",
    "    # import googletrans\n",
    "    # translator = Translator()\n",
    "    # t = final['title']\n",
    "    # t1 =[]\n",
    "    # for i in t:\n",
    "    # #     print(detect(i))\n",
    "    #     translations=translator.translate(i,dest=\"en\")\n",
    "    #     tt = translations.text\n",
    "    #     t1.append(tt)\n",
    "    # final['title'] = t1\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#     print(output_dir)\n",
    "    textfile = open(\"logs.txt\",\"w\")\n",
    "    for i in not_working_functions:\n",
    "        textfile.write(i+\"\\n\")\n",
    "    textfile.close()\n",
    "    logging.info(\"writing output artifact \" + todays_report_filename + \" to \" + output_dir)\n",
    "    final.to_csv(todays_report_filename,index=False)\n",
    "    logging.info(\"completed writing output artifact \" + todays_report_filename + \" to \" + output_dir)\n",
    "\n",
    "  # # final =final.loc[public_date == scrap_date]\n",
    "\n",
    "multilex_scraper(\"/home/prachi_multilex2\", \"/home/prachi_multilex2\")       # uncomment this line to run this as a python script\n",
    "\n",
    "# multilex_scraper( \"\", \"\")  \n",
    "logging.info(\"last line of scraper\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8562b3-458e-4bb1-b4a9-887f350c03a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:03:43,291 - 2210765 - root - INFO - first line of multilex_scraper_xform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korea\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.reuters.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reuters.com/article/idUSL1N2Y41A2', 'https://www.reuters.com/article/idUSKBN2NY0RT', 'https://www.reuters.com/article/idUSL8N2Y42K2', 'https://www.reuters.com/article/idUSL1N2Y40NP', 'https://www.reuters.com/article/idUSL1N2Y40ND', 'https://www.reuters.com/article/idUSZ8N2WR005', 'https://www.reuters.com/article/idUSL4N2Y31C9', 'https://www.reuters.com/article/idUSKBN2NY0N2', 'https://www.reuters.com/article/idUSL1N2Y40J1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:04:44,390 - 2210765 - root - INFO - Reuters function ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame is blank\n",
      "FTC\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:04:51,376 - 2210765 - root - INFO - live.com: invoking requests.url()=https://www.livemint.com/Search/Link/Keyword/ipo\n",
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.livemint.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "2022-06-18 13:04:51,467 - 2210765 - root - INFO - live.com: completed requests.url()=https://www.livemint.com/Search/Link/Keyword/ipo\n",
      "2022-06-18 13:04:51,476 - 2210765 - root - INFO - Live function ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "standartnews bulgaria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:05:12,168 - 2210765 - root - INFO - www.kontan.co.id: invoking requests.url()=https://www.kontan.co.id/search/?search=ipo&Button_search=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:05:13,211 - 2210765 - root - INFO - www.kontan.co.id: completed invoking requests.url()=https://www.kontan.co.id/search/?search=ipo&Button_search=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:05:57,599 - 2210765 - root - INFO - www.xinhuanet.com: invoking requests.url()=http://www.xinhuanet.com/english/mobile/business.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/casia/uzbekistan/3579837.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/business/finance/3582478.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/casia/uzbekistan/3596383.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/business/economy/3607527.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/business/energy/3605277.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/casia/uzbekistan/3582506.html\n",
      "trend.az: err: Failed to find paragraph in page. Link: https://en.trend.az/business/economy/3564275.html\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:05:57,974 - 2210765 - root - INFO - www.xinhuanet.com: completed invoking requests.url()=http://www.xinhuanet.com/english/mobile/business.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFR\n",
      "Moneycontrol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:08:33,661 - 2210765 - root - INFO - cnbc.com: invoking requests.url()=https://www.cnbc.com/id/10000666/device/rss\n",
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.cnbc.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "2022-06-18 13:08:33,825 - 2210765 - root - INFO - cnbc.com: completed requests.url()=https://www.cnbc.com/id/10000666/device/rss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:08:34,004 - 2210765 - root - INFO - CNBC_seeking function ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toi\n",
      "toi: err: Failed to find title in page. Link: https://timesofindia.indiatimes.com/gadgets-news/lic-ipo-paytm-money-launches-pre-ipo-feature-what-it-is-and-how-to-use-it/articleshow/91153774.cms\n",
      "toi: err: Failed to find title in page. Link: https://timesofindia.indiatimes.com/business/india-business/an-impossible-ipo-problem-in-the-age-of-unicorns/articleshow/89964001.cms\n",
      "toi: err: Failed to find title in page. Link: https://timesofindia.indiatimes.com/business/india-business/survey-author-explains-why-govt-is-bullish-on-the-economy/articleshow/89247812.cms\n",
      "German\n",
      "['https://www.tagesschau.de/wirtschaft/unternehmen/gorillas-entlassungen-boersengang-101.html', 'https://www.tagesschau.de/wirtschaft/finanzen/marktberichte/dax-dow-geldanlage-ukraine-inflation-ezb-fed-anleihen-oel-wti-brent-101.html', 'https://www.tagesschau.de/wirtschaft/unternehmen/porsche-volkswagen-boersengang-101.html', 'https://www.tagesschau.de/wirtschaft/unternehmen/biotech-branche-verzeichnet-hohe-investments-101.html', 'https://www.tagesschau.de/wirtschaft/unternehmen/gostudent-bewertung-drei-milliarden-101.html', 'https://www.tagesschau.de/wirtschaft/finanzen/boersengaenge-2021-101.html', 'https://www.tagesschau.de/wirtschaft/finanzen/marktberichte/marktbericht-dax-dow-jones-147.html', 'https://www.tagesschau.de/wirtschaft/unternehmen/daimler-truck-ipo-boersengang-frankfurt-101.html', 'https://www.tagesschau.de/wirtschaft/finanzen/marktberichte/marktbericht-dax-dow-jones-169.html', 'https://www.tagesschau.de/wirtschaft/unternehmen/ionos-united-internet-boersengang-cloud-101.html']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:08:46,344 - 2210765 - root - INFO - Japannews: invoking requests.url()=https://www.japantimes.co.jp/tag/ipo/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japannews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:08:47,729 - 2210765 - root - INFO - Japannews: Completed invoking requests.url()=https://www.japantimes.co.jp/tag/ipo/\n",
      "2022-06-18 13:10:33,769 - 2210765 - root - INFO - Romania: invoking requests.url()=https://adevarul.ro/cauta/?terms=ofert%C4%83%20public%C4%83%20ini%C8%9Bial%C4%83&fromDate=2012-1-1&toDate=2021-3-10&tab=mrarticle&page=1&sortBy=cronologic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romania\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:10:38,750 - 2210765 - root - INFO - Romania: Completed invoking requests.url()=https://adevarul.ro/cauta/?terms=ofert%C4%83%20public%C4%83%20ini%C8%9Bial%C4%83&fromDate=2012-1-1&toDate=2021-3-10&tab=mrarticle&page=1&sortBy=cronologic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian\n",
      "['https://ipo.einnews.com/article/577350269?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/577350267?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/577363395?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/577346551?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/577363700?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.comhttp://ipo.einnews.com/article/577325937?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/577315295?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/577312422?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/577304238?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D', 'https://ipo.einnews.com/article/577300333?lcf=OCDoEYEqDerAgfzLAbKEdQ%3D%3D']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:11:21,060 - 2210765 - root - INFO - Swedish: invoking requests.url()=https://www.svd.se/sok?q=ipo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swedish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:11:21,487 - 2210765 - root - INFO - Swedish: Completed invoking requests.url()=https://www.svd.se/sok?q=ipo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame is blank\n",
      "GoogleAlert1\n",
      "GoogleAlert2\n",
      "GoogleAlert3\n",
      "GoogleAlert4\n",
      "GoogleAlert5\n",
      "IPOmonitor\n",
      "Globallegal\n",
      "Seenews\n",
      "Bisnis\n",
      "['https://market.bisnis.com/read/20220618/7/1545112/incar-dana-segar-rp315-miliar-cerestar-trgu-siap-ipo-tahun-ini', 'https://market.bisnis.com/read/20220617/192/1544949/penjualan-apartemen-turun-kinerja-calon-emiten-saraswanti-indoland-swid-tergerus', 'https://market.bisnis.com/read/20220617/192/1544914/masuk-masa-book-building-cek-jadwal-ipo-saraswanti-indoland-swid', 'https://market.bisnis.com/read/20220617/192/1544849/saraswanti-indoland-swid-pakai-dana-ipo-untuk-modal-kerja', 'https://market.bisnis.com/read/20220617/192/1544746/saraswanti-indoland-swid-siap-ipo-tawarkan-harga-rp180-rp200-per-saham', 'https://market.bisnis.com/read/20220615/192/1544089/hillcon-hill-ipo-jelang-kenaikan-suku-bunga-the-fed-tepatkah-waktunya', 'https://market.bisnis.com/read/20220615/7/1544026/yang-cuan-dan-boncos-dari-saham-ipo-2022', 'https://market.bisnis.com/read/20220615/192/1544038/raih-laba-rp403-miliar-hillcon-hill-komitmen-bagi-dividen-80-persen-setelah-ipo', 'https://market.bisnis.com/read/20220615/192/1544002/hillcon-hill-siap-ipo-andalkan-sektor-tambang-konstruksi-sipil', 'https://market.bisnis.com/read/20220615/192/1543993/incar-dana-ipo-hingga-rp8846-miliar-hillcon-hill-siap-ekspansi']\n",
      "RomaniaNew\n",
      "RomaniaInsider\n",
      "spacemoney\n",
      "Kontan1\n",
      "['https://investasi.kontan.co.id/news/mau-ipo-saraswanti-indoland-development-siwd-incar-dana-segar-rp-68-miliar', 'https://insight.kontan.co.id/news/grup-djarum-agresif-cari-dana-dari-penjualan-saham-via-ipo', 'https://insight.kontan.co.id/news/menakar-valuasi-harga-saham-ipo-hillcon-hill-lebih-menarik-dari-doid-dewa-untr', 'https://stocksetup.kontan.co.id/news/pasang-harga-penawaran-awal-rp-250-rp-400-simak-jadwal-ipo-hillcon', 'https://stocksetup.kontan.co.id/news/bakal-ipo-hillcon-incar-laba-bersih-hingga-rp-600-miliar-di-tahun-2022', 'https://investasi.kontan.co.id/news/bakal-gelar-ipo-hillcon-bidik-dana-rp-8846-miliar', 'https://insight.kontan.co.id/news/kontraktor-tambang-hillcon-membidik-dana-ipo-rp-88460-miliar', 'https://investasi.kontan.co.id/news/hillcon-pasang-harga-penawaran-awal-ipo-di-kisaran-rp-250-rp-400-per-saham', 'https://investasi.kontan.co.id/news/segera-ipo-hillcon-bidik-laba-bersih-hingga-rp-600-miliar-di-tahun-ini', 'https://investasi.kontan.co.id/news/hillcon-lepas-221-miliar-saham-dalam-gelaran-ipo-bidik-dana-rp-8846-miliar', 'https://insight.kontan.co.id/news/coca-cola-tunda-rencana-ipo-unit-pembotolan-afrika-yang-bernilai-sekitar-us-3-miliar', 'https://investasi.kontan.co.id/news/ojk-menyebut-57-perusahaan-masuk-pipeline-ipo-tahun-ini-blibli-siap-melantai-di-bei', 'https://investasi.kontan.co.id/news/incar-rp1349-m-dari-ipo-rp237-t-dari-waran-ini-rencana-mandiri-mineral-perkasa', 'https://insight.kontan.co.id/news/bijak-dan-cermat-memilih-saham-baru-begini-kondisi-saham-ipo-2022', 'https://insight.kontan.co.id/news/blibli-dikabarkan-bidik-dana-us-500-juta-dari-ipo', 'https://investasi.kontan.co.id/news/blibli-dikabarkan-mau-ipo-bagaimana-prospeknya', 'https://investasi.kontan.co.id/news/ini-kata-analis-terhadap-rencana-blibli-gelar-ipo-di-tahun-2022', 'https://investasi.kontan.co.id/news/blibli-dikabarkan-bersiap-ipo-tahun-ini-begini-pandangan-analis', 'https://insight.kontan.co.id/news/bersiap-ipo-di-tahun-2022-blibli-dikabarkan-gandeng-credit-suisse-dan-morgan-stanley', 'https://investasi.kontan.co.id/news/jadi-penghuni-baru-indeks-lq45-saham-goto-melesat-1316-lewati-harga-ipo']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar', 'Bakal Gelar IPO, Hillcon Bidik Dana Rp 884,6 Miliar']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar', 'Bakal Gelar IPO, Hillcon Bidik Dana Rp 884,6 Miliar', 'Hillcon Pasang Harga Penawaran Awal IPO di Kisaran Rp 250-Rp 400 Per Saham']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar', 'Bakal Gelar IPO, Hillcon Bidik Dana Rp 884,6 Miliar', 'Hillcon Pasang Harga Penawaran Awal IPO di Kisaran Rp 250-Rp 400 Per Saham', 'Segera IPO, Hillcon Bidik Laba Bersih Hingga Rp 600 Miliar di Tahun Ini']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar', 'Bakal Gelar IPO, Hillcon Bidik Dana Rp 884,6 Miliar', 'Hillcon Pasang Harga Penawaran Awal IPO di Kisaran Rp 250-Rp 400 Per Saham', 'Segera IPO, Hillcon Bidik Laba Bersih Hingga Rp 600 Miliar di Tahun Ini', 'Hillcon Lepas 2,21 Miliar Saham dalam Gelaran IPO, Bidik Dana Rp 884,6 Miliar']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar', 'Bakal Gelar IPO, Hillcon Bidik Dana Rp 884,6 Miliar', 'Hillcon Pasang Harga Penawaran Awal IPO di Kisaran Rp 250-Rp 400 Per Saham', 'Segera IPO, Hillcon Bidik Laba Bersih Hingga Rp 600 Miliar di Tahun Ini', 'Hillcon Lepas 2,21 Miliar Saham dalam Gelaran IPO, Bidik Dana Rp 884,6 Miliar', 'OJK Menyebut 57 Perusahaan Masuk Pipeline IPO Tahun Ini, Blibli Siap Melantai di BEI?']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar', 'Bakal Gelar IPO, Hillcon Bidik Dana Rp 884,6 Miliar', 'Hillcon Pasang Harga Penawaran Awal IPO di Kisaran Rp 250-Rp 400 Per Saham', 'Segera IPO, Hillcon Bidik Laba Bersih Hingga Rp 600 Miliar di Tahun Ini', 'Hillcon Lepas 2,21 Miliar Saham dalam Gelaran IPO, Bidik Dana Rp 884,6 Miliar', 'OJK Menyebut 57 Perusahaan Masuk Pipeline IPO Tahun Ini, Blibli Siap Melantai di BEI?', 'Incar Rp134,9 M dari IPO & Rp2,37 T dari Waran, Ini Rencana Mandiri Mineral Perkasa ']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar', 'Bakal Gelar IPO, Hillcon Bidik Dana Rp 884,6 Miliar', 'Hillcon Pasang Harga Penawaran Awal IPO di Kisaran Rp 250-Rp 400 Per Saham', 'Segera IPO, Hillcon Bidik Laba Bersih Hingga Rp 600 Miliar di Tahun Ini', 'Hillcon Lepas 2,21 Miliar Saham dalam Gelaran IPO, Bidik Dana Rp 884,6 Miliar', 'OJK Menyebut 57 Perusahaan Masuk Pipeline IPO Tahun Ini, Blibli Siap Melantai di BEI?', 'Incar Rp134,9 M dari IPO & Rp2,37 T dari Waran, Ini Rencana Mandiri Mineral Perkasa ', 'Blibli Dikabarkan Mau IPO, Bagaimana Prospeknya?']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar', 'Bakal Gelar IPO, Hillcon Bidik Dana Rp 884,6 Miliar', 'Hillcon Pasang Harga Penawaran Awal IPO di Kisaran Rp 250-Rp 400 Per Saham', 'Segera IPO, Hillcon Bidik Laba Bersih Hingga Rp 600 Miliar di Tahun Ini', 'Hillcon Lepas 2,21 Miliar Saham dalam Gelaran IPO, Bidik Dana Rp 884,6 Miliar', 'OJK Menyebut 57 Perusahaan Masuk Pipeline IPO Tahun Ini, Blibli Siap Melantai di BEI?', 'Incar Rp134,9 M dari IPO & Rp2,37 T dari Waran, Ini Rencana Mandiri Mineral Perkasa ', 'Blibli Dikabarkan Mau IPO, Bagaimana Prospeknya?', 'Ini Kata Analis Terhadap Rencana Blibli Gelar IPO di Tahun 2022']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar', 'Bakal Gelar IPO, Hillcon Bidik Dana Rp 884,6 Miliar', 'Hillcon Pasang Harga Penawaran Awal IPO di Kisaran Rp 250-Rp 400 Per Saham', 'Segera IPO, Hillcon Bidik Laba Bersih Hingga Rp 600 Miliar di Tahun Ini', 'Hillcon Lepas 2,21 Miliar Saham dalam Gelaran IPO, Bidik Dana Rp 884,6 Miliar', 'OJK Menyebut 57 Perusahaan Masuk Pipeline IPO Tahun Ini, Blibli Siap Melantai di BEI?', 'Incar Rp134,9 M dari IPO & Rp2,37 T dari Waran, Ini Rencana Mandiri Mineral Perkasa ', 'Blibli Dikabarkan Mau IPO, Bagaimana Prospeknya?', 'Ini Kata Analis Terhadap Rencana Blibli Gelar IPO di Tahun 2022', 'Blibli Dikabarkan Bersiap IPO Tahun Ini, Begini Pandangan Analis']\n",
      "['Mau IPO, Saraswanti Indoland Development (SIWD) Incar Dana Segar Rp 68 Miliar', 'Bakal Gelar IPO, Hillcon Bidik Dana Rp 884,6 Miliar', 'Hillcon Pasang Harga Penawaran Awal IPO di Kisaran Rp 250-Rp 400 Per Saham', 'Segera IPO, Hillcon Bidik Laba Bersih Hingga Rp 600 Miliar di Tahun Ini', 'Hillcon Lepas 2,21 Miliar Saham dalam Gelaran IPO, Bidik Dana Rp 884,6 Miliar', 'OJK Menyebut 57 Perusahaan Masuk Pipeline IPO Tahun Ini, Blibli Siap Melantai di BEI?', 'Incar Rp134,9 M dari IPO & Rp2,37 T dari Waran, Ini Rencana Mandiri Mineral Perkasa ', 'Blibli Dikabarkan Mau IPO, Bagaimana Prospeknya?', 'Ini Kata Analis Terhadap Rencana Blibli Gelar IPO di Tahun 2022', 'Blibli Dikabarkan Bersiap IPO Tahun Ini, Begini Pandangan Analis', 'Jadi Penghuni Baru Indeks LQ45, Saham GOTO Melesat 13,16% Lewati Harga IPO']\n",
      "Euronews\n",
      "chinatoday\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:17:23,265 - 2210765 - urllib3.connection - WARNING - Certificate did not match expected hostname: www.taipanpublishinggroup.com. Certificate: {'subject': ((('commonName', 'forextrading.company'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', \"Let's Encrypt\"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '034E402ECD154C425E1CA5CBD61A798897D6', 'notBefore': 'Jun 15 10:50:42 2022 GMT', 'notAfter': 'Sep 13 10:50:41 2022 GMT', 'subjectAltName': (('DNS', 'forextrading.company'),), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}\n",
      "2022-06-18 13:20:38,319 - 2210765 - urllib3.connection - WARNING - Certificate did not match expected hostname: www.hksfc.org.hk. Certificate: {'subject': ((('countryName', 'HK'),), (('localityName', 'Hong Kong'),), (('organizationName', 'Securities and Futures Commission'),), (('commonName', 'www.sfc.hk'),)), 'issuer': ((('countryName', 'GB'),), (('stateOrProvinceName', 'Greater Manchester'),), (('localityName', 'Salford'),), (('organizationName', 'Sectigo Limited'),), (('commonName', 'Sectigo RSA Organization Validation Secure Server CA'),)), 'version': 3, 'serialNumber': 'AE737834AA040781BAD541C7BF8075BB', 'notBefore': 'Jul 24 00:00:00 2020 GMT', 'notAfter': 'Oct 16 23:59:59 2022 GMT', 'subjectAltName': (('DNS', 'www.sfc.hk'), ('DNS', 'sfc.hk')), 'OCSP': ('http://ocsp.sectigo.com',), 'caIssuers': ('http://crt.sectigo.com/SectigoRSAOrganizationValidationSecureServerCA.crt',), 'crlDistributionPoints': ('http://crl.sectigo.com/SectigoRSAOrganizationValidationSecureServerCA.crl',)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al jazeera Qatar\n",
      "Al Jazeera not working\n",
      "Koreatimes\n",
      "zdnet\n",
      "zdnet not working\n",
      "Arabnews\n",
      "Chosun\n",
      "kngnet\n",
      "['https://www.koreanewsgazette.com/one-store-scraps-ipo-plan-due-to-weak-investor-confidence-sources/', 'https://www.koreanewsgazette.com/sk-shieldus-eyes-raising-up-to-3-55-tln-won-through-ipo-next-month/', 'https://www.koreanewsgazette.com/hyundai-mipo-wins-94-bln-won-product-carrier-order-in-asia/', 'https://www.koreanewsgazette.com/sk-ons-ipo-unlikely-to-take-place-until-after-2025-sk-innovation-ceo/', 'https://www.koreanewsgazette.com/money-raised-through-ipos-hits-record-high-in-2021-amid-ample-liquidity-bullish-sentiment/', 'https://www.koreanewsgazette.com/hyundai-mipo-dockyard-remains-in-red-in-2021/', 'https://www.koreanewsgazette.com/hyundai-mipo-dockyard-remains-in-red-in-q4/', 'https://www.koreanewsgazette.com/lg-to-unveil-life-size-omnipod-self-driving-concept-car/']\n",
      "kedg\n",
      "kedg not working\n",
      "Wealthx\n",
      "Antara\n",
      "Asia Insurance\n",
      "economic_times\n",
      "['https://economictimes.indiatimes.com/markets/ipos/fpos/reliance-unlikely-to-announce-retail-jio-ipo-in-agm-says-jp-morgan/articleshow/92271842.cms', 'https://economictimes.indiatimes.com/markets/ipos/fpos/innova-captab-likely-to-file-for-900-crore-ipo-soon/articleshow/92268333.cms', 'https://economictimes.indiatimes.com/markets/ipos/fpos/jk-files-submits-ipo-papers-with-sebi-to-raise-500-600-cr/articleshow/92241438.cms', 'https://economictimes.indiatimes.com/markets/ipos/fpos/phonepe-ceo-sameer-nigam-on-ipo-plan-i-had-no-idea/articleshow/92234758.cms', 'https://economictimes.indiatimes.com/markets/ipos/fpos/phonepe-prepping-for-ipo-seeks-valuation-of-usd-8-10-billion/articleshow/92229191.cms']\n",
      "prnnewswire\n",
      "Arab Finanace\n",
      "Vccircle\n",
      "Allafrica\n",
      "zawya\n",
      "['https://www.zawya.com/en/world/china-and-asia-pacific/beijing-gives-initial-nod-to-revive-ant-ipo-after-crackdown-cools-sources-b1izo76u', 'https://www.zawya.com/en/press-release/companies-news/addx-is-first-singapore-financial-institution-to-recognise-crypto-assets-of-accredited-investors-xcoejn3p', 'https://www.zawya.com/en/press-release/companies-news/addx-raises-us58mln-ijglok0v', 'https://www.zawya.com/en/markets/equities/indias-lic-shares-set-to-slide-in-market-debut-after-record-ipo-udct9yyt', 'https://www.zawya.com/en/opinion/markets-insights/how-wall-street-banks-made-a-killing-on-spac-craze-f449d3vq', 'https://www.zawya.com/en/markets/equities/retal-plans-ipo-and-listing-shares-on-saudi-stock-exchange-u2qbhjdz', 'https://www.zawya.com/en/press-release/companies-news/retal-urban-development-company-announcement-of-intention-to-float-on-the-saudi-stock-exchange-fmrhh8lb', 'https://www.zawya.com/en/press-release/people-in-the-news/nutanix-appoints-rukmini-sivaraman-as-chief-financial-officer-lh2qwav1', 'https://www.zawya.com/en/opinion/business-insights/how-covid-propelled-trillion-dollar-valuations-k1ofrumy', 'https://www.zawya.com/en/economy/abu-dhabis-economy-going-from-strength-to-strength-official-says-w6ciutl7', 'https://www.zawya.com/en/press-release/research-and-studies/iridium-study-finds-that-less-than-1-3-of-ipos-in-the-gcc-succeed-rdfzoenf', 'https://www.zawya.com/en/press-release/research-and-studies/iridium-study-finds-that-less-than-1-3-of-ipos-in-the-gcc-succeed-kya2lr0x', 'https://www.zawya.com/en/wealth/funds/oman-india-fund-to-invest-nearly-10mln-in-senco-gold-kiokkkz4', 'https://www.zawya.com/en/business/professional-services/millennial-secures-35mln-capital-commitment-from-us-group-f3rtc15h', 'https://www.zawya.com/en/press-release/companies-news/millennial-brands-secures-usd35mln-capital-commitment-jpugvl8w', 'https://www.zawya.com/en/press-release/companies-news/markaz-oil-crosses-120-b-and-propels-gcc-markets-even-higher-eth29nas', 'https://www.zawya.com/en/press-release/companies-news/saudi-corporates-funding-mix-is-evolving-nspzfslj', 'https://www.zawya.com/en/markets/equities/dubai-dewas-dividends-will-be-consistent-on-strong-cash-flows-g7oh73ys', 'https://www.zawya.com/en/press-release/companies-news/markaz-2021-marks-a-year-of-achievements-despite-challenges-driven-by-expertise-innovation-and-ongoing-progress-umpk9ht4', 'https://www.zawya.com/en/wealth/funds/kia-seeks-to-up-investments-capital-markets-targeted-hfoidhtj', 'https://www.zawya.com/en/markets/equities/superyacht-maker-ferretti-presses-ahead-with-1bln-hk-listing-despite-choppy-markets-sources-wjxhslko', 'https://www.zawya.com/en/press-release/companies-news/sir-anthony-ritossas-18th-gfois-welcomes-unicorn-hunters-the-creators-of-unicoin-ph62ls2s', 'https://www.zawya.com/en/press-release/events-and-conferences/sir-anthony-ritossas-18th-global-family-office-investment-summit-welcomes-unicorn-hunters-jh3r52b1', 'https://www.zawya.com/en/press-release/research-and-studies/adnoc-retains-title-of-most-valuable-brand-in-uae-uylt58lz', 'https://www.zawya.com/en/press-release/companies-news/investcorp-named-firm-of-the-year-mena-in-private-equity-internationals-award-for-the-fourth-consecutive-year-j3a126zh']\n",
      "Aljarida\n",
      "['https://www.aljarida.com/articles/1655404403142799400/', 'https://www.aljarida.com/articles/1655386968572020700/', 'https://www.aljarida.com/articles/1655404753802801500/', 'https://www.aljarida.com/articles/1655398638162422500/', 'https://www.aljarida.com/articles/1655535163588009400/', 'https://www.aljarida.com/articles/1655507282176404900/', 'https://www.aljarida.com/articles/1655491477555906600/', 'https://www.aljarida.com/articles/1655468022935330400/', 'https://www.aljarida.com/articles/1655394215442289400/', 'https://www.aljarida.com/articles/1655395156972304900/', 'https://www.aljarida.com/articles/1655394596892295900/', 'https://www.aljarida.com/articles/1655394360232291400/', 'https://www.aljarida.com/articles/1655536600158153300/', 'https://www.aljarida.com/articles/1655507141056331100/', 'https://www.aljarida.com/articles/1655423492634747300/', 'https://www.aljarida.com/articles/1655423442664746700/', 'https://www.aljarida.com/articles/1655538663748158200/', 'https://www.aljarida.com/articles/1655537212088154500/', 'https://www.aljarida.com/articles/1655536099088128200/', 'https://www.aljarida.com/articles/1655507386446465000/', 'https://www.aljarida.com/articles/1655491189595905500/', 'https://www.aljarida.com/articles/1655400702802707400/', 'https://www.aljarida.com/articles/1655401066142718800/', 'https://www.aljarida.com/articles/1655400815512713400/', 'https://www.aljarida.com/articles/1655506761766163500/', 'https://www.aljarida.com/articles/1655481370795692600/', 'https://www.aljarida.com/articles/1655480372205691500/', 'https://www.aljarida.com/articles/1655476762145515000/', 'https://www.aljarida.com/articles/1655399512262655100/', 'https://www.aljarida.com/articles/1655396494022355600/', 'https://www.aljarida.com/articles/1655396684442361700/', 'https://www.aljarida.com/articles/1655396927792371600/', 'https://www.aljarida.com/articles/1643208767910217000/', 'https://www.aljarida.com/articles/1632757382053976300/', 'https://www.aljarida.com/articles/1621263866321751100/', 'https://www.aljarida.com/articles/1619707912396232300/', 'https://www.aljarida.com/articles/1616517361283975900/', 'https://www.aljarida.com/articles/1611593100831065500/', 'https://www.aljarida.com/articles/1599582428072951000/', 'https://www.aljarida.com/articles/1589289936299863000/', 'https://www.aljarida.com/articles/1578925320746051600/', 'https://www.aljarida.com/articles/1578935072036835200/', 'https://www.aljarida.com/articles/1655538663748158200/', 'https://www.aljarida.com/articles/1655537212088154500/', 'https://www.aljarida.com/articles/1655536600158153300/', 'https://www.aljarida.com/articles/1655536099088128200/', 'https://www.aljarida.com/articles/1655535163588009400/', 'https://www.aljarida.com/articles/1655507386446465000/', 'https://www.aljarida.com/articles/1655507329096430900/', 'https://www.aljarida.com/articles/1655507282176404900/', 'https://www.aljarida.com/articles/1655507141056331100/', 'https://www.aljarida.com/articles/1655506866106167100/', 'https://www.aljarida.com/articles/1655506761766163500/', 'https://www.aljarida.com/articles/1655495842886161500/', 'https://www.aljarida.com/articles/1655491477555906600/', 'https://www.aljarida.com/articles/1655488992655904400/', 'https://www.aljarida.com/articles/1655487798525903300/', 'https://www.aljarida.com/articles/1655484580205694800/', 'https://www.aljarida.com/articles/1655482521045693700/', 'https://www.aljarida.com/articles/1655481370795692600/', 'https://www.aljarida.com/articles/1655480372205691500/', 'https://www.aljarida.com/articles/1655477320465515900/', 'https://www.aljarida.comhttps://storage.googleapis.com/jarida-cdn/pdfs/1655407248663157800/1655407269000/file.pdf', 'https://www.aljarida.com/articles/1655506866106167100/', 'https://www.aljarida.com/articles/1655506761766163500/', 'https://www.aljarida.com/articles/1655507282176404900/', 'https://www.aljarida.com/articles/1655507386446465000/', 'https://www.aljarida.com/articles/1655507141056331100/', 'https://www.aljarida.com/articles/1655186408134342400/', 'https://www.aljarida.com/articles/1655221513845431200/', 'https://www.aljarida.com/articles/1655231078525974000/', 'https://www.aljarida.com/articles/1655315198549530800/', 'https://www.aljarida.com/articles/1655298845448952400/', 'https://www.aljarida.com/articles/1654363527739264700/', 'https://www.aljarida.com/articles/1654784521384207800/', 'https://www.aljarida.com/articles/1654364103509289300/', 'https://www.aljarida.com/articles/1654084996554750000/', 'https://www.aljarida.com/articles/1654363594819267900/', 'https://www.aljarida.com/articles/1654451968047897800/', 'https://www.aljarida.com/articles/1654447658116648100/', 'https://www.aljarida.com/articles/1654076963353447900/', 'https://www.aljarida.com/articles/1655506866106167100/', 'https://www.aljarida.com/articles/1655313385679478300/', 'https://www.aljarida.com/articles/1655399851432675200/', 'https://www.aljarida.com/articles/1655230676455969900/', 'https://www.aljarida.com/articles/1655139318312656500/', 'https://www.aljarida.com/articles/1655221513845431200/', 'https://www.aljarida.com/articles/1654363527739264700/', 'https://www.aljarida.com/articles/1654182798303922900/', 'https://www.aljarida.com/articles/1654448297966707200/', 'https://www.aljarida.com/articles/1654190869334765900/', 'https://www.aljarida.com/articles/1654440865446045800/', 'https://www.aljarida.com/articles/1655399702962665400/']\n",
      "DataFrame is blank\n",
      "Dziennik\n",
      "Swissinfo\n",
      "IPO EinNews\n",
      "EINnews not working\n",
      "Sabah\n",
      "Livemint India\n",
      "livemint: err: Empty dataframe\n",
      "Livemint not working\n",
      "Bahamas\n",
      "Azernews\n",
      "['https://www.azernews.az/region/158329.html', 'https://www.azernews.az/region/157895.html', 'https://www.azernews.az/region/193841.html', 'https://www.azernews.az/oil_and_gas/159169.html', 'https://www.azernews.az/region/155527.html', 'https://www.azernews.az/region/158089.html', 'https://www.azernews.az/region/158106.html', 'https://www.azernews.az/oil_and_gas/151598.html', 'https://www.azernews.az/region/154768.html', 'https://www.azernews.az/region/150410.html', 'https://www.azernews.az/region/159338.html', 'https://www.azernews.az/business/150521.html']\n",
      "chosenilboenglish\n",
      "['http://english.chosun.com/site/data/html_dir/2021/04/15/2021041500417.html', 'http://english.chosun.com/site/data/html_dir/2021/07/09/2021070900458.html', 'http://english.chosun.com/site/data/html_dir/2021/03/11/2021031101655.html', 'http://english.chosun.com/site/data/html_dir/2021/11/12/2021111200575.html', 'http://english.chosun.com/site/data/html_dir/2021/03/19/2021031901360.html', 'http://english.chosun.com/site/data/html_dir/2021/03/15/2021031501373.html', 'http://english.chosun.com/site/data/html_dir/2020/09/22/2020092200559.html', 'http://english.chosun.com/site/data/html_dir/2021/12/04/2021120400575.html', 'http://english.chosun.com/site/data/html_dir/2021/07/14/2021071401132.html']\n",
      "Brazil\n",
      "Otempo brazil not working\n",
      "Chile\n",
      "lastampa Italy\n",
      "Liputan Indo\n",
      "Mexico\n",
      "Mexico not working\n",
      "New Zealand\n",
      "supchina\n",
      "Romania Insider Dominician Republic\n",
      "Aquila (ticker: AQ), one of the biggest distributors of fast-moving consumer goods (FMCG) in Romania has raised RON 367 mln (EUR 74 mln) in its initial public offering (IPO) on the Bucharest Stock Exchange. This is the largest IPO competed by a private company in Romania in the last four years.The final IPO price was set at RON 5.5 per share, at the bottom end of the interval targeted by the company. The demand from retail investors was lower than in the previous IPOs carried out on the Bucharest Stock Exchange this year. Still, the tranche allotted to individual investors, which represented 15% of the total number of shares on sale, was 100% underwritten.âAquila completed a capital increase of RON 367 million (equivalent to EUR 74 million) through an initial public offering, this being the largest primary public offer made by a company with private shareholding through the Bucharest Stock Exchange. The successful completion of the Public Offer and the trust received from investors give us the opportunity to continue the path of expanding and consolidating the Aquila business to another level,â said Jean Dumitrescu, Investor Relations Director, Aquila.The new shares sold in the IPO represent a third of the companyâs total number of shares after the transaction. Aquila was thus valued at EUR 222 mln, which is 18.3 times more than the net profit reported for 2020 - EUR 12.1 mln.Local brokerage firm Swiss Capital managed the IPO and BRK Financial Group and Tradeville were distribution agents. Local law firm Schoenherr si Asociatii provided the legal assistance for the IPO.editor@romania-insider.com(Photo source: Aquila Facebook page)\n",
      "Romaniaâs biggest naval transport group â Transport Trade Services (TTS) â will carry out an initial public offering (IPO) through which its current shareholders will sell shares worth up to RON 315 mln (EUR 64 mln).The company has set a price range of RON 19-21 for its shares in the IPO. The offer will take place between May 24 and June 4 and will have two tranches: one for institutional investors and one for retail investors.Local brokerage firm Swiss Capital will manage the IPO, with Tradeville and BRK Financial Group part of the distribution group.The group's shareholders will sell half of their holdings, adding up to 15 million shares or 50% of the company's total equity. The targeted valuation for the whole company is thus RON 630 mln (EUR 128 mln), at the top of the IPO price range.TTS group recorded consolidated revenues of RON 520 mln (EUR 107 mln), down 5.7% compared to 2019, while its net profit was RON 48 mln (almost EUR 10 mln), down 9.4% year-on-year.The group's majority shareholder is local businessman Mircea Mihailescu, who owns a 50.4% stake. Notably, the third-biggest shareholder is former finance minister Viorel Stefan, who holds a 10.1% stake.The IPO prospectus is available here.editor@romania-insider.com\n",
      "The shares of Romanian developer One United (ONE) have lost 9% since their listing on the Bucharest Stock Exchange as some investors were disappointed with their weak debut.On July 12, the companyâs shares started trading on BVB at RON 2.14, up 7% compared to the IPO price of RON 2 but closed the first trading day just under RON 2. After that, the share price gradually went down, culminating with a 5.4% drop on Monday, July 19, amid turbulence in international equity markets.One United closed the trading session on Wednesday, July 21, at RON 1.815, down 9% compared to the IPO price.The negative price evolution prompted brokerage firm BRK Financial Group to step in and try to stabilize the price through purchases. The IPOâs intermediary can buy up to 7.5 million shares in the first 30 days after listing to counter negative price evolutions. So far, it has bought some 450,000 shares representing about 15% of the trading volume in the respective days.One United, one of the fastest-growing real estate developers in Romania, raised RON 260 million (EUR 53 mln) in its initial public offering, which ended on July 2 at a pre-money valuation of EUR 530 mln, or a post-IPO capitalization of over EUR 580 mln.The company reported consolidated revenues of RON 542 mln (EUR 111 mln) and a net profit of RON 178 mln (EUR 36.5 mln) in 2020.editor@romania-insider.com(Photo source: BVB.ro)\n",
      "Aquila, one of the biggest distributors of fast-moving consumer goods (FMCG) in Romania, aims to raise up to RON 433 mln (EUR 87.5 million) through an initial public offering (IPO) on the Bucharest Stock Exchange.This is the third major IPO of a private company on the Bucharest Stock Exchangeâs main market this year, after those carried out by local transport group TTS and real estate develop One United.Aquila plans to use most of the money raised in the IPO (between 50% and 70%) to buy other FMCG companies while the rest of the funds will go into digitization projects, developing the companyâs brands and working capital.The company will sell 66.66 million shares representing a third of its total number of shares after the IPO, targeting a post-money valuation of EUR 263 mln. The pre-monet valuation of EUR 175 mln stands for a multiple of about 14.5 times the net profit reported for 2020, which was approximately RON 60 mln (EUR 12.1 mln).Aquila recorded revenues of RON 1.79 bln (EUR 362 mln) in 2020, up 6% compared with 2019, despite a 25% drop in the volume of goods distributed, especially to HORECA companies, because of the pandemic. However, the average price of the distributed goods increased by 27% due to the lower share of cheaper products distributed to HORECA, the company said in its IPO prospectus.With a history of 26 years, Aquila is one of the biggest FMCG distributors in Romania, serving a network of about 67,000 points of sale, which includes traditional retailers, modern retailers and HORECA companies. Its top partners include Unilever, Mars, Ferrero, Philips, Essity, Coca-Cola, Lavazza and ETI Romania. The companyâs founders are Romanian entrepreneurs Constantin-Cataliu Vasile and Alin-Adrian Dociu, who currently hold 50% of its shares each. After the IPO, they will hold stakes of 33.3% of the company.The public offer starts on Monday, November 8 and ends on November 16. The IPO will have two tranches, one dedicated to institutional investors, which will represent 85% of the shares on sale, and one dedicated to retail investors, representing 15% of the shares on sale. The retail tranche will also be split into two sub-tranches: one with guaranteed allocation (investors get the exact number of shares underwritten) on a first-come-first-served basis and the other one with pro-rata allocation. The minimum subscription for retail investors is 750 shares or RON 4,875 (almost EUR 1,000) while the maximum is 12,000 shares.Local brokerage firm Swiss Capital manages the IPO and BRK Financial Group and Tradeville are distribution agents.editor@romania-insider.com(Photo source: the company's Facebook page)\n",
      "Romanian software company Bitdefender, a global leader in the cyber security solutions market, officially confirmed that it had started procedures for future listing in the US.\"Bitdefender Holding BV has confidentially filed a draft Form F-1 registration statement with the US Capital Markets Authority (SEC) for a future initial public offering (IPO) in the United States,\" the company announced.The timing, size and price for the future offer are to be determined. The initial public offering is subject to evaluations by the SEC and other regulatory processes, as well as the market and other conditions.Bitdefender hired investment banks JP Morgan and Morgan Stanley to help it with the launch of an IPO on the US market, local media announced over the weekend.The IPO would envisage a valuation of nearly USD 2 bln for the Romanian company, according to sources familiar with the deal quoted by Ziarul Financiar daily.(Photo: LCVA | Dreamstime.com)andrei@romania-insider.com\n",
      "Romaniaâs energy minister Razvan Nicolescu had a meeting with representatives of the European Bank for Reconstruction and Development (EBRD) in London, on Tuesday, June 18, to talk about EBRDâs participation to Electricaâs IPO and the possibility to become a long term investor in the company.âWe expressed our wish and trust that EBRD will be a long term partner for the Romania state in Electrica and that it will use its vast experience to support implementing international corporate governance standards in the company. We hope this IPO will promote Electrica and, implicitly, Romania as a good business destination, as well as stable and predictable for investments,â Nicolescu said.Romanian state owned electricity distributor Electrica launched, on June 16, an initial public offering (IPO) to sell 177.19 million shares, which make some 51 percent of the companyâs capital. The company aims to get between EUR 440 million and EUR 540 million from the IPO.EBRD might be one of the largest investors in the Electrica IPO. The representatives of the financial institution haveÂ already been involved in the making of the new incorporation document for Electrica, and insisted that some clauses beÂ introduced which will not allow the Romanian state to take important decisions for the company by itself, sources from the capital market told Romania-Insider.com.After the IPO, the Romanian state will remain the companyâs main shareholder, with 49 percent of the voting rights, this is why institutional investors wanted guarantees that the state will not be able to take decisions incongruous with the interests of the other shareholders.EBRD might hold between 5 percent and 10 percent of the company and will have a representative in the companyâs administrative board, the same sources said.The Romanian energy minister and the companyâs management, together with members of the intermediation consortium for the IPO, met with EBRD at the start of their international road-show which is part of the IPO marketing to international institutional investors. The road-show will cover all the major financial centers in Europe as well as New York and Boston in the U.S.The IPO will end on June 25. Citigroup, Societe General, Raiffeisen Bank Romania, BRD and Swiss Capital make the consortium which intermediates the offering.The small tranche for individual investors in Electricaâs IPO was subscribed in just two days.Andrei Chirileasa, andrei@romania-insider.com\n",
      "UiPath, the biggest robotic processing automation (RPA) company in the world, launched by two Romanian entrepreneurs in Bucharest in 2005, has set a price of USD 56 per share for its initial public offering (IPO), according to an official announcement.The total number of shares sold in the IPO is 23.89 million, which places the total value of the deal at over USD 1.33 bln. The companyâs valuation was thus set at USD 29 bln. UiPathâs shares will start trading on the New York Stock Exchange (NYSE) Today (April 21) under the ticket PATH.The company will sell 9.41 mln new shares in the IPO, for which it will get USD 527 mln. Existing shareholders, including co-founder and CEO Daniel Dines, will sell another 14.47 mln shares worth USD 810 mln. In addition, UiPath has granted the underwriters a 30-day option to purchase up to 3.58 million additional shares of Class A common stock at the IPO price.Daniel Dines will sell about 1.38 million UiPath shares in the IPO, for which he will get USD 77 mln. After the IPO, he will still hold 109.5 mln UiPath class A and Class B shares, worth USD 6.1 bln (at the IPO price). Dines thus officially becomes the richest Romanian. Currently, former tennis player Ion Tiriac (81) is the only Romanian in the Forbes list of the world's billionaires, with a fortune of USD 1.7 bln.UiPath filed the documentation for its NYSE IPO at the end of March. In mid-April, the company announced a price range of USD 43-50 per share for its IPO, which implied a valuation of up to USD 26 bln. A few days later, the company increased the price range to USD 52-54 per share and the number of shares it planned to sell in the IPO from 6.8 mln to 9.4 mln.The company was most recently valued at USD 35 billion in the latest funding round earlier this year. In the financial year ended in January 2021, UiPath recorded annualized recurring revenues (ARR) of USD 580 mln, up by 65% compared to the previous year, and a net loss of USD 92 mln, down from USD 520 mln in the previous year.Morgan Stanley and J.P. Morgan are the lead bookrunners for UiPath's IPO. BofA Securities, Credit Suisse, Barclays, and Wells Fargo Securities are active bookrunners while SMBC Nikko, BMO Capital Markets, Mizuho Securities, KeyBanc Capital Markets, TD Securities, Truist Securities, Cowen, Evercore ISI, Macquarie Capital, Nomura, and RBC Capital Markets are passive bookrunners. Canaccord Genuity, D.A. Davidson & Co., Oppenheimer & Co., and Needham & Company are co-managers for the offering.editor@romania-insider.com(Photo source: UiPath Facebook page)\n",
      "Romaniaâs electricity distributor Electricaâs initial public offering (IPO) should be fully subscribed by Monday, June 23, two days before it is set to end, energy minister Razvan Nicolescu said.Electrica, which is currently 100 percent state-owned, launched an IPO on Monday, June 16, to sell 177.19 million shares, representing 51 percent of the companyâs capital. The minimum value of the offer is EUR 443 million, as the company sells its shares at a minimum of RON 11 and a maximum of RON 13.5. The shares will then be listed on the Bucharest Stock Exchange (BVB).A part of the offer will be structured in global depository receipts (GDRs) which are financial instruments to be traded on the London Stock Exchange. One GDR stands for four Electrica shares and the price for GDRs is between USD 13.55 and USD 16.63.âElectricaâs IPO is going well. I expect that it will be fully subscribed by Monday, June 23, at the latest,â Nicolescu said.He went in an international roadshow to promote the IPO along with the companyâs management and the brokers of the offering. They first met with representatives of the European Reconstruction and Development Bank (EBRD) in London, on Tuesday, to talk about EBRDâs participation in the IPO as a long term investor in Electrica.âElectrica is a company with good perspectives and thatâs why its prospectus is worth analyzing. We wish it to become a model for Romania on corporate governance. The agreement with EBRD on this matter will help the company a lot,â Nicolescu mentioned.According to the prospectus, 85 percent of the shares in the IPO are allotted for institutional investors and 15 percent for individual investors, which have two tranches, one for small subscriptions, of up to 20,000 shares, which is 7 percent of the total number of shares, and one for large subscriptions of more than 20,000 shares, which stands for 8 percent. The final allotment may change depending on the level of subscriptions on each tranche.The small tranche has already been oversubscribed by 76 percent. On Thursday, June 19, at 17:00, there were more than 10,500 orders registered on this tranche, for a total of 21.87 million shares. Some 7 million shares were subscriptions smaller than 1,000 shares, which get guaranteed allocation (one share allotted for each share subscribed), while some 14.8 million shares were subscriptions larger than 1,000 shares, which get a 5 percent discount on the final IPO price.On the tranche for large subscriptions, there were 68 orders for almost 4 million shares, which is about 28 percent of the number of shares dedicated to this tranche.The total value of subscriptions from individual investors is EUR 67 million.Andrei Chirileasa, andrei@romania-insider.com\n",
      "Small individual investors who will subscribe on the small tranche of Romanian electricity distributor Electrica initial public offering (IPO) will have the option to choose between having their orders guaranteed up to 1,000 shares or getting a 5 percent discount to the final IPO price, according to the prospectus. The procedure, however, is not the easiest.The tranche for small investors in the Electrica IPO is 7 percent of the total number of shares sold by the company. That is 12.4 million shares out of a total of 177.19 million shares. The price interval for the IPO is RON 11 to 13.5 per share.Subscriptions for the small retail tranche start at a minimum of 250 shares and go up to 20,000 shares. When making the subscription, investors must pay the maximum price per share of RON 13.5. At the end of the IPO period, which is June 16-25, the price of the IPO will be set based on subscriptions made by institutional investors, who get 85 percent of the total shares.Retail investors will pay the same price as institutional investors. For example, if this price is RON 12 per share, small investors will get back the difference of RON 1.5 per share.Small investors who make their subscriptions in the first five days of the offering period (up until and including June 20) will get a 5 percent discount to the final price. Again, as an example, if the price is RON 12, then the discounted price would be RON 11.4. However, investors who choose to have their subscriptions guaranteed do not get the discount.Investors can choose to have their subscriptions guaranteed up to 1,000 shares. In this case, the advantage is that they know from the start how many shares they will get in the IPO, so the allocation ratio is one on one. A maximum of 10 million shares will be set aside for guaranteed subscriptions, with a first come, first served policy.Itâs uncertain if an investor who wants to subscribe between 250 and 1,000 shares can also choose not to have his subscriptions guaranteed, in order to get the discount. However, in this case he could get significantly less shares than the number he applies for, because the allocation will be pro-rata. So if the non-guaranteed portion of the tranche is five time oversubscribed for example, he will get only 20 percent of the shares he subscribed.Small investors who want to subscribe more than 1,000 shares also have more options. For example, someone who wants to subscribe 10,000 shares can make two orders, one for 1,000 shares with guaranteed subscription, and one for 9,000 shares non-guaranteed, for which he gets the 5 percent discount. He can also go with a single order for 10,000 shares non-guaranteed.Itâs not the easiest subscription procedure and investors will have to be very careful how they structure their orders in order to get the expected results. Assistance from a broker would prove useful, but even among brokers there are different interpretations given to the prospect. Hopefully all these will be clarified by Monday, June 16, when the offer starts.Investors in Romania can subscribe in the Electrica IPO through BRD, Raiffeisen Bank and brokerage firm Swiss Capital, which are members of the intermediary consortium. But most local brokerage firms can also receive subscriptions from investors which they pass forward to the syndicate.More details in the IPO prospectus.Andrei Chirileasa, andrei@romania-insider.comÂ\n",
      "Romania's Government will support Fondul Proprietatea in its attempt to list the shares of the salt company Salrom on the Bucharest Stock Exchange (BVB) under an IPO likely to take place in the first half of next year, minister of economy Florin Spataru announced.Fondul Proprietatea holds 49% of Salrom.\"In May next year, Salrom's shares will be listed. (â¦) Fondul Proprietatea wants to exit and sell its shares on BVB. They came up with a plan for the transaction, which, according to our estimates, will be completed in the first half of next year. The other shareholder, the Ministry of Economy, agrees in principle, and we will work together for this listing,\" minister Spataru said during a working visit to Salina SlÄnic Prahova, G4media.ro reported.andrei@romania-insider.com(Photo source: Facebook/Florin Spataru)\n",
      "CNBC Barbados\n",
      "AIF Russia\n",
      "Monitor uganda\n",
      "the sun UK\n",
      "Parool Netherlands\n",
      "Netherlands is not working\n",
      "Oman shabiba\n",
      "Koreannewsgazette\n",
      "['https://www.koreanewsgazette.com/one-store-scraps-ipo-plan-due-to-weak-investor-confidence-sources/', 'https://www.koreanewsgazette.com/sk-shieldus-eyes-raising-up-to-3-55-tln-won-through-ipo-next-month/', 'https://www.koreanewsgazette.com/hyundai-mipo-wins-94-bln-won-product-carrier-order-in-asia/', 'https://www.koreanewsgazette.com/sk-ons-ipo-unlikely-to-take-place-until-after-2025-sk-innovation-ceo/', 'https://www.koreanewsgazette.com/money-raised-through-ipos-hits-record-high-in-2021-amid-ample-liquidity-bullish-sentiment/', 'https://www.koreanewsgazette.com/hyundai-mipo-dockyard-remains-in-red-in-2021/', 'https://www.koreanewsgazette.com/hyundai-mipo-dockyard-remains-in-red-in-q4/', 'https://www.koreanewsgazette.com/lg-to-unveil-life-size-omnipod-self-driving-concept-car/']\n",
      "Hungary\n",
      "Hungary not working\n",
      "jauns\n",
      "['https://jauns.lv/raksts/bizness/504222-indexo-latvijai-ir-nepieciesama-nozimiga-vieteja-kapitala-banka', 'https://jauns.lv/raksts/bizness/486641-airbaltic-cer-sasniegt-miljarda-eiro-apgrozijumu-2026gada', 'https://jauns.lv/raksts/bizness/485953-indexo-birza-plano-piesaistit-5-6-miljonus-eiro', 'https://jauns.lv/raksts/bizness/484618-indexo-plano-sakt-akciju-kotaciju-birza-nasdaq-riga-un-dibinat-banku', 'https://jauns.lv/raksts/bizness/478234-eiropas-komisija-atlavusi-latvijai-airbaltic-pamatkapitala-investet-45-miljonus-eiro', 'https://jauns.lv/raksts/zinas/460884-gatis-kokins-politikas-un-baleta-nebus-bet-nauda-bus-visiem', 'https://jauns.lv/raksts/bizness/434979-delfingroup-plano-akciju-sakotnejo-publisko-piedavajumu-nasdaq-riga', 'https://jauns.lv/raksts/bizness/432598-gauss-ietekme-uz-aviacijas-nozari-ir-pratam-neaptverama', 'https://jauns.lv/raksts/sports/53686-speka-sportistam-kindzulim-otra-vieta-prestizas-sacensibas-vacija', 'https://jauns.lv/raksts/bizness/442100-par-nebanku-kreditetaja-delfingroup-netieso-lidzipasnieku-kluvusi-kesenfeldu-gimene', 'https://jauns.lv/raksts/bizness/142281-facebook-sasniedz-vienu-miljardu-lietotaju', 'https://jauns.lv/raksts/bizness/44815-vai-tviterim-pedejie-ciepstieni-jauni-lietotaji-klat-nenak-akciju-cena-rekordzema', 'https://jauns.lv/raksts/zinas/50812-rigas-valsts-1gimnazijas-skolniece-darta-sanem-bronzu-starptautiskaja-filozofijas-olimpiade', 'https://jauns.lv/raksts/bizness/459308-kina-plano-liegt-tehnologiju-uznemumiem-kotet-akcijas-arvalstis', 'https://jauns.lv/raksts/bizness/362346-saudi-aramco-klust-par-pasaule-lielako-birza-koteto-uznemumu']\n",
      "Pulse\n",
      "Pulse not working\n",
      "vnexpress\n",
      "['https://vnexpress.net/ipo-la-gi-4459152.html', 'https://vnexpress.net/ttg-holdlings-ki-vong-ipo-trong-nam-2025-4472635.html', 'https://vnexpress.net/chu-tich-aura-capital-chia-se-kinh-nghiem-chuan-bi-ipo-cho-smes-4472648.html', 'https://vnexpress.net/nova-consumer-cong-bo-gia-co-phan-ipo-4435110.html', 'https://startup.vnexpress.net/tin-tuc/hanh-trinh-khoi-nghiep/bukalapak-chuan-bi-ipo-4307403.html', 'https://vnexpress.net/tp-hcm-dao-tao-500-startup-huong-toi-ipo-4400998.html', 'https://vnexpress.net/vng-du-kien-ipo-tai-my-4340110.html', 'https://vnexpress.net/airbnb-nop-don-ipo-4193122.html', 'https://vnexpress.net/cach-startup-dau-tien-cua-viet-nam-ipo-tai-nhat-ban-4420207.html', 'https://vnexpress.net/doanh-nghiep-chay-dua-ipo-ky-luc-4300883.html', 'https://startup.vnexpress.net/tin-tuc/xu-huong/lalamove-nop-ho-so-ipo-tai-my-4299931.html', 'https://vnexpress.net/airbnb-nop-don-ipo-4149389.html', 'https://vnexpress.net/cong-ty-cong-nghe-viet-dau-tien-ipo-tai-nhat-ban-4407266.html', 'https://vnexpress.net/doanh-nghiep-trung-quoc-bi-cam-ipo-tai-my-4333411.html', 'https://vnexpress.net/didi-uber-trung-quoc-nop-don-ipo-tai-my-4292453.html', 'https://startup.vnexpress.net/tin-tuc/xu-huong/ipo-se-khai-sinh-them-nhieu-startup-dong-nam-a-4292261.html', 'https://startup.vnexpress.net/tin-tuc/hanh-trinh-khoi-nghiep/nhung-thach-thuc-grab-phai-doi-mat-sau-ipo-4265891.html', 'https://vnexpress.net/uber-trung-quoc-bi-mat-nop-don-ipo-tai-my-4261189.html', 'https://vnexpress.net/ong-trinh-van-quyet-muon-ipo-bamboo-airways-tai-my-4263069.html', 'https://vnexpress.net/cong-ty-cua-jessica-alba-nop-don-ipo-4260907.html', 'https://startup.vnexpress.net/tin-tuc/xu-huong/ky-lan-giao-duc-cua-my-chuan-bi-ipo-4258872.html', 'https://vnexpress.net/trung-quoc-se-cam-cac-cong-ty-co-du-lieu-quan-trong-ipo-o-nuoc-ngoai-4347924.html', 'https://vnexpress.net/lei-jun-lan-dau-tiet-lo-ngay-ipo-tham-hai-cua-xiaomi-4340355.html', 'https://startup.vnexpress.net/tin-tuc/hanh-trinh-khoi-nghiep/ipo-con-dao-2-luoi-cho-cac-ky-lan-4237294.html']\n",
      "vnexpress not working\n",
      "jamaicaobserver\n",
      "independent\n",
      "Albania\n",
      "EWN\n",
      "err: Failed to find date in page. Link: https://ewnews.com/i-was-just-trying-to-get-him-help-mother-of-inmate-found-dead-says-mentally-ill-son-was-supposed-to-go-to-sandilands\n",
      "Bloombergquint\n",
      "https://www.bloombergquint.com/pti/deltatech-gaming-files-rs-550-crore-ipo-papers-with-sebi\n",
      "https://www.bloombergquint.com/author/27956/pti\n",
      "https://www.bloombergquint.com/business/ipos-sebi-mulls-option-for-pre-filing-of-offer-documents\n",
      "https://www.bloombergquint.com/author/27956/pti\n",
      "https://www.bloombergquint.com/pti/5g-spectrum-auction-dot-invites-players-for-pre-bid-conference-on-june-20\n",
      "https://www.bloombergquint.com/author/27956/pti\n",
      "https://www.bloombergquint.com/pti/notification-allowing-linking-aadhaar-with-voter-list-issued\n",
      "https://www.bloombergquint.com/author/27956/pti\n",
      "Bloomberg Quint : err: None type object found for https://www.bloombergquint.com/author/27956/pti\n",
      "Bloomberg Quint : err: None type object found for https://www.bloombergquint.com/author/27956/pti\n",
      "Bloomberg Quint : err: None type object found for https://www.bloombergquint.com/author/27956/pti\n",
      "Bloomberg Quint : err: None type object found for https://www.bloombergquint.com/author/27956/pti\n",
      "ECNS\n",
      "{'title': ['Xi underscores improved capability in fighting corruption to ensure full victory', '1 killed in Shanghai petrochemical company fire', \"China's renewable energy capacity expands in first five months\", \"China refutes U.S. ambassador's remarks over anti-epidemic policy\", \"Fire breaks out in Shanghai's petrochemical firm\", 'Chinese mainland reports 11 new local confirmed COVID-19 cases', 'Cradle of Civilization: Niuheliang Site', 'Chinese president addresses 25th St. Petersburg International Economic Forum', 'China sees progress in stabilizing employment', \"Horn of Africa could face 'unprecedented' food insecurity in 2023, WFP warns\", 'China to allow more international flights', 'WTO secures unprecedented package of trade outcomes at Ministerial Conference', 'Xinjiang expands investment in energy sector', 'China subsidizes over 69 mln people in accessing basic medical insurance in Q1', 'China has 3,013 higher education institutions', 'China always stands on the side of peace and justice: Chinese FM', 'China has world largest monitoring network for disease, health risk factors', \"Beijing's Fengtai railway station to reopen after expansion\", \"British home secretary approves Assange's U.S. extradition\", 'Butterfly dragonfly spotted in Jiangxi', \"New findings at China's Sanxingdui Ruins stun archaeologists\", \"World's first railway loop line circling a desert put into operation\", \"State-owned companies backbones of China's high-quality growth: official\", 'China restores over half of its desertified land', 'WTO members agree on key issues at ministerial conference', 'China to build more national air monitoring stations', 'Deserts retreat as China plants millions of trees', 'China bird rescuer identifies species with birdsong', 'Sand control a priority in Hotan-Ruoqiang Railway construction', '666 drones put on spectacular light show in Jiangxi', 'Beijing Fengtai Railway Station ready to be put into service', 'Spectacular fishscale clouds appear over Jinshanling Great Wall', \"China's disease control makes big stride in 10 years\", 'Nansha port turns smarter, more efficient', 'Cancer treatment by Fosun reaches new user milestone', 'Shanghai scientists make important discovery in heat resistance genes in rice', 'Parcel delivery growth remains steady in China', 'China launches 3rd aircraft carrier', 'China renews yellow alert for high temperatures', 'One dead, two injured in Alabama church shooting', \"Russia's Pacific Fleet conducts training in Philippine Sea\", \"China's commercial space telescope completes all-sky optical survey\", 'China harvests first batch of self-bred deep-sea Atlantic salmons', 'China launches third aircraft carrier', 'President Xi to chair 14th BRICS Summit', 'Ancient sculpture remnants reunited after 3,000 years', \"China opens world's first desert rail loop in Xinjiang\", 'Night scenery of cross-sea bridge in Zhejiang', 'Giant panda Shuang Shuang celebrates 35th birthday in Mexico(1/3)', 'Government to provide more jobs', '$84 bln deals shine spotlight on Shanghai', 'Northern China swelters as mercury rises as high as 40 C', 'Online, telecom fraud to get heavier penalties', 'Multiple people shot at church in U.S. state of Alabama -- police', 'Questions remain over acute hepatitis cases', 'Trade expansion in 2022 to deepen resilience', 'City vows to thoroughly resolve all crime tips', 'Moon water may have originated below ground', \"Three-stage engine of China's new manned carrier rocket to enter prototype development\", 'China issues guidance on acute severe hepatitis of unknown causes in children for early recognition', 'Russia fines Google for not localizing user data', 'China prepares ground recipient system for space-based solar power station', 'Recovery expected in China despite Fed hike', 'Frontier areas see decade of transformation', \"'Russian Davos' moves forward\", 'Archaeological find in Sanxingdui casts new light on ancient rituals', 'Team China targets gold-laden worlds', 'Pro-growth policies set to stabilize economy', \"China inaugurates world's first desert rail loop in Xinjiang\", \"Japan's PM remains positive about raising defense R&D spending\", 'China accelerates revising industry catalog to encourage foreign investment', 'China to support private firms, boost private investment', 'Chinese economy revs up recovery engine as May data improves', 'Xinjiang expands subsidies to stabilize employment', \"Chinese scientists identify indigenous lunar water in Chang'e-5 samples\", \"Zelensky, European leaders discuss Ukraine's EU integration, conflict with Russia\", 'China, Russia hold business dialogue to seek closer cooperation', 'Chinese diplomat urges U.S. to take concrete actions to compensate Afghans', '8 injured, 6 missing in factory blast in NW China', \"China's commercial space telescope completes all-sky optical survey\", \"Experts call for more cooperation with China to revitalize Pakistan's agriculture sector\", \"China's national lawmakers to mull laws on wire fraud, civil enforcement, monopoly\", 'China-developed COVID-19 mRNA vaccine as booster safe, immunogenic in clinical trial', 'Chinese brands build up strength amid headwinds from COVID-19', 'China to recruit 67,000 rural teachers this year', 'China confident on keeping foreign trade within reasonable range in 2022', 'China welcomes overseas investment in R&D: ministry', 'RCEP offers broader room for China-ROK cooperation: official', '(W.E. Talk) Facts and Truths Show Workers of All Ethnic Groups in Xinjiang Enjoy Decent Work', 'Too many cases where US shields and exonerates military from its crimes: FM Says', 'Xinjiang attracts over 200 bln yuan of investment in Jan.-May', 'China-Central Asia pipeline transports over 400 bln cubic meters of natural gas', \"China's draft law revision on women's protection garners wide public attention\", 'It is those few Western countries that ignore and trample upon human rights: Chinese FM', \"So-called 'forced labor' in Xinjiang is a big lie made by anti-China forces: FM\", 'Ancient sculpture remnants reunited after 3,000 years', 'China sees fewer hazy days, more clear blue skies', 'China capable of meeting 3-percent CPI target: official', 'Shanghai Disney Resort reopens Disneytown and hotel', '240-meter-long glass bridge over Dashbashi Canyon opens'], 'link': ['http://www.ecns.cn/news/2022-06-18/detail-ihaziuqy8696991.shtml', 'http://www.ecns.cn/news/2022-06-18/detail-ihaziuqy8696977.shtml', 'http://www.ecns.cn/news/2022-06-18/detail-ihaziuqy8696975.shtml', 'http://www.ecns.cn/news/2022-06-18/detail-ihaziuqy8696973.shtml', 'http://www.ecns.cn/news/2022-06-18/detail-ihaziuqy8696961.shtml', 'http://www.ecns.cn/news/2022-06-18/detail-ihaziuqy8696959.shtml', 'http://www.ecns.cn/photo/2022-06-18/detail-ihaziuqy8696956.shtml', 'http://www.ecns.cn/news/politics/2022-06-18/detail-ihaziuqy8696826.shtml', 'http://www.ecns.cn/news/economy/2022-06-17/detail-ihaziuqy8696701.shtml', 'http://www.ecns.cn/news/economy/2022-06-17/detail-ihaziuqy8696698.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8696566.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8696458.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8696430.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8696428.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8696160.shtml', 'http://www.ecns.cn/photo/2022-06-17/detail-ihaziuqy8696105.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8696068.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8695998.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8695996.shtml', 'http://www.ecns.cn/photo/2022-06-17/detail-ihaziuqy8695787.shtml', 'http://www.ecns.cn/photo/2022-06-17/detail-ihaziuqy8695733.shtml', 'http://www.ecns.cn/video/2022-06-17/detail-ihaziuqy8695528.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8695462.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8695460.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8695310.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8695308.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8695272.shtml', 'http://www.ecns.cn/news/cns-wire/2022-06-17/detail-ihaziuqy8695235.shtml', 'http://www.ecns.cn/news/cns-wire/2022-06-17/detail-ihaziuqy8695122.shtml', 'http://www.ecns.cn/photo/2022-06-17/detail-ihaziuqy8695061.shtml', 'http://www.ecns.cn/photo/2022-06-17/detail-ihaziuqy8695046.shtml', 'http://www.ecns.cn/photo/2022-06-17/detail-ihaziuqy8695005.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694946.shtml', 'http://www.ecns.cn/business/2022-06-17/detail-ihaziuqy8694952.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694948.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694950.shtml', 'http://www.ecns.cn/business/2022-06-17/detail-ihaziuqy8694954.shtml', 'http://www.ecns.cn/cns-wire/2022-06-17/detail-ihaziuqy8694940.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694918.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694920.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694916.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-17/detail-ihaziuqy8694732.shtml', 'http://www.ecns.cn/business/2022-06-17/detail-ihaziuqy8694720.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694718.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694419.shtml', 'http://www.ecns.cn/photo/2022-06-17/detail-ihaziuqy8694416.shtml', 'http://www.ecns.cn/photo/2022-06-17/detail-ihaziuqy8694409.shtml', 'http://www.ecns.cn/photo/2022-06-17/detail-ihaziuqy8694398.shtml', 'http://www.ecns.cn/photo/2022-06-17/detail-ihaziuqy8694391.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694225.shtml', 'http://www.ecns.cn/business/2022-06-17/detail-ihaziuqy8694227.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694221.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694223.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8694197.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693910.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693907.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693915.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693913.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-17/detail-ihaziuqy8693900.shtml', 'http://www.ecns.cn/news/society/2022-06-17/detail-ihaziuqy8693881.shtml', 'http://www.ecns.cn/business/2022-06-17/detail-ihaziuqy8693846.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-17/detail-ihaziuqy8693823.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693813.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693809.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693815.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693807.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693817.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693811.shtml', 'http://www.ecns.cn/news/society/2022-06-17/detail-ihaziuqy8693786.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693744.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693754.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693746.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693752.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693750.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693748.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693720.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693722.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693718.shtml', 'http://www.ecns.cn/news/2022-06-17/detail-ihaziuqy8693716.shtml', 'http://www.ecns.cn/news/sci-tech/2022-06-17/detail-ihaziuqy8693497.shtml', 'http://www.ecns.cn/news/economy/2022-06-17/detail-ihaziuqy8693486.shtml', 'http://www.ecns.cn/news/economy/2022-06-16/detail-ihaziuqy8693463.shtml', 'http://www.ecns.cn/news/2022-06-16/detail-ihaziuqy8693357.shtml', 'http://www.ecns.cn/news/2022-06-16/detail-ihaziuqy8693361.shtml', 'http://www.ecns.cn/news/2022-06-16/detail-ihaziuqy8693359.shtml', 'http://www.ecns.cn/news/2022-06-16/detail-ihaziuqy8693132.shtml', 'http://www.ecns.cn/news/2022-06-16/detail-ihaziuqy8693134.shtml', 'http://www.ecns.cn/news/2022-06-16/detail-ihaziuqy8692994.shtml', 'http://www.ecns.cn/news/cns-wire/2022-06-16/detail-ihaziuqy8692991.shtml', 'http://www.ecns.cn/photo/2022-06-16/detail-ihaziuqy8692654.shtml', 'http://www.ecns.cn/news/2022-06-16/detail-ihaziuqy8692633.shtml', 'http://www.ecns.cn/business/2022-06-16/detail-ihaziuqy8692635.shtml', 'http://www.ecns.cn/news/2022-06-16/detail-ihaziuqy8692637.shtml', 'http://www.ecns.cn/photo/2022-06-16/detail-ihaziuqy8692618.shtml', 'http://www.ecns.cn/photo/2022-06-16/detail-ihaziuqy8692538.shtml', 'http://www.ecns.cn/news/2022-06-16/detail-ihaziuqy8692505.shtml', 'http://www.ecns.cn/photo/2022-06-16/detail-ihaziuqy8692440.shtml', 'http://www.ecns.cn/news/2022-06-16/detail-ihaziuqy8692434.shtml', 'http://www.ecns.cn/photo/2022-06-16/detail-ihaziuqy8692429.shtml', 'http://www.ecns.cn/photo/2022-06-16/detail-ihaziuqy8692266.shtml'], 'publish_date': ['18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '17-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022', '16-06-2022'], 'scraped_date': ['18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022', '18-06-2022'], 'text': ['Xi Jinping, general secretary of the Communist Party of China (CPC) Central Committee, urged coordinated improvement to guarantee that officials do not have the audacity, opportunity, or desire to engage in corruption.', \"One person was killed after a fire broke out at a petrochemical enterprise in China's Shanghai early on Saturday, the company said.\", \"China's installed capacity of renewable energy registered double-digit growth in the first five months of the year as the country strived to reach its carbon peaking and carbon neutrality goals.\", \"Chinese Foreign Ministry spokesperson Wang Wenbin on Friday refuted remarks by U.S. ambassador to China Nicholas Burns concerning China's dynamic zero-COVID approach.\", \"A fire broke out at a petrochemical enterprise in China's Shanghai early on Saturday, the company said on its Weibo account.\", 'The Chinese mainland on Friday reported 11 locally-transmitted confirmed COVID-19 cases, including five in Inner Mongolia Autonomous Region, four in Liaoning, and one each in Beijing and Shanghai, the National Health Commission said Saturday.', \"The Niuheliang site lies on a mountain ridge bordering Jianping county and Lingyuan city in Chaoyang, Northeast China's Liaoning province. The site dates back to between 5,500 and 5,000 years ago, a time period in the late Neolithic era and defined by Chinese archaeologists as the Hongshan Culture.\", 'Chinese President Xi Jinping on Friday attended and addressed the plenary session of the 25th St. Petersburg International Economic Forum in virtual format upon invitation.', 'China is steadily carrying out measures to keep jobs stable and has helped 1.96 million unemployed people return to the workforce in the first five months of the year, an official said Friday.', 'The Horn of Africa could face \"unprecedented food unavailability problems\" in 2023 due to ongoing drought and food supply shortages caused by the Russia-Ukraine conflict, David Beasley Executive Director of the UN World Food Program (WFP) has warned.', \"China's civil aviation authority said Friday that it is discussing with its counterparts in certain countries the possibility of gradually increasing international passenger flights in and out of the country.\", 'As the 12th Ministerial Conference of the World Trade Organization (WTO) closed at dawn on Friday, multilaterally negotiated outcomes on a series of key trade initiatives have been secured.', 'Xinjiang Uygur Autonomous Region in northwest China has been expanding investment in the energy sector in a bid to reinforce its status as a key energy base of the country.', \"China's healthcare security authorities have subsidized 69.33 million people to incorporate them into the country's basic medical insurance scheme during the first quarter of this year.\", \"There were 3,013 higher education institutions across China as of May 31, 2022, according to the country's latest list of colleges and universities unveiled Friday.\", 'China always stands on the side of peace and justice: Chinese FM', \"China has established the world's largest monitoring network for disease and health risk factors, the National Health Commission said Friday.\", \"BEIJING, June 17 (Xinhua) -- Beijing Fengtai Railway Station, the Chinese capital's oldest, is set to resume services on June 20 after a four-year reconstruction.Once reopened, the station will serve as the terminal for multiple arterial train lines in China, such as the Beijing-Guangzhou High-speed Railway and Beijing-Kowloon Railway, said the China State Railway Group Co., Ltd., the country's railway operator.\", 'British Home Secretary Priti Patel has approved the extradition of WikiLeaks founder Julian Assange to the United States, the Home Office confirmed on Friday.\\u3000\\u3000', \"A butterfly dragonfly rests on a lotus flower at a Park in Jiujiang city, east China's Jiangxi Province, June 17, 2022.\", \"Archaeologists have recently made some stunning discoveries at the famed Sanxingdui Ruins site in southwest China's Sichuan Province. A treasure trove of exquisite bronze, gold and jade wares, including at least 10 bronzewares unearthed for the first time in the history of human civilization, have been excavated at the site.\", \"The Hotan-Ruoqiang Railway in northwest China's Xinjiang Uyghur Autonomous Region commenced operation on Thursday.\", \"China's state-owned companies have become backbones in the country's pursuit of high-quality economic development over the past decade, an official said Friday.\", \"China has restored over half of its manageable desertified land over the past decade, making great strides in addressing desertification, according to the country's forestry authority.\", \"The World Trade Organization's (WTO) 12th Ministerial Conference (MC12) closed Friday at dawn at WTO headquarters in Geneva, Switzerland.\", \"China plans to build eight new national atmosphere background watch stations by the end of 2025 to strengthen greenhouse gas observation, according to the country's meteorological authority.\", 'By 2025, China plans to rehabilitate an additional 6.7 million hectares of desertified land, adding to the 18.8 million hectares the country has rehabilitated over the last decade.', 'Wu Jianfeng, a Chinese bird lover, has rescued tens of thousands of animals in the past ten years.', 'Ecology restoration and sand control have been a priority during the construction of the Hotan-Ruoqiang Railway, which started operation on Thursday, completing the world’s first desert railway loop line in northwest China’s Xinjiang Uyghur Autonomous Region.', \"A total of 666 drones light up the sky above Tengwang Pavilion in Nanchang, east China's Jiangxi Province, June 16, 2022.\", 'Photo taken on June 16, 2022 shows the exterior view of Beijing Fengtai Railway Station, which will be put into service soon, in Beijing, capital of China.', \"Spectacular fishscale clouds appear in the sky in Jinshanling section of the Great Wall in Luanping County, Chengde City, north China's Hebei Province, June 17, 2022. \", 'China has made significant improvements in containing infectious diseases and has ramped up its disease control capacity in the past decade, the National Health Commission said on Friday.', 'Inside the active testing area of the fourth phase of Nansha port in Guangzhou, Guangdong province, containers are handled automatically by intelligent guided vehicles and yard cranes, after regular testing of the operation started in April.', 'More than 200 adult cancer patients in China have been treated with the CAR-T cell therapy by Shanghai-based Fosun Kite Biotechnology Co Ltd following its market approval in the country in June last year.', 'Shanghai scientists have made new breakthroughs in the research of heat resistance genes and their mechanisms in rice, and their findings were published on the website of Science on Friday.', \"China's parcel delivery sector has maintained a steady growth in the first five months of this year, according to data released by the industry regulator the State Postal Bureau of China on Thursday.\", 'China launched its third aircraft carrier Fujian at the Shanghai Jiangnan Shipyard on Friday morning.', \"China's national observatory on Friday renewed a yellow alert for high temperatures, as heatwaves hit parts of the country.\", 'One person died and two others were injured after a shooting at a church in Vestavia Hills in the U.S. state of Alabama on Thursday evening.', \"A detachment of Russia's Pacific Fleet has entered the Philippine Sea, engaging in anti-aircraft training activities as well as those in search of submarines of a mock enemy, said the press service of the fleet Friday.\", \"China's commercial space telescope Yangwang-1 has completed an optical survey of the whole sky, according to its developer.\", \"JINAN, June 17 (Xinhua) -- About 15,000 Atlantic salmons in the net cage of a fully submersible deep-sea fish farming equipment were fished during the period from May 22 to June 9, which marks China's first successful harvest of self-bred deep-sea Atlantic salmons.\", 'China launched its third aircraft carrier, the Fujian, in Shanghai on Friday morning. The carrier, named after Fujian Province, was completely designed and built by the country.', 'Chinese President Xi Jinping will chair the 14th BRICS Summit in Beijing on June 23, a Chinese Foreign Ministry spokesperson announced here Friday.', \"Archaeologists later found that another bronzeware part, which was unearthed from the No. 2 pit in 1986, can perfectly match with the figure's lost body part.\", \"The last section of a 2,712-km rail loop line around China's largest desert, the Taklimakan, in Xinjiang was put into operation on Thursday.\", \"Aerial view shows the night scenery of cross-sea bridge shrouded in mist in Zhoushan, east China's Zhejiang Province, June 16, 2022. \", 'Giant panda Shuang Shuang celebrate 35th birthday in Chapultepec zoo, Mexico city, Mexico, June 15, 2022. ', 'The central government has prioritized the employment of young people, mainly new college graduates, with measures to create opportunities at all levels and tap into previously less noticed sectors and areas.', 'Shanghai witnessed deals worth 565.8 billion yuan ($84.4 billion) for 322 major industrial projects on Thursday.', 'The strongest heat wave of the year so far has started to sweep across northern parts of China with Henan province hit the hardest, according to Weather China, a website of the China Meteorological Administration.', \"Any individuals or groups involved in online or telecom fraud will soon face harsher punishments with the rollout of more stringent measures to protect people's interests.\", 'Multiple people were shot at a church in the southeastern U.S. state of Alabama on Thursday evening and a suspect has been in custody, police said.', \"There is insufficient evidence to support the view that the recent unexplained acute hepatitis in children is contagious, and the disease's link to COVID-19 or adenovirus remains unclear, the National Health Commission said on Thursday.\", 'China is confident of achieving reasonable growth in foreign trade this year that should contribute to macroeconomic resilience, the Ministry of Commerce said on Thursday.', \"Top officials in Tangshan, Hebei province, have vowed to improve the regular work system and mechanism involving public security, in all-around efforts to ensure citizens' sense of security and satisfaction.\", 'Chinese scientists have discovered that the majority of water found on the moon may have originated from its interior rather than from solar wind bombarding its surface with hydrogen ions that eventually formed water.', \"A three-stage engine of China's new generation manned carrier rocket will go to prototype sample development, according to the China Aerospace Science and Technology Corporation.\", \"China's National Health Commission on Tuesday issued diagnosis and treatment guidelines on the acute severe hepatitis of unknown etiology in children for early recognition as cases have been reported across multiple countries and regions.\", 'A court in Moscow ruled on Thursday to fine Google LLC 15 million rubles (about 260,000 U.S. dollars) for repeated refusal to localize the personal data of Russian users.', \"China's building of its space-based solar power station (SSPS) has achieved a new milestone, as a research team with Xidian University announced recently that the ground recipient verification system has passed examination, proving breakthroughs in key technology.\", 'China has ample space in monetary policy to achieve steady economic recovery in the second half of the year, bucking the latest acceleration in monetary tightening by the United States, experts said on Thursday.', \"Xi said that he was happy to learn about the great changes brought to local people by targeted poverty-relief efforts. Continued efforts should be made to step up rural vitalization and facilitate people's prosperity in border areas, he said.\", 'Under the theme of \"New Opportunities in a New World\", the 25th St. Petersburg International Economic Forum started on Wednesday in Russia\\'s second-largest city.', 'An exquisite and exotic-looking bronze statue recently excavated from the Sanxingdui site in Guanghan, Sichuan province, may offer tantalizing clues to decoding the mysterious religious rituals surrounding the famous 3,000-year-old archaeological site, scientific experts said.', 'Boasting a blend of experience and promising young talent, Team China is eyeing gold medals and breakthroughs at the FINA World Championships, which begin in Budapest on Saturday.', \"China will accelerate the implementation of a raft of pro-growth policies to keep its economic performance within a reasonable range in the second quarter, the country's top economic regulator said on Thursday.\", \"With the very first train roaring northeastward from Hotan, northwest China's Xinjiang Uygur Autonomous Region, the Hotan-Ruoqiang railway was formally put into operation Thursday. It also marks the inauguration of the world's first desert rail loop line.\", 'Japanese Prime Minister Fumio Kishida said Thursday that he is positive about a significant increase in spending on defense research and development after a group of lawmakers within his ruling party urged the government to secure 1 trillion yen within five years for that purpose.', 'China is accelerating the revision of the industry catalog that encourages foreign investment, as part of efforts to expand high-level opening-up, the Ministry of Commerce (MOC) said on Thursday.', \"China's top economic planner vowed to take multiple measures to ramp up the development of private firms and promote private investment, said an official on Thursday.\", \"China's key economic indicators showed improvements in May, adding to evidence that the world's second-largest economy is poised for a steady recovery from the COVID-19 impact and policies to coordinate epidemic control and economic growth take effect.\", \"URUMQI, June 16 (Xinhua) -- The government of northwest China's Xinjiang Uygur Autonomous Region has granted over 500 million yuan (about 74.5 million U.S.According to the regional department of human resources and social security, by the end of May, a total of 222,800 employees in Xinjiang had brought employment subsidies of 509 million yuan to their employers.\", \"An in situ detection by China's Chang'e-5 lunar probe and the subsequent analysis of its returned samples revealed at least two sources of water on the Moon, one brought by solar wind and the other from indigenous sources.\", \"Ukrainian President Volodymyr Zelensky met with the leaders of France, Germany, Italy and Romania in Kiev Thursday to discuss his country's prospects to join the European Union and the Russia-Ukraine conflict.\", 'The Russia-China Business Dialogue was held Thursday in the Russian port city of St. Petersburg in the framework of the 25th St. Petersburg International Economic Forum (SPIEF).', 'A Chinese diplomat on Thursday stressed that the United States should be responsible for the current situation in Afghanistan and should take meaningful steps to compensate the Afghan people for their sufferings.', \"Eight people were injured and six others remain missing after an explosion ripped through a factory Thursday night in northwest China's Gansu Province, local authorities said.\", \"China's commercial space telescope Yangwang-1 has completed an optical survey of the whole sky, according to its developer.\", \"As Pakistan's agriculture sector remained crucial for the country's economic growth, experts and scientists said that enhanced cooperation with China under the China-Pakistan Economic Corridor (CPEC) can help revitalize the country's agriculture sector and combat the challenge of food insecurity.\", \"China's national lawmakers will deliberate draft laws regarding issues such as anti-telecom and cyber fraud, civil compulsory enforcement, and anti-monopoly during a forthcoming session by the country's top legislature.\", 'A China-developed mRNA COVID-19 vaccine as a third booster dose has been shown in human trials to be safe and able to induce an immune response against the Omicron variant.', 'Chinese brands have built up strength despite the challenges brought about by COVID-19, according to a report released by the international market research firm Kantar on Wednesday.', 'China plans to recruit 67,000 teachers for compulsory education in rural areas in 2022, the Ministry of Education said Thursday.', 'China is confident to keep its foreign trade running within a reasonable range for the whole year, making more contributions to the economic fundamentals, the Ministry of Commerce (MOC) said Thursday.', 'China welcomes foreign companies to ramp up spending on research and development (R) and set up R centers in the country, Shu Jueting, spokesperson for the Ministry of Commerce, said Thursday.', 'The Regional Comprehensive Economic Partnership (RCEP) agreement offers broader room for China and the Republic of Korea (ROK) to deepen economic and trade cooperation, a Chinese official said Thursday.\\u3000', 'The Xinjiang Uygur Autonomous Region has actively practiced the international concept of decent work, followed the relevant conventions and initiatives of the International Labour Organization, and strictly implemented national laws, policies, and regulations.', '', \"The commerce department of northwest China's Xinjiang Uygur Autonomous Region said investment from outside the region has seen robust growth in the January-May period.\", 'The China-Central Asia Gas Pipeline has delivered more than 400 billion cubic meters of natural gas to China over a period of more than 12 years, according to PipeChina West Pipeline Company.', 'A draft revision to the Law on the Protection of Rights and Interests of Women, which stipulates stringent measures to tackle women trafficking, has garnered wide public attention in China.', '', '', 'A newly unearthed bronze sculpture at the famed Sanxingdui Ruins site was successfully matched with another bronzeware part after being set apart about 3,000 years ago, the archaeological team confirmed Thursday.', '', 'China will see its consumer price index (CPI) continue to run within a reasonable range and is capable of attaining the 3-percent annual target amid the global inflation surge, an official said Thursday.', \"Photo taken on June 16, 2022 shows the World of Disney Store in east China's Shanghai. Shanghai Disney Resort reopened Disneytown and Shanghai Disneyland hotel on Thursday, the 6th anniversary of the resort.\", 'People attend the opening ceremony 240-meter long glass bridge over the Tsalka canyon, with a diamond-shaped cafe in the middle outside the city of Tsalka some 100 km from Tbilisi, Georgia, June 14, 2022. ']}\n",
      "energy voice\n",
      "Euronews\n",
      "The free press journal\n",
      "DW\n",
      "Star\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.reuters.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reuters.com/article/idUSL4N2Y40ME', 'https://www.reuters.com/article/idUSKBN2NU04R', 'https://www.reuters.com/article/idUSKBN2NQ0XE', 'https://www.reuters.com/article/idUSL1N2XW1T5', 'https://www.reuters.com/article/idUSL1N2XW0ZQ', 'https://www.reuters.com/article/idUSL4N2XW2A1', 'https://www.reuters.com/article/idUSKBN2NQ0V9', 'https://www.reuters.com/article/idUSL1N2XW0WM', 'https://www.reuters.com/article/idUSL1N2XW0RE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:39:23,616 - 2210765 - root - INFO - Reuters function ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame is blank\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.reuters.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.reuters.com/article/idUSKBN2NY0RT', 'https://www.reuters.com/article/idUSL8N2Y42K2', 'https://www.reuters.com/article/idUSKBN2NY0N2', 'https://www.reuters.com/article/idUSL1N2Y40J1', 'https://www.reuters.com/article/idUSKBN2NY09X', 'https://www.reuters.com/article/idUSL8N2Y35TG', 'https://www.reuters.com/article/idUSKBN2NY06M', 'https://www.reuters.com/article/idUSL4N2Y40ME', 'https://www.reuters.com/article/idUSKBN2NX1WV']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 13:39:40,836 - 2210765 - root - INFO - Reuters function ended\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame is blank\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ipo.einnews.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/ipykernel_launcher.py:902: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages/ipykernel_launcher.py:908: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "2022-06-18 13:40:42,860 - 2210765 - root - INFO - writing output artifact /home/prachi_multilex2/todays_report.csv to /home/prachi_multilex2\n",
      "2022-06-18 13:40:42,882 - 2210765 - root - INFO - completed writing output artifact /home/prachi_multilex2/todays_report.csv to /home/prachi_multilex2\n",
      "2022-06-18 13:40:42,886 - 2210765 - root - INFO - last line of scraper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import pandas as pd \n",
    "from requests_html import HTMLSession\n",
    "import os\n",
    "from pathlib import Path\n",
    "from googletrans import Translator\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "#the scrapper file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import bs4\n",
    "from datetime import timedelta \n",
    "from newspaper import Article\n",
    "import sys\n",
    "import importlib.util\n",
    "import warnings\n",
    "import logging\n",
    "import dateparser\n",
    "import pytz\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(process)d - %(name)s - %(levelname)s - %(message)s\")\n",
    "logging.info(\"first line of multilex_scraper_xform\")\n",
    "def infin_transform_all_objects(input_dir, output_dir, **kwargs):\n",
    "    logging.info(\"input_dir=\" + input_dir + \", output_dir=\" + output_dir)\n",
    "    # onlyfiles = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n",
    "    # for f in onlyfiles:\n",
    "    #  logging.info(\"file in directory %s = %s. size = %d\", input_dir, os.path.join(input_dir, f), Path(os.path.join(input_dir, f)).stat().st_size)\n",
    "    for path, subdirs, files in os.walk(input_dir):\n",
    "          for name in files:\n",
    "            logging.info(\"file in directory %s = %s. size = %d\", path, os.path.join(path, name), Path(os.path.join(path, name)).stat().st_size)\n",
    "\n",
    "    \n",
    "    #s1 = dynamic_module_import(os.path.join(input_dir, \"s1.py\"), \"s1\")\n",
    "\n",
    "    multilex_scraper(input_dir, output_dir)\n",
    "\n",
    "def dynamic_module_import(file_path, module_name):\n",
    "  # import s1\n",
    "    \n",
    "  \n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "\n",
    "    logging.info(\"dynamically loaded module %s contents = %s\", file_path, dir(module))\n",
    "\n",
    "    return module\n",
    "\n",
    "\n",
    "def multilex_scraper(input_dir, output_dir):\n",
    "   \n",
    "    # import matplotlib.pyplot as plt\n",
    "    cur_dat = datetime.now()\n",
    "    cur_date = str(date.today())\n",
    "    # from googletrans import Translator\n",
    "    not_working_functions = []\n",
    "    log_format = (\n",
    "        '[%(asctime)s] %(levelname)-8s %(name)-12s %(message)s')\n",
    "    def emptydataframe(name,df):\n",
    "        if df.empty:\n",
    "            not_working_functions.append(name+\" : err : Empty datframe\")\n",
    "    months = [\"Jan\" , \"Feb\" , \"Mar\" , \"Apr\" , \"May\" , \"Jun\" , \"Jul\" , \"Aug\" , \"Sep\" , \"Oct\" , \"Nov\" , \"Dec\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format=log_format,\n",
    "        filename=os.path.join(output_dir,'debug.log'),\n",
    "    )\n",
    "    def translate(text):\n",
    "        translator = Translator()\n",
    "        translation = translator.translate(text, dest='en')\n",
    "        return translation.text\n",
    "    def translate_dataframe(df):\n",
    "        try:\n",
    "            for i,row in df.iterrows():\n",
    "                # row[\"publish_date\"]= translate(row[\"publish_date\"])\n",
    "                row[\"title\"] = translate(row[\"title\"])\n",
    "                row[\"text\"] = translate(row[\"text\"])\n",
    "\n",
    "                # time.sleep(0.2)\n",
    "            return df\n",
    "        except:\n",
    "            for i,row in df.iterrows():\n",
    "                # row[\"publish_date\"]= translate(row[\"publish_date\"])\n",
    "                row[\"title\"] = translate(row[\"title\"])\n",
    "                # row[\"text\"] = translate(row[\"text\"])\n",
    "\n",
    "                # time.sleep(0.2)\n",
    "            return df\n",
    "\n",
    "    def link_correction(data):\n",
    "        link = data[\"link\"].to_list()\n",
    "        new = []\n",
    "        for i in link :\n",
    "            try :\n",
    "                new.append(i.split(\"&ct\")[0])\n",
    "            except :\n",
    "                print(\"Link is messed up \")\n",
    "\n",
    "        new_links = pd.DataFrame(new)\n",
    "        data[\"link\"] = new_links\n",
    "        return data\n",
    "\n",
    "    def correct_link(link):\n",
    "        link = str(link)\n",
    "        if(link.find(\"&ct\")!=-1):\n",
    "            link = link.split(\"&ct\")[0]\n",
    "        return link\n",
    "    def get_date_mname_d_y(date):\n",
    "        #Apr 21, 2022 \n",
    "        #21 Apr 2022\n",
    "        month = re.findall(r'''(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)''',date)\n",
    "        m = str(months.index(month[0][0].strip()) % 12 + 1)\n",
    "        y = str(re.findall(r'\\d{4}',date)[0])\n",
    "        day = str(re.findall(r'\\d{1,2}',date)[0])\n",
    "        return \"-\".join([day,m,y])\n",
    "    def get_date_min_read(date):\n",
    "    #2min read . Updated: 21 Apr 2022, time\n",
    "        return get_date_mname_d_y(date.split(\":\")[1].split(\",\")[0].strip())\n",
    "    def correct_publish_date(i):\n",
    "        try:\n",
    "            i = str(i)\n",
    "            i = i.strip()\n",
    "            if len(re.findall(\"\\d{1,2}/\\d{1,2}/\\d{4}\",i)):\n",
    "                hi = re.findall(\"\\d{1,2}/\\d{1,2}/\\d{4}\",i)\n",
    "                l1 = hi[0].split(\"/\")\n",
    "                temp = l1[0]\n",
    "                l1[0] = l1[1]\n",
    "                l1[1] = temp\n",
    "                i = \"-\".join(l1)\n",
    "                return i\n",
    "            elif(re.findall(\"\\d{1,2}/\\w{3}/\\d{4}\",i)):\n",
    "                i = get_date_mname_d_y(re.findall(\"\\d{1,2}/\\w{3}/\\d{4}\",i)[0])\n",
    "                return i\n",
    "            elif(len(re.findall(\"\\d{1,2}\\s\\w{3}\\s\\d{4}\",i))):\n",
    "\n",
    "                i = get_date_mname_d_y(re.findall(\"\\d{1,2}\\s\\w{3}\\s\\d{4}\",i)[0])\n",
    "                # print(i)\n",
    "                return i\n",
    "            elif(re.findall(r'min read . Updated:',i)):\n",
    "                i = get_date_min_read(i)   \n",
    "                return i   \n",
    "            elif(len(i.split(\".\")) == 3):\n",
    "                if(len(i.split(\".\")[0]) == 4):\n",
    "                    i = \"-\".join(i.split(\".\")[::-1])\n",
    "                    return i\n",
    "                else:\n",
    "                    i = i.replace(\".\",\"-\")\n",
    "                    return i  \n",
    "            elif len(re.findall(r'\\d{1,2}.\\d{1,2}.\\d{4}',i)) and i.find(\":\"):\n",
    "                if len(i.split(\" \"))>1:\n",
    "                    i = i.split(\" \")[2].replace(\".\",\"-\")\n",
    "                    return i\n",
    "            elif(re.findall(r'\\d{1,2}-\\d{1,2}-\\d{4}',i)):\n",
    "                return i\n",
    "            \n",
    "            elif(i.count(\":\") >= 2):\n",
    "                if len(re.findall(r'T',i)):\n",
    "                    i = \"-\".join(i.split(\"T\")[0].split(\"-\")[::-1])\n",
    "                    return i\n",
    "                if len(re.findall(r'Newswire',i)):\n",
    "                    i = \"-\".join(i.strip().split(\" \")[-3].split(\"-\")[::-1])\n",
    "                    return i\n",
    "                i = i.split(\" \")[0].strip()\n",
    "                if len(re.findall(r'[a-zA-Z]+',i)):\n",
    "                    i = get_date_mname_d_y(i)\n",
    "                else:\n",
    "                    i = \"-\".join(i.split(\"-\")[::-1])\n",
    "                    return i\n",
    "            elif len(i.split(\"-\")[0]) == 4:\n",
    "                i = \"-\".join(i.split(\"-\")[::-1])\n",
    "                return i\n",
    "            \n",
    "            i = get_date_mname_d_y(i)\n",
    "            return i\n",
    "        except:\n",
    "            i = i.strip()\n",
    "            \n",
    "            i = translate(i)\n",
    "            month = re.findall(r'''(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)''',i)\n",
    "            if not len(month):\n",
    "                i = \"Date\"\n",
    "                return i\n",
    "            m = str(months.index(month[0][0].strip()) % 12 + 1)\n",
    "            y = str(re.findall(r'\\d{4}',i)[0])\n",
    "            day = str(re.findall(r'\\d{1,2}',i)[0])\n",
    "            i = \"-\".join([day,m,y])\n",
    "            return i\n",
    "    def correct_navigable_string(df1):\n",
    "        err = []\n",
    "        for i , row in df1.iterrows():\n",
    "            try:\n",
    "                if(row[\"publish_date\"] == None):\n",
    "                    row[\"publish_date\"] = \"Date\"\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                soup = BeautifulSoup('''\n",
    "                <html>\n",
    "                    ''' + str(row[\"publish_date\"]) + '''\n",
    "                </html>\n",
    "                ''', \"lxml\")\n",
    "\n",
    "            # Get the whole h2 tag\n",
    "                row[\"publish_date\"]=str(soup.p.string)\n",
    "                row[\"publish_date\"] = str(row[\"publish_date\"]).strip()\n",
    "                row[\"publish_date\"] = correct_publish_date(row[\"publish_date\"])\n",
    "                row[\"link\"] = correct_link(row[\"link\"])\n",
    "            except:\n",
    "                # print(row)\n",
    "                # print(\"\\n\")\n",
    "                err.append(i)\n",
    "        print(err)\n",
    "        df_final=df1\n",
    "        df2 = df_final[df_final[\"publish_date\"] == \"Date\"]\n",
    "        df_final['publish_date']=pd.to_datetime(df_final['publish_date'],format=\"%d-%m-%Y\",errors='coerce',utc=True).dt.strftime(\"%d/%b/%Y\" \" \" \"%H:%M:%S\")\n",
    "        one_year_from_now = datetime.now()\n",
    "        date_formated = one_year_from_now.strftime(\"%d/%b/%Y\" \" \" \"%H:%M:%S\")\n",
    "        df_final['scraped_date'] = date_formated\n",
    "\n",
    "        public_date = pd.to_datetime(df_final['publish_date'],errors='coerce',utc=True).dt.strftime('%d-%m-%Y')\n",
    "        scrap_date= pd.to_datetime(df_final['scraped_date'],errors='coerce',utc=True).dt.strftime('%d-%m-%Y')\n",
    "\n",
    "        ## morning \n",
    "        yesterday = (date.today() - timedelta(days=1)).strftime('%d-%m-%Y')\n",
    "        daybefore = (date.today() - timedelta(days=2)).strftime('%d-%m-%Y')\n",
    "        final_1 = df_final.loc[public_date == yesterday]\n",
    "        final_2 = df_final.loc[public_date == scrap_date]\n",
    "        final_3 = df_final.loc[public_date == daybefore]\n",
    "        fn = [final_1,final_2,final_3,df2]\n",
    "        final = pd.concat(fn)\n",
    "        return final\n",
    "    \n",
    "    def log_errors(err_logs):\n",
    "        for i in err_logs:\n",
    "            print(i)\n",
    "    def FilterFunction(final):\n",
    "        # import matplotlib.pyplot as plt\n",
    "        try:\n",
    "            keyword = [ 'IPO','IPO','IPO ','SPACs','ipo','pre-IPO','pre-ipo','PRE-IPO','pre-IPO','going public','spac','shares','public']\n",
    "            \n",
    "            # keyword = [ \"Follow-on Offering\", 'FPO', 'Seasoned Equity Offering', 'SEO',' Bookrunner', 'Underwriter', 'Rumour', 'Primary Exchange', 'Currency','raised','IPO','IPO','IPO ','SPACs','ipo','pre-IPO','pre-ipo','PRE-IPO','pre-IPO','going public','public','closes','listing','planning','closing','excellent','Public','Initial','Offering','initial','Announces','Pricing','pricing','announces','launches','Launches','SPAC','spac']\n",
    "            keywords1=['IPO,','IPO,','IPO, ','SPACs,','ipo,','pre-IPO,','pre-ipo,','PRE-IPO,','pre-IPO,','going public,','public,','closes,','listing,','planning,','closing,','excellent,','Public,','Initial,','Offering,','initial,','Announces,','Pricing,','pricing,','announces,','launches,','Launches,','SPAC,','spac,']\n",
    "            keywords=keyword\n",
    "            title=[]\n",
    "            link=[]\n",
    "            published_date=[]\n",
    "            scraped_date=[]\n",
    "            text=[]\n",
    "            flag=False\n",
    "              #Here is the dataframe to be passed\n",
    "                \n",
    "            for i in range(0,final.shape[0]):\n",
    "                article=final[\"title\"][i] + \" \" + final['text'][i]\n",
    "                article=article.split(\" \")\n",
    "                for let in article:\n",
    "                    if let in keywords :\n",
    "                        flag=True\n",
    "                        break\n",
    "                if flag==True:\n",
    "                    title.append(final['title'][i])\n",
    "                    link.append(final['link'][i])\n",
    "                    published_date.append(final['publish_date'][i])\n",
    "                    scraped_date.append(final['scraped_date'][i])\n",
    "                    text.append(final['text'][i])\n",
    "                    flag=False\n",
    "        except:\n",
    "            print('DataFrame is blank')\n",
    "        final = pd.DataFrame(list(zip(title,link,published_date,scraped_date,text)), \n",
    "                   columns =['title','link','publish_date','scraped_date','text'])\n",
    "        final = final[~final['title'].isin([\"private placement\", \"reverse merger\", \"blank check merger\"])]\n",
    "        final = final[~final['text'].isin([\"private placement\", \"reverse merger\", \"blank check merger\"])]\n",
    "        \n",
    "        return final \n",
    "    \n",
    "    def MoneyControl():\n",
    "        try:\n",
    "            print(\"Moneycontrol\")\n",
    "            err_logs = []\n",
    "            baseSearchUrl = \"https://www.moneycontrol.com/rss/iponews.xml\"\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            err_index = []\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            queryUrl = baseSearchUrl\n",
    "            try:\n",
    "                pageSource = requests.get(baseSearchUrl)\n",
    "            except:\n",
    "                err = \"moneycontrol: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                exit()\n",
    "            # with open(\"response.xml\", \"wb\") as f:\n",
    "            #     f.write(pageSource.content)\n",
    "            # break\n",
    "            parsedSource = BeautifulSoup(pageSource.content, \"xml\")\n",
    "            for eachItem in parsedSource.find_all(\"item\"):\n",
    "                currentArticleTitle = eachItem.find(\"title\").text\n",
    "                currentArticleLink = eachItem.find(\"link\").text\n",
    "                currentArticleDate = datetime.strptime(str(eachItem.find(\"pubDate\").text).split(\"+\", maxsplit=2)[0].strip(),\n",
    "                                                    \"%a, %d %b %Y %H:%M:%S\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "                titles.append(currentArticleTitle)\n",
    "                links.append(currentArticleLink)\n",
    "                ArticleDates.append(currentArticleDate)\n",
    "                ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "            scrapedData[\"title\"] = titles\n",
    "            scrapedData[\"link\"] = links\n",
    "            scrapedData[\"publish_date\"] = ArticleDates\n",
    "            scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "\n",
    "            # Article's date and description scraping\n",
    "            ArticleBody = []\n",
    "            for link in links:\n",
    "                articleText = \"\"\n",
    "                try:\n",
    "                    headers = {\n",
    "                        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                    pageSource = requests.get(link, headers=headers)\n",
    "                    parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                except:\n",
    "                    err = \"moneycontrol: err: Failed to access article link : \" + link\n",
    "                    ArticleDates.append(\"Error\")\n",
    "                    err_index.append(links.append(link))\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                # with open(\"response.html\", \"wb\") as f:\n",
    "                #     f.write(pageSource.content)\n",
    "                # break\n",
    "                requiredDiv = parsedSource.find(\"div\", class_=\"content_wrapper arti-flow\")\n",
    "                if not requiredDiv:\n",
    "                    articleText = titles[links.index(link)]\n",
    "                    ArticleBody.append(articleText)\n",
    "                    continue\n",
    "                else:\n",
    "                    for item in requiredDiv.find_all(\"p\"):\n",
    "                        try:\n",
    "                            if not (re.search(\"/Click Here/gm\", item.text) or re.search(\"/Also read/gm\", item.text) or\n",
    "                                    re.search(\"/Disclaimer/gm\", item.text)):\n",
    "                                articleText += (\" \" + item.text)\n",
    "                            else:\n",
    "                                continue\n",
    "                        except:\n",
    "                            continue\n",
    "                    ArticleBody.append(articleText)\n",
    "                scrapedData[\"text\"] = ArticleBody\n",
    "\n",
    "            # DataFrame creation\n",
    "            moneycontrolDF = pd.DataFrame(scrapedData)\n",
    "            moneycontrolDF = moneycontrolDF.drop_duplicates(subset=[\"link\"])\n",
    "            if moneycontrolDF.empty:\n",
    "                err = \"moneycontrol: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(moneycontrolDF)\n",
    "            emptydataframe(\"Moneycontrol\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(moneycontrolDF)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Moneycontrol not working\")\n",
    "            not_working_functions.append(\"Moneycontrol\")\n",
    "\n",
    "\n",
    "\n",
    "    def Cnbc_Seeking():\n",
    "        try:\n",
    "            print('2')\n",
    "            s_dates=[]  \n",
    "            title = []\n",
    "            text = []\n",
    "            dates = []\n",
    "            links = []\n",
    "            try:\n",
    "                urls=\"https://www.cnbc.com/id/10000666/device/rss\"\n",
    "                logging.info(\"cnbc.com: invoking requests.url()=\" + urls)\n",
    "\n",
    "            #ipo links for seeking alpha and cnbc.com\n",
    "                import requests\n",
    "                #This is CNBC scraper\n",
    "                page=requests.get(urls,verify = False)\n",
    "                logging.info(\"cnbc.com: completed requests.url()=\" + urls)\n",
    "                soup=BeautifulSoup(page.content,features='lxml')\n",
    "                soup.find_all()\n",
    "                # print(soup)\n",
    "                import datetime\n",
    "                x=datetime.datetime.now()\n",
    "                y=x.date()\n",
    "                x=x.date()\n",
    "                x=str(x)\n",
    "                x=x.split('-')\n",
    "                curr_date=x[2]\n",
    "                curr_date=int(curr_date)\n",
    "                curr_date=curr_date-1\n",
    "                curr_date=str(curr_date)\n",
    "                # print(curr_date)\n",
    "\n",
    "\n",
    "                for i in range(1,len(soup.find_all('title'))):\n",
    "                    dates.append(soup.find_all('pubdate')[i].text)\n",
    "                    text.append(soup.find_all('description')[i].text)\n",
    "                    s_dates.append(y)\n",
    "                    title.append(soup.find_all('title')[i].text)\n",
    "                    #company country link\n",
    "                    link1=soup.find('item').text\n",
    "                    link1=link1.split(\" \")\n",
    "                    for wordse in link1:\n",
    "                        wordse=wordse.split(':')\n",
    "                    if wordse[0]=='https':\n",
    "                        link1=wordse[0]+\":\"+wordse[1]\n",
    "                        links.append(link1)\n",
    "\n",
    "\n",
    "            except:\n",
    "                print('CNDC_seeking is not working')\n",
    "                not_working_functions.append('Cnbc')\n",
    "\n",
    "            cnbc = pd.DataFrame(list(zip(title,dates,s_dates,links,text)), \n",
    "                            columns =['title','publish_date','scraped_date','link','text'],index=None)\n",
    "            df = FilterFunction(cnbc)\n",
    "            emptydataframe(\"Cnbc\",df)\n",
    "            logging.info(\"CNBC_seeking function ended\")\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "\n",
    "    def korea():\n",
    "        try :\n",
    "            print(\"Korea\")\n",
    "            err_logs = []\n",
    "            url = \"http://www.koreaherald.com/search/index.php?kr=0&q=IPO\"\n",
    "            domain_url = \"http://www.koreaherald.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Korea : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            for a in soup.find_all('ul', {'class':'main_sec_li'}):\n",
    "                            #links.append(a[\"href\"])\n",
    "                for l in a.find_all('a',href=True):\n",
    "                                #print(l['href'])\n",
    "                    links.append(domain_url + l[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Korea : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    \n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" : \"view_tit_byline_r\"}).text)\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"view_tit\"}).text)       \n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" :\"view_con article\"}).text)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Korea : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Korea\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Korea not working\")\n",
    "            not_working_functions.append('Korea')\n",
    "    def proactive(keyword):\n",
    "        try:\n",
    "            err_logs = []\n",
    "            url = f\"https://www.proactiveinvestors.com.au/search/advancedSearch/news?url=&keyword={keyword}\"\n",
    "            domain_url = \"https://www.proactiveinvestors.com.au/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            div_class = \"advanced-search-block\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_class = \"h2\"\n",
    "            date_p_itemprop = \"datePublished\"\n",
    "            para_div_itemprop = \"articleBody\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link[1:]\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"proactiveinvestors\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link, headers=headers)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                try:\n",
    "                    date_ele = l_soup.find(\"p\", {\"itemprop\": date_p_itemprop})\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                try:\n",
    "                    para_ele = (l_soup.findAll(\"div\", {\"itemprop\": para_div_itemprop}))[-1]\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Proactive investors\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"Proactive Inverstors\")\n",
    "            print(\"Proactive investors not working\")\n",
    "\n",
    "    def Reuters(keyword):\n",
    "        print('7')\n",
    "        try:\n",
    "            title = []\n",
    "            text = []\n",
    "            s_dates = []\n",
    "            links=[]\n",
    "            pub_date=[]\n",
    "            try:\n",
    "\n",
    "                url = f'https://www.reuters.com/search/news?blob={keyword}&sortBy=date&dateRange=all'\n",
    "                url1 = 'https://www.reuters.com'\n",
    "                page=requests.get(url,verify = False)\n",
    "\n",
    "                soup=BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "                y = soup.findAll(\"h5\", {\"class\" : \"search-result-timestamp\"})\n",
    "\n",
    "                for x in y:\n",
    "                    import re\n",
    "                    TAG_RE = re.compile(r'<[^>]+>')\n",
    "                    pubdate = TAG_RE.sub('',str(x))\n",
    "                    pub_date.append(str(pubdate))\n",
    "                for i in range(1,len(soup.find_all('h3'))):\n",
    "                    pdd=soup.find_all('h3')[i]\n",
    "                    for a in pdd.find_all('a',href=True):\n",
    "                        links.append(url1 + a[\"href\"])\n",
    "\n",
    "                print(links)\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'lxml')\n",
    "                    t = sp\n",
    "                    x=sp.find(\"h1\", { \"class\" : \"text__text__1FZLe text__dark-grey__3Ml43 text__medium__1kbOh text__heading_2__1K_hh heading__base__2T28j heading__heading_2__3Fcw5\" })\n",
    "                    if x is not None:\n",
    "                        n = x.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    z=sp.find(\"div\", { \"class\":\"article-body__content__17Yit paywall-article\" })\n",
    "                    if z is not None:\n",
    "                        k = z.text\n",
    "                    else:\n",
    "                        k = None\n",
    "                    #print(z)\n",
    "                    text.append(k)\n",
    "                    title.append(n)\n",
    "                    s_dates.append(cur_date)\n",
    "\n",
    "            except:\n",
    "                print('Reuters is not working')\n",
    "                not_working_functions.append('Reuters')\n",
    "            percentile_list ={'publish_date': pub_date,'scraped_date': s_dates,'title': title,'link': links,'text':text}\n",
    "            reuters = pd.DataFrame.from_dict(percentile_list, orient='index')\n",
    "            df= reuters.transpose()\n",
    "            df.dropna(inplace=True)\n",
    "            df = FilterFunction(reuters)\n",
    "            emptydataframe(\"reuters\",df)\n",
    "            logging.info(\"Reuters function ended\")\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def TradingChart():\n",
    "        try :\n",
    "            print(\"FTC\")\n",
    "            err_logs = []\n",
    "            url = \"https://futures.tradingcharts.com/search.php?keywords=IPO&futures=1\"\n",
    "            domain_url = \"https://futures.tradingcharts.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"FTC : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"clUnSeResItemTitle\"})\n",
    "            for div in all_divs:\n",
    "                links.append(\"https:\"+div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"FTC : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"h2\" , {\"class\" : \"fe_heading2\"}).text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                div = soup.find(\"div\" , {\"class\" : \"news_story m-cellblock m-padding\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"i\"):\n",
    "                    t += i.text + \" \"\n",
    "                pub_date.append(t)\n",
    "               \n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"news_story m-cellblock m-padding\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"FTC : err: Empty datframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"FTC\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"FTC not working\")\n",
    "            not_working_functions.append('FTC')\n",
    "    def einnews():\n",
    "        try:\n",
    "            print(\"IPO EinNews\")\n",
    "            err_logs = []\n",
    "\n",
    "            baseSearchUrl = \"https://ipo.einnews.com/\"\n",
    "            domainUrl = \"https://ipo.einnews.com\"\n",
    "            keywords = ['IPO', 'pre-IPO', 'initial public offering']\n",
    "\n",
    "            # use this for faster testing\n",
    "            tkeywords = [\"IPO\"]\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            err_index = []\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            ArticleBody = []\n",
    "            for keyword in tkeywords:\n",
    "                queryUrl = baseSearchUrl\n",
    "                try:\n",
    "                    session = HTMLSession()\n",
    "                    resp = session.post(queryUrl)\n",
    "                    resp.html.render()\n",
    "                    pageSource = resp.html.html\n",
    "                    parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "                except:\n",
    "                    err = \"einnews: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                # with open(\"response.html\", \"w\") as f:\n",
    "                #     f.write(pageSource)\n",
    "                # break\n",
    "                for item in parsedSource.find(\"ul\", class_=\"pr-feed\").find_all(\"li\"):\n",
    "                    requiredTag = item.find(\"h3\")\n",
    "                    currentArticleTitle = str(requiredTag.find(\"a\").text).strip()\n",
    "                    # print(currentArticleTitle)\n",
    "                    currentArticleLink = requiredTag.find(\"a\")[\"href\"]\n",
    "                    # print(currentArticleLink)\n",
    "                    if currentArticleLink[0] == \"/\":\n",
    "                        links.append(domainUrl + currentArticleLink)\n",
    "                    else:\n",
    "                        links.append(currentArticleLink)\n",
    "                    titles.append(currentArticleTitle)\n",
    "                    currentArticleDateText = item.find(\"span\", class_=\"date\").text\n",
    "                    if re.search(\"^\\d.*\", currentArticleDateText):\n",
    "                        try:\n",
    "                            currentArticleDate = datetime.today().strftime(\"%d-%m-%Y\")\n",
    "                        except:\n",
    "                            err = \"einnews: err: Failed to retrieve date from article : \" + currentArticleLink\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(currentArticleLink))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        ArticleDates.append(currentArticleDate)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    else:\n",
    "                        try:\n",
    "                            currentArticleDate = datetime.strptime(currentArticleDateText,\n",
    "                                                                \"%b %d, %Y\").strftime(\"%d-%m-%Y\")\n",
    "                        except:\n",
    "                            err = \"einnews: err: Failed to retrieve date from article : \" + currentArticleLink\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(currentArticleLink))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        ArticleDates.append(currentArticleDate)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    articleText = \"\"\n",
    "                    for pitem in item.find_all(\"p\"):\n",
    "                        articleText += pitem.text\n",
    "                    ArticleBody.append(articleText.strip(\"\\n\"))\n",
    "\n",
    "                scrapedData[\"title\"] = titles\n",
    "                scrapedData[\"link\"] = links\n",
    "                scrapedData[\"publish_date\"] = ArticleDates\n",
    "                scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                scrapedData[\"text\"] = ArticleBody\n",
    "                # print(titles)\n",
    "                # print(links)\n",
    "                # print(ArticleDates)\n",
    "                # print(ArticleBody)\n",
    "\n",
    "            # DataFrame creation\n",
    "            einnewsDF = pd.DataFrame(scrapedData)\n",
    "            if einnewsDF.empty:\n",
    "                err = \"einnews: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(einnewsDF)\n",
    "            emptydataframe(\"Einnews\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"IPO Einnews\")\n",
    "            print(\"EINnews not working\")\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def rss():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            description=[]\n",
    "            scraped_date=[]\n",
    "            cleaned_description=[]\n",
    "            try:\n",
    "                print('9')\n",
    "                URL=\"https://ipo.einnews.com/rss/cpCdwuL2w4azHHGe\"\n",
    "                # logging.info(\"go_rss.com: invoking requests.url()=\" + URL)\n",
    "                page=requests.get(URL,verify = False)\n",
    "                # logging.info(\"go_rss.com: completed requests.url()=\" + URL)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "\n",
    "                import datetime  \n",
    "\n",
    "                # using now() to get current time  \n",
    "                current_time = datetime.datetime.now() \n",
    "                import html\n",
    "\n",
    "                for i in range(1,len(soup.find_all('title'))):\n",
    "                    pdd=soup.find_all('title')[i]\n",
    "                    if soup.find_all('title')[i].get_text() not in articles:\n",
    "                        articles.append(soup.find_all('title')[i].get_text())\n",
    "                        links.append(soup.find_all('link')[i].get_text())\n",
    "                        dates.append(soup.find_all('pubDate')[i-1].get_text())\n",
    "                        description.append(html.unescape(soup.find_all('description')[i].get_text()))\n",
    "\n",
    "                        scraped_date.append(current_time)\n",
    "\n",
    "                #cleaning description\n",
    "                \n",
    "                import re\n",
    "                for i in range(0,len(description)-1):\n",
    "                    art=description[i]\n",
    "                    art= art.lower().replace(\"don't\",\"do not\")\n",
    "                    art = art.replace('â€¦', '').replace('<span class=\"match\">', '').replace('</span>','').replace('\\n','').replace('     ','')\n",
    "                    art = art.replace(\",,\",\",\")\n",
    "                    cleaned_description.append(re.sub('\\n','',art))\n",
    "\n",
    "                cleaned_description\n",
    "\n",
    "            except:\n",
    "                print('Rss not working')\n",
    "                # not_working_functions.append('RSS')\n",
    "            df1 = pd.DataFrame(list(zip(articles,links,cleaned_description,dates,scraped_date)), \n",
    "                        columns =['title', 'link','text','publish_date','scraped_date'])\n",
    "\n",
    "            import html\n",
    "            for i in range(0,df1.shape[0]):\n",
    "                df1['title'][i]=html.unescape(df1['title'][i])\n",
    "\n",
    "\n",
    "            for i in range(0,df1.shape[0]):\n",
    "    #             print(df1['link'][i])\n",
    "                response=requests.get(df1['link'][i])\n",
    "                df1['link'][i]=response.url\n",
    "\n",
    "            Rss = FilterFunction(df1)\n",
    "            # emptydataframe(\"RSS\",Rss)\n",
    "            # logging.info(\"Rss function ended\")\n",
    "            # Rss  = link_correction(Rss)\n",
    "            return Rss\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def GoogleAlert():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            try:\n",
    "                print('10')\n",
    "                import html\n",
    "                url=\"https://www.google.co.in/alerts/feeds/15296043414695393299/12391429027627390948\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                    pdd=soup.find_all('entry')[i]\n",
    "                    for title in pdd.find_all('title'):\n",
    "                        articles.append(html.unescape(title.text))\n",
    "                    for link in pdd.find_all('link'):\n",
    "                        links.append(html.unescape(link['href']))\n",
    "                    for date in pdd.find_all('published'):\n",
    "                        dates.append(date.text)\n",
    "                        scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert is not working')\n",
    "                not_working_functions.append('Google Alert')\n",
    "                \n",
    "            df_google = pd.DataFrame(list(zip(articles,articles,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            \n",
    "            # df = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google alert\",df)\n",
    "            logging.info(\"GoogleAlert function ended\")\n",
    "            df_google  = link_correction(df_google)\n",
    "            return df_google\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "            \n",
    "\n",
    "    def live(keyword):\n",
    "        try:\n",
    "            print('11')\n",
    "            links = []\n",
    "            title = []\n",
    "            dates = []\n",
    "            s_date = []\n",
    "            text = []\n",
    "            try:\n",
    "                print('11')\n",
    "                url = f'https://www.livemint.com/Search/Link/Keyword/{keyword}'\n",
    "                url1 = 'https://www.livemint.com'\n",
    "                logging.info(\"live.com: invoking requests.url()=\" + url)\n",
    "                page = requests.get(url,verify = False)\n",
    "                logging.info(\"live.com: completed requests.url()=\" + url)\n",
    "\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                for a in soup.find_all('h2', {'class': 'headline'}):\n",
    "                    for i in a.find_all('a',href=True):\n",
    "                        #print(i['href'])\n",
    "                        links.append(url1 + i[\"href\"])\n",
    "\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"h1\", { \"class\" : \"headline\" }).text\n",
    "                    #print(x)\n",
    "                    title.append(x)\n",
    "                    y = sp.find(\"span\", { \"class\" : \"articleInfo pubtime\" }).text\n",
    "                    dates.append(y)\n",
    "                    z = sp.find(\"div\", { \"class\" : \"mainArea\" }).text\n",
    "                    #print(z)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "                    text.append(z)\n",
    "            except:\n",
    "                print('Live is not working')\n",
    "                not_working_functions.append('Live mint')\n",
    "\n",
    "            live = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                                columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            \n",
    "            df = FilterFunction(live)\n",
    "            emptydataframe(\"Livemint\",df)\n",
    "            logging.info(\"Live function ended\")\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def xinhuanet():\n",
    "        try:\n",
    "            try:\n",
    "                print('13')\n",
    "                url = \"http://www.xinhuanet.com/english/mobile/business.htm\"\n",
    "                logging.info(\"www.xinhuanet.com: invoking requests.url()=\" + url)\n",
    "                page=requests.get(url)\n",
    "                logging.info(\"www.xinhuanet.com: completed invoking requests.url()=\" + url)\n",
    "                soup=BeautifulSoup(page.content,'lxml')\n",
    "                lists = []\n",
    "                title = []\n",
    "                dates = []\n",
    "                text = []\n",
    "                s_date = []\n",
    "                for h in soup.findAll('li'):\n",
    "                    for k in h.findAll('a'):\n",
    "                        lists.append(k['href'])\n",
    "\n",
    "                links= [line for line in lists if 'c_' in line]\n",
    "\n",
    "                for link in links:\n",
    "                    fetch = requests.get(link)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"h1\", { \"class\" : \"Btitle\" })\n",
    "                    if x is not None:\n",
    "                        s = x.text\n",
    "                    else:\n",
    "                        s = None\n",
    "                    #print(s)\n",
    "                        #print(s)\n",
    "                    title.append(s)\n",
    "                    y = sp.find(\"i\", { \"class\" : \"time\" }).text\n",
    "                        #print(y)\n",
    "                    dates.append(y)\n",
    "                    z = sp.find(\"div\", { \"class\" : \"content\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    #print(n)\n",
    "                    #print(n)\n",
    "                    text.append(n)\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('xinhuanet is not working')\n",
    "                not_working_functions.append('Xinhuanet')\n",
    "            import pandas as pd\n",
    "\n",
    "\n",
    "            df2 = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text']) \n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Xinhuanet\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "\n",
    "    def kontan(keyword):\n",
    "        try:\n",
    "            links = []\n",
    "            title = []\n",
    "            dates = []\n",
    "            text = []\n",
    "            s_date = []\n",
    "            try:\n",
    "                print('14')\n",
    "                \n",
    "                url = f\"https://www.kontan.co.id/search/?search={keyword}&Button_search=\"\n",
    "                url1 = \"https:\"\n",
    "                logging.info(\"www.kontan.co.id: invoking requests.url()=\" + url)\n",
    "                page=requests.get(url)\n",
    "                logging.info(\"www.kontan.co.id: completed invoking requests.url()=\" + url)\n",
    "                soup=BeautifulSoup(page.content,'lxml')\n",
    "                for divtag in soup.find_all('div', {'class': 'sp-hl linkto-black'}):\n",
    "                        for a in divtag.find_all('a',href=True):\n",
    "                            links.append(url1 + a['href'])\n",
    "                for link in links:\n",
    "                    fetch = requests.get(link)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"h1\", { \"class\" : \"detail-desk\" })\n",
    "                    if x is not None:\n",
    "                        s = x.text\n",
    "                    else:\n",
    "                        s = None\n",
    "                    #print(s)\n",
    "                        #print(s)\n",
    "                    title.append(s)\n",
    "                    y = sp.find(\"div\", { \"class\" : \"fs14 ff-opensans font-gray\" })\n",
    "                    if y is not None:\n",
    "                        k = y.text\n",
    "                        k = k.replace('Mei','May')\n",
    "                        k = k.split(',')\n",
    "                        k = k[1]\n",
    "                    else:\n",
    "                        k = None\n",
    "                        #print(y)\n",
    "                    dates.append(k)\n",
    "                    z = sp.find(\"div\", { \"class\" : \"tmpt-desk-kon\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    #print(n)\n",
    "                    #print(n)\n",
    "                    text.append(n)\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('kontan not working')\n",
    "                not_working_functions.append(\"Kontan\")\n",
    "            import pandas as pd\n",
    "\n",
    "\n",
    "            df2 = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            \n",
    "            from googletrans import Translator\n",
    "            # import googletrans\n",
    "            translator = Translator()\n",
    "            def convert_to_arabic(df2):\n",
    "                translator = Translator()\n",
    "                new_list = [];new_list1=[]\n",
    "                for i in df2[\"title\"]:\n",
    "                    translations = translator.translate(i,dest=\"en\")\n",
    "    #             print(translations.text)\n",
    "                    new_list.append(str(translations.text))\n",
    "                df2[\"title\"] = new_list\n",
    "                return df2\n",
    "            \n",
    "            \n",
    "            # df = convert_to_arabic(df2)\n",
    "    #         print(df2.head())\n",
    "            df = translate_dataframe(df2)\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Kontan\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    \n",
    "    def AZ(keyword):\n",
    "        try:\n",
    "            print(\"AZ\")\n",
    "            err_logs = []\n",
    "            url = f\"https://en.trend.az/search?query={keyword}\"\n",
    "            domain_url = \"https://en.trend.az/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            div_class = \"inlineSearchResults\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_div_class = \"top-part\"\n",
    "            date_span_class = \"date-time\"\n",
    "            para_div_class = \"article-content\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"trend.az\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"div\", {\"class\": title_div_class})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": date_span_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                try:\n",
    "                    para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"AZ\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"AZ is not working\")\n",
    "            not_working_functions.append(\"AZ\")\n",
    "    def German():\n",
    "        try :\n",
    "            print(\"German\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.tagesschau.de/suche2.html?query=IPO&sort_by=date\"\n",
    "            domain_url = \"https://www.tagesschau.de\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"German : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"h3\",{\"class\":\"headline\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"German : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"span\" , {\"class\" : \"seitenkopf__headline--text\"}).text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"metatextline\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"main\" , {\"class\" : \"content-wrapper content-wrapper--show-cuts\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\" , {\"class\" : \"m-ten  m-offset-one l-eight l-offset-two textabsatz columns twelve\"}):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"German : err: Empty datframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            # df = FilterFunction(df)\n",
    "            emptydataframe(\"German\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"German not working\")\n",
    "            not_working_functions.append('German')\n",
    "    def Japannews():\n",
    "        try:\n",
    "            title=[]\n",
    "            text=[]\n",
    "            dates = []\n",
    "            s_date = []\n",
    "            text = []\n",
    "            try:\n",
    "                print('Japannews')\n",
    "                url = \"https://www.japantimes.co.jp/tag/ipo/\"\n",
    "                logging.info(\"Japannews: invoking requests.url()=\" + url)\n",
    "                page=requests.get(url)\n",
    "                logging.info(\"Japannews: Completed invoking requests.url()=\" + url)\n",
    "                soup=BeautifulSoup(page.content,'html.parser')\n",
    "                #print(soup)\n",
    "                links=[]\n",
    "                for divtag in soup.find_all('div', {'class': 'main_content'}):\n",
    "                        for a in divtag.find_all('a',href=True):\n",
    "                            if a[\"href\"].startswith(\"http\"):\n",
    "    #                             print(a[\"href\"])\n",
    "                                links.append(a[\"href\"])\n",
    "\n",
    "\n",
    "                for l in links:\n",
    "                    try:\n",
    "                        fetch = requests.get(l)\n",
    "                        sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                        #print(sp)\n",
    "                        title.append(sp.find(\"h1\").text)\n",
    "\n",
    "                        t=sp.find(\"div\", {'class': 'entry'})\n",
    "                        x=t.find_all(\"p\")\n",
    "                        tet=[]\n",
    "                        for i in x:\n",
    "                            tet.append(i.text)\n",
    "                        text.append(''.join(tet))   \n",
    "\n",
    "                        j=sp.find('div',{'class': 'meta-right'})\n",
    "                        m=j.find_all('li')\n",
    "        #                 print(m[2].text)\n",
    "                        dates.append(m[2].text.strip())\n",
    "                        from datetime import datetime, date\n",
    "                        cur_date = str(datetime.today())\n",
    "                        s_date.append(cur_date)     \n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                print('japan not working')\n",
    "                not_working_functions.append(\"Japan\")\n",
    "\n",
    "            japanese = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                                columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            japanese['publish_date']  = pd.to_datetime(japanese['publish_date'],errors='coerce',utc=True).dt.strftime('%d-%b-%Y' \" \" \"%H:%M:%S\")\n",
    "            df = FilterFunction(japanese)\n",
    "            emptydataframe(\"Japan\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Romania():\n",
    "        try:\n",
    "            try:\n",
    "                print('Romania')\n",
    "                url = \"https://adevarul.ro/cauta/?terms=ofert%C4%83%20public%C4%83%20ini%C8%9Bial%C4%83&fromDate=2012-1-1&toDate=2021-3-10&tab=mrarticle&page=1&sortBy=cronologic\"\n",
    "                url1 = \"https://adevarul.ro\"\n",
    "                logging.info(\"Romania: invoking requests.url()=\" + url)\n",
    "                page=requests.get(url).content\n",
    "                logging.info(\"Romania: Completed invoking requests.url()=\" + url)\n",
    "                unicode_str = page.decode(\"utf-8\")\n",
    "                encoded_str = unicode_str.encode(\"ascii\",'ignore')\n",
    "                soup = BeautifulSoup(encoded_str, \"html.parser\")\n",
    "\n",
    "                links = []\n",
    "                dates = []\n",
    "                s_date = []\n",
    "                text = []\n",
    "                title = []\n",
    "\n",
    "                for divtag in soup.find_all('h2', {'class': 'defaultTitle'}):\n",
    "                    for a in divtag.find_all('a',href=True):\n",
    "                        links.append(url1 + a[\"href\"])\n",
    "\n",
    "                for link in links:\n",
    "                    fetch = requests.get(link)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find('h1').text\n",
    "                    title.append(x)\n",
    "\n",
    "                    y = sp.findAll(\"aside\", { \"class\" : \"tools clearfix\" })\n",
    "\n",
    "                    for i in y:\n",
    "                        date_time = i.time['datetime']\n",
    "\n",
    "                        dates.append(date_time)\n",
    "\n",
    "                    z = sp.find(\"div\", { \"class\" : \"article-body\" }).text\n",
    "            #         print(z)\n",
    "                    text.append(z)\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('romania is not working')\n",
    "                not_working_functions(\"Romania\")\n",
    "            romania = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            # romania['publish_date']  = pd.to_datetime(romania['publish_date'],errors='coerce',utc=True).dt.strftime('%d-%b-%Y' \" \" \"%H:%M:%S\")\n",
    "            # df = FilterFunction(romania)\n",
    "            romania = translate_dataframe(romania)\n",
    "            romania = FilterFunction(romania)\n",
    "            emptydataframe(\"Romania\",romania)\n",
    "            #nonenglish\n",
    "            romania  = link_correction(romania)\n",
    "            return romania\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Swedish():\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            try:\n",
    "                print('Swedish')\n",
    "                url = \"https://www.svd.se/sok?q=ipo\"\n",
    "                url1 = \"https://www.svd.se\"\n",
    "                logging.info(\"Swedish: invoking requests.url()=\" + url)\n",
    "                page=requests.get(url).content\n",
    "                logging.info(\"Swedish: Completed invoking requests.url()=\" + url)\n",
    "                unicode_str = page.decode(\"utf-8\")\n",
    "                encoded_str = unicode_str.encode(\"ascii\",'ignore')\n",
    "                soup = BeautifulSoup(encoded_str, \"html.parser\")\n",
    "\n",
    "                links = []\n",
    "                dates = []\n",
    "                s_date = []\n",
    "                text = []\n",
    "                title = []\n",
    "\n",
    "                for divtag in soup.find_all('a', {'class': 'Teaser-link'}):\n",
    "                    links.append(url1 + divtag['href'])\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        fetch = requests.get(link)\n",
    "                    except requests.exceptions.ConnectionError:\n",
    "                        requests.status_code = \"Connection refused\"\n",
    "\n",
    "\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find('h1', {'class': 'ArticleHead-heading'})\n",
    "                    if x is not None:\n",
    "                            ementa = x.text\n",
    "                    else:\n",
    "                            ementa = None\n",
    "                    title.append(ementa)\n",
    "\n",
    "                    y = sp.findAll(\"div\", { \"class\" : \"Meta-part Meta-part--published\" })\n",
    "                    for x in y:\n",
    "                        x = x.get_text()\n",
    "                        x = re.sub('[^0-9-:.]+', ' ', x)\n",
    "                        dates.append(x)\n",
    "\n",
    "                    z = sp.find(\"div\", { \"class\" : \"Body\"})\n",
    "                    if z is not None:\n",
    "                            n = z.text\n",
    "                    else:\n",
    "                            n = None\n",
    "                    text.append(n)\n",
    "                    s_date.append(cur_date)\n",
    "\n",
    "            except:\n",
    "                print('swedish is not working')\n",
    "                not_working_functions.append(\"Swedish\")\n",
    "            swedish = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            # swedish['publish_date']  = pd.to_datetime(swedish['publish_date'],errors='coerce',utc=True).dt.strftime('%d-%b-%Y' \" \" \"%H:%M:%S\")\n",
    "            df = FilterFunction(swedish)\n",
    "            # emptydataframe(\"Swedish\",df)\n",
    "            swedish  = link_correction(swedish)\n",
    "            return swedish\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Spanish():\n",
    "        try:\n",
    "            try:\n",
    "                print('Spanish')\n",
    "                urls=[\n",
    "                    \"https://www.abc.es/rss/feeds/abc_EspanaEspana.xml\",\n",
    "                    \"http://ep01.epimg.net/rss/elpais/inenglish.xml\",\n",
    "                    \"https://feeds.thelocal.com/rss/es\",\n",
    "                    \"http://www.tenerifenews.com/feed/\"\n",
    "                ]\n",
    "                logging.info(\"www.fr.de: invoking requests.url()=\" + url)\n",
    "                page=requests.get(urls[0])\n",
    "                soup=BeautifulSoup(page.content,features='lxml')\n",
    "                soup.find_all()\n",
    "\n",
    "\n",
    "\n",
    "                title=[]\n",
    "                link=[]\n",
    "                publish_date=[]\n",
    "                scraped_date=[]\n",
    "                #finding date for the above script\n",
    "                import datetime\n",
    "                x=datetime.datetime.now()\n",
    "                x=x.date()\n",
    "                x=str(x)\n",
    "                x=x.split('-')\n",
    "                date=x[2]\n",
    "                #print(date)\n",
    "                month=x[1]\n",
    "                month=str(month)\n",
    "                #print(month)\n",
    "                if (month=='01'):\n",
    "                    mon='Jan'\n",
    "                elif (month=='02'):\n",
    "                    mon='Feb'\n",
    "                elif (month=='03'):\n",
    "                    mon='Mar'\n",
    "                elif (month=='04'):\n",
    "                    mon='Apr'\n",
    "                elif (month=='05'):\n",
    "                    mon='May'\n",
    "                elif (month=='06'):\n",
    "                    mon='Jun'\n",
    "                elif (month=='07'):\n",
    "                    mon='Jul'\n",
    "                elif (month=='08'):\n",
    "                    mon='Aug'\n",
    "                elif (month=='09'):\n",
    "                    mon='Sep'\n",
    "                elif (month=='10'):\n",
    "                    mon=='Oct'\n",
    "                elif (month=='11'):\n",
    "                    mon='Nov'\n",
    "                else:\n",
    "                    mon='Dec'\n",
    "                date=str(date)\n",
    "    \n",
    "                date=int(date)\n",
    "                date_int=date-1\n",
    "                date_int=str(date_int)\n",
    "                if(len(date_int)==1):\n",
    "                    date_int='0'+date_int\n",
    "                date_int=str(date_int)\n",
    "                date=str(date)\n",
    "                date_int=str(date_int)\n",
    "    \n",
    "\n",
    "                for i in range(0,len(soup.find_all('title'))):\n",
    "                    pub_date=soup.find_all('pubdate')[i-1].text\n",
    "                #print(pub_date)\n",
    "                    pub_date=pub_date.split(' ')\n",
    "                    pub_date = str(pub_date)\n",
    "                    pub_date = re.sub('[^A-Za-z0-9%:]+', ' ', pub_date)\n",
    "                    act_date=pub_date[1]\n",
    "                    act_mon=pub_date[2]\n",
    "                    #print(act_date,act_mon)\n",
    "                    act_date=str(act_date)\n",
    "                    act_mon=str(act_mon)\n",
    "                    #print(act_mon,mon)\n",
    "\n",
    "                    if(act_mon==mon):\n",
    "                        if(act_date==date or act_date==date_int):\n",
    "                            title.append(soup.find_all('title')[i].text)\n",
    "                            link.append(soup.find_all('guid')[i-1].text)\n",
    "                            scraped_date.append(pub_date)\n",
    "                            publish_date.append(pub_date)\n",
    "\n",
    "            except:\n",
    "                print('Spanish not working')\n",
    "                not_working_functions(\"Spanish\")\n",
    "\n",
    "            import pandas as pd \n",
    "            df2 = pd.DataFrame(list(zip(title,link,publish_date,scraped_date)), \n",
    "                        columns =['title','link','publish_date','scraped_date'])\n",
    "            df2\n",
    "\n",
    "\n",
    "            # In[175]:\n",
    "\n",
    "\n",
    "            title=[]\n",
    "            link=[]\n",
    "            publish_date=[]\n",
    "            scraped_date=[]\n",
    "\n",
    "\n",
    "            try:\n",
    "                page=requests.get(urls[2])\n",
    "                soup=BeautifulSoup(page.content,features='lxml')\n",
    "                soup.find_all()\n",
    "\n",
    "\n",
    "                # In[176]:\n",
    "\n",
    "\n",
    "                for i in range(0,len(soup.find_all('title'))):\n",
    "                        pub_date=soup.find_all('pubdate')[i-1].text\n",
    "                        #print(pub_date)\n",
    "                        pub_date=pub_date.split(' ')\n",
    "\n",
    "                        act_date=pub_date[1]\n",
    "                        act_mon=pub_date[2]\n",
    "                        #print(act_date,act_mon)\n",
    "                        act_date=str(act_date)\n",
    "                        act_mon=str(act_mon)\n",
    "                        #print(act_mon,mon)\n",
    "\n",
    "                        if(act_mon==mon):\n",
    "                            if(act_date==date or act_date==date_int):\n",
    "                                title.append(soup.find_all('title')[i].text)\n",
    "                                link.append(soup.find_all('guid')[i-1].text)\n",
    "                                scraped_date.append(pub)\n",
    "                                publish_date.append(pub_date)\n",
    "\n",
    "\n",
    "                # In[177]:\n",
    "            except:\n",
    "                print('Spanish not working')\n",
    "                not_working_functions.append(\"Spanish\")\n",
    "\n",
    "            import pandas as pd \n",
    "            df3 = pd.DataFrame(list(zip(title,link,publish_date,scraped_date)), \n",
    "                        columns =['title','link','publish_date','scraped_date'])\n",
    "            df3\n",
    "\n",
    "\n",
    "            # In[178]:\n",
    "\n",
    "\n",
    "            title=[]\n",
    "            link=[]\n",
    "            publish_date=[]\n",
    "            scraped_date=[]\n",
    "\n",
    "            try:\n",
    "                page=requests.get(urls[3])\n",
    "                soup=BeautifulSoup(page.content,features='lxml')\n",
    "                soup.find_all()\n",
    "\n",
    "\n",
    "                # In[179]:\n",
    "\n",
    "\n",
    "                for i in range(0,len(soup.find_all('lastBuildDate'))):\n",
    "                        pub_date=soup.find_all('pubdate')[i-1].text\n",
    "                #print(pub_date)\n",
    "                        pub_date=pub_date.split(' ')\n",
    "                        act_date=pub_date[1]\n",
    "                        act_mon=pub_date[2]\n",
    "                #print(act_date,act_mon)\n",
    "                        act_date=str(act_date)\n",
    "                        act_mon=str(act_mon)\n",
    "                #print(act_mon,mon)\n",
    "\n",
    "                        if(act_mon==mon):\n",
    "                            if(act_date==date or act_date==date_int):\n",
    "                                title.append(soup.find_all('title')[i].text)\n",
    "                                link.append(soup.find_all('guid')[i-1].text)\n",
    "                                scraped_date.append(pub)\n",
    "                                publish_date.append(pub_date)\n",
    "\n",
    "\n",
    "                # In[180]:\n",
    "            except:\n",
    "                print('Spanish not working')\n",
    "                not_working_functions.append((\"Spanish\"))\n",
    "\n",
    "            import pandas as pd \n",
    "            df4 = pd.DataFrame(list(zip(title,link,publish_date,scraped_date)), \n",
    "                        columns =['title','link','publish_date','scraped_date'])\n",
    "\n",
    "            frames = [df2,df3,df4]\n",
    "            fin=pd.concat(frames)\n",
    "            fin=fin.reset_index()\n",
    "\n",
    "            fin=fin.drop(['index'],axis=1)\n",
    "\n",
    "    #         get_ipython().system('pip install googletrans==3.1.0a0')\n",
    "            import sys\n",
    "\n",
    "\n",
    "            from googletrans import Translator\n",
    "            import googletrans\n",
    "\n",
    "\n",
    "            translator = Translator()\n",
    "            #translated=translator.translate(listtt[0][:2000],dest='en')\n",
    "\n",
    "\n",
    "            title=[]\n",
    "            link=[]\n",
    "            scraped_date=[]\n",
    "            publish_date=[]\n",
    "            for i in range(0,fin.shape[0]):\n",
    "                    translations=translator.translate(fin['title'][i])\n",
    "                    title.append(translations.text)\n",
    "                    link.append(fin['link'][i])\n",
    "                    scraped_date.append(fin['scraped_date'][i])\n",
    "                    publish_date.append(fin['publish_date'][i])\n",
    "            fin = pd.DataFrame(list(zip(title,link,publish_date,scraped_date)), \n",
    "                        columns =['title','link','publish_date','scraped_date'])\n",
    "            fin['publish_date']  = pd.to_datetime(fin['publish_date'],errors='coerce',utc=True).dt.strftime('%d-%b-%Y' \" \" \"%H:%M:%S\")\n",
    "            Fin = FilterFunction(fin)\n",
    "            emptydataframe(\"Spanish\",Fin)\n",
    "            Fin  = link_correction(Fin)\n",
    "            return Fin\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Russian():\n",
    "        try :\n",
    "            print(\"Russian\")\n",
    "            err_logs = []\n",
    "            url = \"https://ipo.einnews.com/search/IPO/?search%5B%5D=news&search%5B%5D=press&order=relevance\"\n",
    "            domain_url = \"https://ipo.einnews.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Russian : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"article-content\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.h3.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = Article(link)\n",
    "                    page.download()\n",
    "                    page.parse()\n",
    "                    title.append(page.title)  \n",
    "                  \n",
    "                    pub_date.append(page.publish_date)\n",
    "                    \n",
    "                    text.append(page.text)\n",
    "                    # print(text)\n",
    "\n",
    "                    #Scrapped date \n",
    "                    scraped_date.append(str(today))\n",
    "\n",
    "                    #Working links\n",
    "                    final_links.append(link)\n",
    "                except:\n",
    "                  continue\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Russian : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Russian\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Russian not working\")\n",
    "            not_working_functions.append('Russian')\n",
    "\n",
    "\n",
    "    def GoogleAlert1():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            text = []\n",
    "            try:\n",
    "                print('GoogleAlert1')\n",
    "                import html\n",
    "                url=\"https://www.google.com/alerts/feeds/01154643345605641334/12910713483086187784\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                        pdd=soup.find_all('entry')[i]\n",
    "                        for title in pdd.find_all('title'):\n",
    "                            articles.append(html.unescape(title.text))\n",
    "                        for content in pdd.find_all('content'):\n",
    "                            text.append(html.unescape(content.text))\n",
    "                        for link in pdd.find_all('link'):\n",
    "                            links.append(html.unescape(link['href']))\n",
    "                        for date in pdd.find_all('published'):\n",
    "                            dates.append(date.text)\n",
    "                            scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert is not working')\n",
    "                not_working_functions.append(\"Google alert 1\")\n",
    "            df_google = pd.DataFrame(list(zip(articles,text,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            Df2 = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google alert1\",Df2)\n",
    "            Df2  = link_correction(Df2)\n",
    "            return Df2\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def GoogleAlert2():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            text = []\n",
    "            try:\n",
    "                print('GoogleAlert2')\n",
    "                import html\n",
    "                url=\"https://www.google.com/alerts/feeds/01154643345605641334/6308064720496016673\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                        pdd=soup.find_all('entry')[i]\n",
    "                        for title in pdd.find_all('title'):\n",
    "                            articles.append(html.unescape(title.text))\n",
    "                        for content in pdd.find_all('content'):\n",
    "                            text.append(html.unescape(content.text))\n",
    "                        for link in pdd.find_all('link'):\n",
    "                            links.append(html.unescape(link['href']))\n",
    "                        for date in pdd.find_all('published'):\n",
    "                            dates.append(date.text)\n",
    "                            scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert2 is not working')\n",
    "                not_working_functions.append(\"Google Alert2\")\n",
    "            df_google = pd.DataFrame(list(zip(articles,text,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            Df2 = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google Alert 2\",Df2)\n",
    "            Df2  = link_correction(Df2)\n",
    "            return Df2\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def GoogleAlert3():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            text = []\n",
    "            try:\n",
    "                print('GoogleAlert3')\n",
    "                import html\n",
    "                url=\"https://www.google.com/alerts/feeds/01154643345605641334/13304767747222280933\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                        pdd=soup.find_all('entry')[i]\n",
    "                        for title in pdd.find_all('title'):\n",
    "                            articles.append(html.unescape(title.text))\n",
    "                        for content in pdd.find_all('content'):\n",
    "                            text.append(html.unescape(content.text))\n",
    "                        for link in pdd.find_all('link'):\n",
    "                            links.append(html.unescape(link['href']))\n",
    "                        for date in pdd.find_all('published'):\n",
    "                            dates.append(date.text)\n",
    "                            scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert 3 is not working')\n",
    "                not_working_functions.append(\"Google Alert 3\")\n",
    "            df_google = pd.DataFrame(list(zip(articles,text,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            Df2 = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google Alert 3\",Df2)\n",
    "            Df2  = link_correction(Df2)\n",
    "            return Df2\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def GoogleAlert4():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            text = []\n",
    "            try:\n",
    "                print('GoogleAlert4')\n",
    "                import html\n",
    "                url=\"https://www.google.com/alerts/feeds/01154643345605641334/3217985541207435755\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                        pdd=soup.find_all('entry')[i]\n",
    "                        for title in pdd.find_all('title'):\n",
    "                            articles.append(html.unescape(title.text))\n",
    "                        for content in pdd.find_all('content'):\n",
    "                            text.append(html.unescape(content.text))\n",
    "                        for link in pdd.find_all('link'):\n",
    "                            links.append(html.unescape(link['href']))\n",
    "                        for date in pdd.find_all('published'):\n",
    "                            dates.append(date.text)\n",
    "                            scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert4 is not working')\n",
    "                not_working_functions(\"Google Alert 4\")\n",
    "            df_google = pd.DataFrame(list(zip(articles,text,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            Df2 = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google Alert 4\",Df2)\n",
    "            Df2  = link_correction(Df2)\n",
    "            return Df2\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def GoogleAlert5():\n",
    "        try:\n",
    "            articles=[]\n",
    "            links=[]\n",
    "            dates=[]\n",
    "            scraped_date=[]\n",
    "            text = []\n",
    "            try:\n",
    "                print('GoogleAlert5')\n",
    "                import html\n",
    "                url=\"https://www.google.com/alerts/feeds/01154643345605641334/6882160862931884057\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,features='xml')\n",
    "\n",
    "                #print(soup.prettify())\n",
    "                for i in range(0,len(soup.find_all('entry'))):\n",
    "                        pdd=soup.find_all('entry')[i]\n",
    "                        for title in pdd.find_all('title'):\n",
    "                            articles.append(html.unescape(title.text))\n",
    "                        for content in pdd.find_all('content'):\n",
    "                            text.append(html.unescape(content.text))\n",
    "                        for link in pdd.find_all('link'):\n",
    "                            links.append(html.unescape(link['href']))\n",
    "                        for date in pdd.find_all('published'):\n",
    "                            dates.append(date.text)\n",
    "                            scraped_date.append(date.text)\n",
    "            except:\n",
    "                print('Google Alert5 is not working')\n",
    "                not_working_functions.append(\"Google alert 5\")\n",
    "            df_google = pd.DataFrame(list(zip(articles,text,links,dates,scraped_date)), \n",
    "                        columns =['title','text', 'link','publish_date','scraped_date'])\n",
    "\n",
    "            def clean_link(u):\n",
    "                    u = u.replace('https://www.google.com/url?rct=j&sa=t&url=', '')\n",
    "\n",
    "                    return(u)\n",
    "\n",
    "            urls = df_google['link']\n",
    "\n",
    "            clean_url = []\n",
    "            for url in urls:\n",
    "                clean_url.append(clean_link(url))\n",
    "\n",
    "            df_google['link'] = clean_url\n",
    "            # Df2 = FilterFunction(df_google)\n",
    "            emptydataframe(\"Google Alert 5\",Df2)\n",
    "            df_google  = link_correction(df_google)\n",
    "            return df_google\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def IPOMonitor():\n",
    "        try:\n",
    "            links = []\n",
    "            title = []\n",
    "            dates = []\n",
    "            s_date = []\n",
    "            text = []\n",
    "            try:\n",
    "                print('IPOmonitor')\n",
    "                url = \"https://www.ipomonitor.com/pages/ipo-news.html\"\n",
    "                page=requests.get(url)\n",
    "                soup=BeautifulSoup(page.content,'html.parser')\n",
    "                for divtag in soup.find_all('span'):\n",
    "                    for a in divtag.find_all('a',href=True):\n",
    "\n",
    "                        links.append(a[\"href\"])\n",
    "                        title.append(a.text)\n",
    "                for i in soup.find_all('dd'):\n",
    "                    for k in i.find_all('span'):\n",
    "                        s = k.text\n",
    "                        dates.append(s)\n",
    "                        s_date.append(cur_date)\n",
    "                        text.append(' ')\n",
    "            except:\n",
    "                print('Ipo monitor not working')\n",
    "                not_working_functions.append(\"IPOMonitor\")\n",
    "            \n",
    "            df2 = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"IPO Monitor\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def Globallegal():\n",
    "        try:\n",
    "            try:\n",
    "                urls = \"https://www.globallegalchronicle.com/?s=ipo\"\n",
    "                links = [] \n",
    "                datess=[]\n",
    "                title =[]\n",
    "                dates=[]\n",
    "                s_date=[]\n",
    "                text =[]\n",
    "                page=requests.get(urls)\n",
    "                soup=BeautifulSoup(page.content,'html')\n",
    "                divs = soup.find_all('article',{'class':'widget-entry entry-large col-sm-6'})\n",
    "                for i in divs:\n",
    "                    links.append(i.div.a[\"href\"])\n",
    "\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"h1\", { \"class\" : \"entry-title\" })\n",
    "                    if x is not None:\n",
    "                        ementa = x.text\n",
    "                    else:\n",
    "                        ementa = None\n",
    "                    title.append(ementa)\n",
    "                    y=sp.find(\"time\", { \"class\" : \"entry-date published updated\" })\n",
    "                    if y is not None:\n",
    "                        d = y.text\n",
    "                    else:\n",
    "                        d = None\n",
    "                    dates.append(d)\n",
    "        #             print(dates)\n",
    "                    for i in dates:\n",
    "                        i = i.replace('maio','may')\n",
    "                        i = i.replace('h',':')\n",
    "                        i = i.replace('Por Redação SpaceMoney','')\n",
    "                        datess.append(i)\n",
    "                    z=sp.find(\"div\", { \"class\" : \"content\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    text.append(n)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('Globallegal not working')\n",
    "                not_working_functions.append(\"Gloabllegal\")\n",
    "            import pandas as pd\n",
    "            df2 = pd.DataFrame(list(zip(title,links,datess,s_date,text)), \n",
    "                            columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Globallegal\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def Seenews():\n",
    "        try:\n",
    "            links = [] \n",
    "            title = []\n",
    "            dates = []\n",
    "            text = []\n",
    "            s_date = []\n",
    "            try:\n",
    "                print('Seenews')\n",
    "                urls = \"https://seenews.com/news/search_results/?keywords=ipo&order_by=name&order=asc&optradio=on&company_id=&company_owner=&capital_from=&capital_to=&total_assets_from=&total_assets_to=&total_revenue_from=&total_revenue_to=&number_of_employees_from=&number_of_employees_to=&net_profit_from=&net_profit_to=&net_loss_from=&net_loss_to=&seeci_from=&seeci_to=&ebitda_from=&ebitda_to=&year=&statement_type=\"\n",
    "                url1 = \"https://seenews.com\"\n",
    "                page=requests.get(urls)\n",
    "                soup=BeautifulSoup(page.content,'html')\n",
    "                divs = soup.find_all('dt',{'class':'search-result-title'})\n",
    "                for i in divs:\n",
    "                    links.append(url1 + i.a[\"href\"])\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"div\", { \"class\" : \"heading--content f-java\" })\n",
    "                    if x is not None:\n",
    "                        ementa = x.text\n",
    "                    else:\n",
    "                        ementa = None\n",
    "                    title.append(ementa)\n",
    "                    y=sp.find(\"div\", { \"class\" : \"post-date\" })\n",
    "                    if y is not None:\n",
    "                        d = y.text\n",
    "                        d = d.split()\n",
    "                        d = d[1:5]\n",
    "                        d = ' '.join(d)\n",
    "                    else:\n",
    "                        d = None\n",
    "                    dates.append(d)\n",
    "                    z=sp.find(\"div\", { \"class\" : \"content-description\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    text.append(n)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('Seenews not working')\n",
    "                not_working_functions.append(\"Seenews\")\n",
    "            df2 = pd.DataFrame(list(zip(title,links,dates,s_date,text)), \n",
    "                    columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Seenews\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Bisnis():\n",
    "        try :\n",
    "            print(\"Bisnis\")\n",
    "            err_logs = []\n",
    "            url = \"https://search.bisnis.com/?q=IPO\"\n",
    "            domain_url = \"https://www.reuters.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Bisnis : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"col-sm-8\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Bisnis : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "            \n",
    "                #Title of article \n",
    "                if(soup.find(\"h1\" , {\"class\" : \"title-only\"}) == None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"title-only\"}).text)  \n",
    "                # print(title)\n",
    "\n",
    "                if(soup.find(\"div\" , {\"class\" :\"author\"}) == None):\n",
    "                    continue \n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"author\"}).span)\n",
    "                #Text of article\n",
    "                if(soup.find(\"div\" , {\"class\" : \"col-sm-10\"}) == None ) :\n",
    "                    continue \n",
    "                div = soup.find(\"div\" , {\"class\" : \"col-sm-10\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                \n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Bisnis : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df.head(10)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Bisnis\",df)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Bisnis not working\")\n",
    "            not_working_functions.append('bisnis')\n",
    "\n",
    "    \n",
    "    def RomaniaNew():\n",
    "        try:\n",
    "            try:\n",
    "                print('RomaniaNew')\n",
    "                urls = \"https://www.romania-insider.com/search/node?keys=ipo\"\n",
    "                links = [] \n",
    "                title = []\n",
    "                dates= []\n",
    "                s_date = []\n",
    "                text = []\n",
    "                page=requests.get(urls)\n",
    "                soup=BeautifulSoup(page.content,'html')\n",
    "                h = soup.find_all('h3',{'class':'search-result__title'})\n",
    "                for i in h:\n",
    "                    links.append(i.a[\"href\"])\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "                    x=sp.find(\"h1\", {\"class\" : \"field field--name-field-title field--type-string field--label-hidden field__item\" })\n",
    "                    if x is not None:\n",
    "                        ementa = x.text\n",
    "                    else:\n",
    "                        ementa = None\n",
    "                    title.append(ementa)\n",
    "                    y=sp.find(\"div\", { \"class\" : \"field field--name-field-date field--type-datetime field--label-hidden field__item\" })\n",
    "                    if y is not None:\n",
    "                        d = y.text\n",
    "                    else:\n",
    "                        d = None\n",
    "                    dates.append(d)\n",
    "                    z=sp.find(\"div\", { \"class\" : \"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    text.append(n)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "                \n",
    "                df2 = pd.DataFrame({\"text\":text,\"link\":links,\"publish_date\":dates,\"scraped_date\":s_date,\"title\":title})\n",
    "                df2 = FilterFunction(df2)\n",
    "                emptydataframe(\"Romania New \",df2)\n",
    "                df2  = link_correction(df2)\n",
    "                return df2\n",
    "            except:\n",
    "                print('RomaniaNew not working')\n",
    "                not_working_functions.append(\"Romania New\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1   \n",
    "\n",
    "    def romania_insider():\n",
    "        try:\n",
    "            print(\"Romania Insider Dominician Republic\")\n",
    "            err_logs = []\n",
    "            err_index = []\n",
    "            baseSearchUrl = \"https://www.romania-insider.com/search/node?keys=ipo\"\n",
    "            domainUrl = \"https://www.romania-insider.com\"\n",
    "\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            queryUrl = baseSearchUrl\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                pageSource = requests.get(queryUrl, headers=headers)\n",
    "            except:\n",
    "                err = \"romania_insider: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "            # with open(\"response.html\", \"wb\") as f:\n",
    "            #     f.write(pageSource.content)\n",
    "            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "            for item in parsedSource.find(\"ol\", class_=\"search-results node_search-results\").find_all(\"li\"):\n",
    "                requiredTag = item.find(\"h3\").find(\"a\")\n",
    "                currentArticleTitle = requiredTag.text\n",
    "                # print(currentArticleTitle)\n",
    "                currentArticleLink = requiredTag[\"href\"]\n",
    "                # print(currentArticleLink)\n",
    "                if currentArticleLink[0] == \"/\":\n",
    "                    links.append(domainUrl + currentArticleLink)\n",
    "                else:\n",
    "                    links.append(currentArticleLink)\n",
    "                titles.append(currentArticleTitle)\n",
    "\n",
    "            scrapedData[\"title\"] = titles\n",
    "            scrapedData[\"link\"] = links\n",
    "            # print(titles)\n",
    "            # print(links)\n",
    "\n",
    "            # Article's date and description scraping\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            ArticleBody = []\n",
    "            for link in links:\n",
    "                articleText = \"\"\n",
    "                try:\n",
    "                    pageSource = requests.get(link, headers=headers)\n",
    "                    parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                except:\n",
    "                    err = \"romania_insider: err: Failed to access article link : \" + link\n",
    "                    ArticleDates.append(\"Error\")\n",
    "                    err_logs.append(err)\n",
    "                    err_index.append(link)\n",
    "                    continue\n",
    "                # with open(\"response.html\", \"wb\") as f:\n",
    "                #     f.write(pageSource.content)\n",
    "                # break\n",
    "                if parsedSource.find(\"div\", class_=\"field field--name-field-date field--type-datetime field--label-hidden field__item\"):\n",
    "                    sourceDateTimeTag = parsedSource.find(\"div\", class_=\"field field--name-field-date field--type-datetime field--label-hidden field__item\")\n",
    "                else:\n",
    "                    err = \"romania_insider: err: Failed to retrieve date from article : \" + link\n",
    "                    ArticleDates.append(\"Error\")\n",
    "                    err_logs.append(err)\n",
    "                    err_index.append(link)\n",
    "                    continue\n",
    "                sourceDateTime = datetime.strptime(sourceDateTimeTag.text, \"%d %B %Y\").strftime(\"%d-%m-%Y\")\n",
    "                # print(sourceDateTime)\n",
    "                ArticleDates.append(sourceDateTime)\n",
    "                ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                # print(ArticleDates)\n",
    "                # if parsedSource.find(\"div\", class_=\"entry-content\"):\n",
    "                #     textBodyDiv = parsedSource.find(\"div\", class_=\"entry-content\")\n",
    "                # else:\n",
    "                #     err = \"romania_insider: err: Failed to retrieve article text: \" + link\n",
    "                #     ArticleBody.append(\"Error\")\n",
    "                #     err_logs.append(err)\n",
    "                #     err_index.append(link)\n",
    "                #     continue\n",
    "                for item in parsedSource.find(\"div\", class_=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\").find_all(\"p\"):\n",
    "                    articleText += item.text.strip()\n",
    "                print(articleText)\n",
    "                ArticleBody.append(articleText)\n",
    "            scrapedData[\"publish_date\"] = ArticleDates\n",
    "            scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "            scrapedData[\"text\"] = ArticleBody\n",
    "            # print(ArticleBody)\n",
    "\n",
    "            # Clean and Normalize links\n",
    "            if len(err_index) != 0:\n",
    "                for e in err_index:\n",
    "                    idx = scrapedData[\"link\"].index(e)\n",
    "                    scrapedData[\"link\"].pop(idx)\n",
    "                    scrapedData[\"title\"].pop(idx)\n",
    "                    scrapedData[\"publish_date\"].pop(idx)\n",
    "\n",
    "            # DataFrame creation\n",
    "            romania_insiderDF = pd.DataFrame(scrapedData)\n",
    "            romania_insiderDF = romania_insiderDF.drop_duplicates(subset=[\"link\"])\n",
    "            if romania_insiderDF.empty:\n",
    "                err = \"romania_insider: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(romania_insiderDF)\n",
    "            emptydataframe(\"Romania Insider\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Romania Insider Dominic Republic not working\")\n",
    "            not_working_functions.append(\"Romania Insider Dominic Rep\")\n",
    "    def cnbc1():\n",
    "        try:\n",
    "            print(\"CNBC Barbados\")\n",
    "            err_logs = []\n",
    "            baseSearchUrl = \"https://www.cnbc.com/id/10000666/device/rss\"\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            err_index = []\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            ArticleBody = []\n",
    "            queryUrl = baseSearchUrl\n",
    "            try:\n",
    "                pageSource = requests.get(baseSearchUrl)\n",
    "            except:\n",
    "                err = \"cnbc: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                exit()\n",
    "            # with open(\"response.xml\", \"wb\") as f:\n",
    "            #     f.write(pageSource.content)\n",
    "            # break\n",
    "            parsedSource = BeautifulSoup(pageSource.content, \"xml\")\n",
    "            for eachItem in parsedSource.find_all(\"item\"):\n",
    "                currentArticleTitle = eachItem.find(\"title\").text\n",
    "                currentArticleLink = eachItem.find(\"link\").text\n",
    "                currentArticleDate = datetime.strptime(str(eachItem.find(\"pubDate\").text).split(\"GMT\", maxsplit=2)[0].strip(),\n",
    "                                                    \"%a, %d %b %Y %H:%M\").strftime(\"%d-%m-%Y\")\n",
    "                # articleText = str(eachItem.find(\"description\").text).split(\"CDATA[ \", maxsplit=2)[1].rstrip(\" ]]\")\n",
    "                articleText = str(eachItem.find(\"description\").text).split(\"CDATA[ \", maxsplit=2)[0].replace(\"#039;\", \"\")\n",
    "                ArticleBody.append(articleText)\n",
    "\n",
    "                titles.append(currentArticleTitle)\n",
    "                links.append(currentArticleLink)\n",
    "                ArticleDates.append(currentArticleDate)\n",
    "                ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "            scrapedData[\"title\"] = titles\n",
    "            scrapedData[\"link\"] = links\n",
    "            scrapedData[\"publish_date\"] = ArticleDates\n",
    "            scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "            scrapedData[\"text\"] = ArticleBody\n",
    "\n",
    "            # DataFrame creation\n",
    "            cnbcDF = pd.DataFrame(scrapedData)\n",
    "            cnbcDF = cnbcDF.drop_duplicates(subset=[\"link\"])\n",
    "            if cnbcDF.empty:\n",
    "                err = \"cnbc: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(cnbcDF)\n",
    "            # emptydataframe(\"Cnbc barbados\",df)\n",
    "            # log_errors(err_logs)\n",
    "            # df  = link_correction(df)\n",
    "            return cnbcDF        \n",
    "        except:\n",
    "            not_working_functions.append(\"CNBC Barbados\")\n",
    "            print(\"CNBC Barbados not working\")\n",
    "    def RomaniaInsider():\n",
    "        try:\n",
    "            try:\n",
    "                print('RomaniaInsider')\n",
    "                urls = \"https://www.romania-insider.com/index.php/daily-news/capital-markets?page=0\"\n",
    "                url1 = \"https://www.romania-insider.com\"\n",
    "\n",
    "                links = [] \n",
    "                title = []\n",
    "                dates= []\n",
    "                s_date = []\n",
    "                text = []\n",
    "                page=requests.get(urls)\n",
    "                soup=BeautifulSoup(page.content,'html')\n",
    "                h = soup.find_all('div',{'class':'field field--name-field-title field--type-string field--label-hidden field__item'})\n",
    "                for i in h:\n",
    "                    links.append(url1 + i.a[\"href\"])\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "\n",
    "                    x=sp.find(\"h1\", {\"class\" : \"field field--name-field-title field--type-string field--label-hidden field__item\" })\n",
    "                    if x is not None:\n",
    "                        ementa = x.text\n",
    "                    else:\n",
    "                        ementa = None\n",
    "                    title.append(ementa)\n",
    "                    y=sp.find(\"div\", { \"class\" : \"field field--name-field-date field--type-datetime field--label-hidden field__item\" })\n",
    "                    if y is not None:\n",
    "                        d = y.text\n",
    "                    else:\n",
    "                        d = None\n",
    "                    dates.append(d)\n",
    "                    z=sp.find(\"div\", { \"class\" : \"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    text.append(n)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('RomaniaInsider not working')\n",
    "                not_working_functions.append(\"RomaniaInsider\")\n",
    "\n",
    "            df2 = pd.DataFrame({\"text\":text,\"link\":links,\"publish_date\":dates,\"scraped_date\":s_date,\"title\":title})\n",
    "            df2 = FilterFunction(df2)\n",
    "            emptydataframe(\"RomaniaInsider\",df2)\n",
    "            df2  = link_correction(df2)\n",
    "            return df2\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def SpaceMoney():\n",
    "        try:\n",
    "            try:\n",
    "                print('spacemoney')\n",
    "                urls = \"https://www.spacemoney.com.br/noticias/ipos/\"\n",
    "                links = [] \n",
    "                title = []\n",
    "                dates = []\n",
    "                s_date = []\n",
    "                text = []\n",
    "                page=requests.get(urls)\n",
    "                soup=BeautifulSoup(page.content,'html')\n",
    "                divs = soup.find_all('div',{'class':'linkNoticia crop'})\n",
    "                for i in divs:\n",
    "                    links.append(i.a[\"href\"])\n",
    "                for l in links:\n",
    "                    fetch = requests.get(l)\n",
    "                    sp = BeautifulSoup(fetch.content, 'html.parser')\n",
    "\n",
    "                    x=sp.find(\"h1\")\n",
    "                    if x is not None:\n",
    "                        ementa = x.text\n",
    "                    else:\n",
    "                        ementa = None\n",
    "                    title.append(ementa)\n",
    "                    y=sp.find(\"section\", { \"class\" : \"dataAutor\" })\n",
    "                    if y is not None:\n",
    "                        d = y.text\n",
    "                    else:\n",
    "                        d = None\n",
    "                    dates.append(d)\n",
    "                    z=sp.find(\"article\", { \"class\" : \"grid_8 alpha\" })\n",
    "                    if z is not None:\n",
    "                        n = z.text\n",
    "                    else:\n",
    "                        n = None\n",
    "                    text.append(n)\n",
    "                    from datetime import datetime, date\n",
    "                    cur_date = str(datetime.today())\n",
    "                    s_date.append(cur_date)\n",
    "            except:\n",
    "                print('Spacemoney not working')\n",
    "                not_working_functions.append(\"Spacemoney\")\n",
    "\n",
    "            df2 = pd.DataFrame({\"text\":text,\"link\":links,\"publish_date\":dates,\"scraped_date\":s_date,\"title\":title})\n",
    "            # df2 = FilterFunction(df2)\n",
    "            df = translate_dataframe(df2)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Spacemoney\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def Carteira():\n",
    "        try:\n",
    "            # print(\"Carteira\")\n",
    "            body = [] \n",
    "            link = []\n",
    "            date = []\n",
    "            title=[]\n",
    "            # link=[]\n",
    "            pub_date=[]\n",
    "            scraped_date=[]\n",
    "            try:\n",
    "                print('Carteira')\n",
    "                urls = \"https://carteirasa.com.br/?s=Ipo\"\n",
    "                def get_links(urls):\n",
    "\n",
    "                        links = [] \n",
    "                        page=requests.get(urls)\n",
    "                        soup=BeautifulSoup(page.content,'html')\n",
    "                        divs = soup.find_all('div',{'class':'item-mid'})\n",
    "                        for i in divs:\n",
    "                            links.append(i.a[\"href\"])\n",
    "                        return links\n",
    "                \n",
    "                links1 = get_links(urls)\n",
    "                \n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(current_time)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "\n",
    "                df = pd.DataFrame({\"Article\":body,\"Link\":link,\"Publish Date\":date,\"Scrape Date\":scraped_date,\"Title\":title})\n",
    "\n",
    "                links1 = get_links(urls)\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(current_time)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "\n",
    "                df = pd.DataFrame({\"Article\":body,\"Link\":link,\"Publish Date\":date,\"Scrape Date\":scraped_date,\"Title\":title})\n",
    "\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(current_time)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "            except:\n",
    "                print('Carteira is not working')\n",
    "                not_working_functions.append(\"Carteira\")\n",
    "            dic = {\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title}\n",
    "\n",
    "            df = pd.DataFrame.from_dict(dic,orient='index')\n",
    "            df = df.transpose()\n",
    "            df.dropna(inplace=True)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Carteira\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def Kontan1():\n",
    "        try :\n",
    "            print(\"Kontan1\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.kontan.co.id/search/?search=ipo&Button_search=\"\n",
    "            domain_url = \"https://www.kontan.co.id\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Kontan1 : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"sp-hl linkto-black\"})\n",
    "            for div in all_divs:\n",
    "                links.append(\"https:\"+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Kontan1 : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                if(soup.find(\"h1\" , {\"class\" : \"detail-desk\"})== None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"detail-desk\"}).text)\n",
    "                print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"fs14 ff-opensans font-gray\"}))\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"tmpt-desk-kon\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":scraped_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Kontan1 : err: Empty datKontan1ame\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Kontan1\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Kontan1 not working\")\n",
    "            not_working_functions.append('Kontan1')\n",
    "    def euronews():\n",
    "        try:\n",
    "            print(\"Euronews\")\n",
    "            try:\n",
    "                urls = \"https://www.euronews.com/search?query=ipo\"\n",
    "                def get_links(urls):\n",
    "                    links = [] \n",
    "                    page=requests.get(urls)\n",
    "                    soup=BeautifulSoup(page.content,'html')\n",
    "                    divs = soup.find_all('h3',{'class':'m-object__title qa-article-title'})\n",
    "                    for i in divs:\n",
    "                        links.append(i.a[\"href\"])\n",
    "                    return links\n",
    "                title=[]\n",
    "                pub_date=[]\n",
    "                scraped_date=[]\n",
    "                links1 = get_links(urls)\n",
    "                body = [] \n",
    "                link = []\n",
    "                date = []\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "                #   print(ur)\n",
    "                        ur='https://www.euronews.com'+ur\n",
    "                        try:\n",
    "                            article = Article(ur)\n",
    "                            article.download()\n",
    "                            article.parse()\n",
    "                            # print(article.text)\n",
    "\n",
    "                            link.append(ur)\n",
    "                            scraped_date.append(now)\n",
    "                            body.append(article.text)\n",
    "                            title.append(article.title)\n",
    "                            date.append(article.publish_date)\n",
    "\n",
    "                        except:\n",
    "                            continue\n",
    "                        time.sleep(0.5)\n",
    "            except:\n",
    "                    print(\"Euronews not working\")\n",
    "                    not_working_functions.append(\"Euronews\")\n",
    "            df2 = pd.DataFrame({\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            \n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Euronews\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def franchdailynews():\n",
    "        try:\n",
    "            print(\"Franchdailynews\")\n",
    "            try:\n",
    "                urls = \"https://frenchdailynews.com/?s=ipo\"\n",
    "                def get_links(urls):\n",
    "                    links = [] \n",
    "                    page=requests.get(urls)\n",
    "                    print(page)\n",
    "                    soup=BeautifulSoup(page.content,'html')\n",
    "                    divs = soup.find_all('h2',{'class':'entry-title'})\n",
    "                    for i in divs:\n",
    "            #         print('hi')\n",
    "                        links.append(i.a[\"href\"])\n",
    "                    return links\n",
    "                title=[]\n",
    "                link=[]\n",
    "                pub_date=[]\n",
    "                scraped_date=[]\n",
    "                links1 = get_links(urls)\n",
    "                body = [] \n",
    "                link = []\n",
    "                date = []\n",
    "                from datetime import datetime\n",
    "                now = datetime.now()\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "            #       print(ur)\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(now)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "            except:\n",
    "                    print(\"franchdailynew not working\")\n",
    "                    not_working_functions.append(\"French Daily news\")\n",
    "            df2 = pd.DataFrame({\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"French daily news \" ,df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "   \n",
    "    def norway():\n",
    "        try:\n",
    "            print(\"Norway\")\n",
    "            try:\n",
    "                urls = \"https://www.thelocal.no/?s=ipo\"\n",
    "                def get_links(urls):\n",
    "\n",
    "                    links = [] \n",
    "                    page=requests.get(urls)\n",
    "            #       print(page)\n",
    "                    soup=BeautifulSoup(page.content,'html')\n",
    "                    divs = soup.find_all('div',{'class':'article-search-title'})\n",
    "                    for i in divs:\n",
    "                #         print('hi')\n",
    "                        links.append(i.a[\"href\"])\n",
    "                    return links\n",
    "                title=[]\n",
    "                link=[]\n",
    "                pub_date=[]\n",
    "                scraped_date=[]\n",
    "                links1 = get_links(urls)\n",
    "                body = [] \n",
    "                link = []\n",
    "                date = []\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "            #       print(ur)\n",
    "\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(current_time)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "            except:\n",
    "                print(\"Normay is not working\")\n",
    "                not_working_functions.append(\"Norway\")\n",
    "            df2 = pd.DataFrame({\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Norway\",df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    \n",
    "    def localde():\n",
    "        try:\n",
    "            print(\"localde\")\n",
    "            try:\n",
    "                urls = \"https://www.thelocal.de/?s=ipo\"\n",
    "                def get_links(urls):\n",
    "\n",
    "                    links = [] \n",
    "                    page=requests.get(urls)\n",
    "                #       print(page)\n",
    "                    soup=BeautifulSoup(page.content,'html')\n",
    "                    divs = soup.find_all('div',{'class':'article-search-title'})\n",
    "                    for i in divs:\n",
    "                #         print('hi')\n",
    "                        links.append(i.a[\"href\"])\n",
    "                    return links\n",
    "                title=[]\n",
    "                link=[]\n",
    "                pub_date=[]\n",
    "                scraped_date=[]\n",
    "                links1 = get_links(urls)\n",
    "                body = [] \n",
    "                link = []\n",
    "                date = []\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "            #       print(ur)\n",
    "\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(now)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "            except:\n",
    "                    print(\"localde not working\")\n",
    "                    not_working_functions.append(\"localde\")\n",
    "            df2 = pd.DataFrame({\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"localde\",df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def chinatoday():\n",
    "        try:\n",
    "            print(\"chinatoday\")\n",
    "            try:\n",
    "                urls = \"http://www.chinatoday.com/inv/a.htm\"\n",
    "                def get_links(urls):\n",
    "\n",
    "                    links = [] \n",
    "                    page=requests.get(urls)\n",
    "                    #   print(page)\n",
    "                    soup=BeautifulSoup(page.content,'html')\n",
    "                    divs = soup.find_all('li')\n",
    "                    for i in divs:\n",
    "                    #     print('hi')\n",
    "                        try:\n",
    "                            links.append(i.a[\"href\"])\n",
    "                        except:\n",
    "                            pass\n",
    "                    return links\n",
    "                title=[]\n",
    "                link=[]\n",
    "                pub_date=[]\n",
    "                scraped_date=[]\n",
    "                links1 = get_links(urls)\n",
    "                body = [] \n",
    "                link = []\n",
    "                date = []\n",
    "                from datetime import datetime\n",
    "\n",
    "                now = datetime.now()\n",
    "\n",
    "                current_time = now.strftime(\"%H:%M:%S\")\n",
    "                from newspaper import Article\n",
    "                for ur in links1:\n",
    "            #       print(ur)\n",
    "\n",
    "                    try:\n",
    "                        article = Article(ur)\n",
    "                        article.download()\n",
    "                        article.parse()\n",
    "                        # print(article.text)\n",
    "\n",
    "                        link.append(ur)\n",
    "                        scraped_date.append(now)\n",
    "                        body.append(article.text)\n",
    "                        title.append(article.title)\n",
    "\n",
    "                        date.append(article.publish_date)\n",
    "\n",
    "                    except:\n",
    "\n",
    "                        continue\n",
    "                    time.sleep(0.5)\n",
    "            except:\n",
    "                    print(\"chinatoday not working\")\n",
    "                    not_working_functions.append(\"Chinatoday\")\n",
    "            df2 = pd.DataFrame({\"text\":body,\"link\":link,\"publish_date\":date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            df = FilterFunction(df2)\n",
    "            emptydataframe(\"Chinatoday\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def Koreatimes():\n",
    "        try :\n",
    "            print(\"Koreatimes\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.koreatimes.co.kr/www2/common/search.asp?kwd=IPO\"\n",
    "            domain_url = \"https://www.kontan.co.id\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Koreatimes : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"list_article_headline HD\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Koreatimes : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                if (soup.find(\"div\" , {\"class\" : \"view_headline HD\"}) == None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"view_headline HD\"}).text)  \n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"date\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Title of article \n",
    "                \n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"view_article\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"span\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            \n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Koreatimes : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Koreatimes\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Koreatimes not working\")\n",
    "            not_working_functions.append('Koreatimes')\n",
    "\n",
    " \n",
    "        \n",
    "  \n",
    "  # TODO: commented out for testing.  Uncomment later\n",
    "  # CombineFunction()\n",
    "\n",
    "\n",
    "  \n",
    "    def zdnet():\n",
    "        try :\n",
    "            print(\"zdnet\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.zdnet.com/search/?q=IPO\"\n",
    "            domain_url = \"https://www.zdnet.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"zdnet : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"article\",{\"class\":\"item\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"zdnet : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"header\" , {\"class\" : \"storyHeader precap-variation article\"}).h1.text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"byline-details\"}).time.text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"storyBody\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"zdnet : err: Empty datframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"zdnet\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"zdnet not working\")\n",
    "            not_working_functions.append('zdnet')\n",
    "\n",
    "    def arabNews():\n",
    "        try:\n",
    "            print(\"Arabnews\")\n",
    "            err_logs = []\n",
    "            err_index = []\n",
    "            baseSearchUrl = \"https://www.arabnews.com/search/site/\"\n",
    "            domainUrl = \"https://www.arabnews.com\"\n",
    "            keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "            # use this for faster testing\n",
    "            tkeywords = [\"IPO\"]\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            for keyword in tkeywords:\n",
    "                queryUrl = baseSearchUrl + keyword\n",
    "                try:\n",
    "                    headers = {\n",
    "                        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                    pageSource = requests.get(queryUrl, headers=headers)\n",
    "                    parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                except:\n",
    "                    err = \"arabNews: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                # with open(\"response.html\", \"wb\") as f:\n",
    "                #     f.write(pageSource.content)\n",
    "                # break\n",
    "                for item in parsedSource.find_all(\"div\", class_=\"article-item-title\"):\n",
    "                    requiredTag = item.find(\"h4\").find(\"a\")\n",
    "                    currentArticleTitle = requiredTag.text\n",
    "                    # print(currentArticleTitle)\n",
    "                    currentArticleLink = requiredTag[\"href\"]\n",
    "                    # print(currentArticleLink)\n",
    "                    if currentArticleLink[0] == \"/\":\n",
    "                        links.append(domainUrl + currentArticleLink)\n",
    "                    else:\n",
    "                        links.append(currentArticleLink)\n",
    "                    titles.append(currentArticleTitle)\n",
    "\n",
    "                scrapedData[\"title\"] = titles\n",
    "                scrapedData[\"link\"] = links\n",
    "                # print(titles)\n",
    "                # print(links)\n",
    "                #\n",
    "                # Article's date and description scraping\n",
    "                ArticleDates = []\n",
    "                ScrapeDates = []\n",
    "                ArticleBody = []\n",
    "                for link in links:\n",
    "                    articleText = \"\"\n",
    "                    try:\n",
    "                        pageSource = requests.get(link, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                    except:\n",
    "                        err = \"arabNews: err: Failed to access article link : \" + link\n",
    "                        ArticleDates.append(\"Error\")\n",
    "                        err_logs.append(err)\n",
    "                        err_index.append(link)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "                    if parsedSource.find(\"div\", class_=\"entry-date\"):\n",
    "                        sourceDateTimeTag = parsedSource.find(\"div\", class_=\"entry-date\").find(\"time\")\n",
    "                    else:\n",
    "                        err = \"arabNews: err: Failed to retrieve date from article : \" + link\n",
    "                        ArticleDates.append(\"Error\")\n",
    "                        err_logs.append(err)\n",
    "                        err_index.append(link)\n",
    "                        continue\n",
    "                    sourceDateTime = datetime.strptime(sourceDateTimeTag.text, \"%B %d, %Y %H:%M\").strftime(\"%d-%m-%Y\")\n",
    "                    # print(sourceDateTime)\n",
    "                    ArticleDates.append(sourceDateTime)\n",
    "                    ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    # print(ArticleDates)\n",
    "                    if parsedSource.find(\"div\", class_=\"entry-content\"):\n",
    "                        textBodyDiv = parsedSource.find(\"div\", class_=\"entry-content\")\n",
    "                    else:\n",
    "                        err = \"arabNews: err: Failed to retrieve article text: \" + link\n",
    "                        ArticleBody.append(\"Error\")\n",
    "                        err_logs.append(err)\n",
    "                        err_index.append(link)\n",
    "                        continue\n",
    "                    for item in textBodyDiv.find_all(\"p\"):\n",
    "                        articleText += item.text.strip()\n",
    "                    ArticleBody.append(articleText)\n",
    "                scrapedData[\"publish_date\"] = ArticleDates\n",
    "                scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                scrapedData[\"text\"] = ArticleBody\n",
    "                # print(ArticleBody)\n",
    "\n",
    "            # Clean and Normalize links\n",
    "            if len(err_index) != 0:\n",
    "                for e in err_index:\n",
    "                    idx = scrapedData[\"link\"].index(e)\n",
    "                    scrapedData[\"link\"].pop(idx)\n",
    "                    scrapedData[\"title\"].pop(idx)\n",
    "                    scrapedData[\"publish_date\"].pop(idx)\n",
    "\n",
    "            # DataFrame creation\n",
    "            arabNewsDF = pd.DataFrame(scrapedData)\n",
    "            arabNewsDF = arabNewsDF.drop_duplicates(subset=[\"link\"])\n",
    "            if arabNewsDF.empty:\n",
    "                err = \"arabNews: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(arabNewsDF)\n",
    "            emptydataframe(\"Arabnews\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Arabnews not working\")\n",
    "            not_working_functions.append(\"Arabnews\")\n",
    "    def chosun():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Chosun\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://english.chosun.com/svc/list_in/search.html?query=\"\n",
    "                domainUrl = \"https://english.chosun.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                        pageSource = requests.get(queryUrl, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                    except:\n",
    "                        err = \"chosun: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "                    for item in parsedSource.find_all(\"dl\", class_=\"list_item\"):\n",
    "                        requiredTag = item.find(\"a\")\n",
    "                        currentArticleTitle = requiredTag.text\n",
    "                        currentArticleLink = requiredTag[\"href\"]\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                # print(titles)\n",
    "                # print(links)\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleDates = []\n",
    "                    ScrapeDates = []\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                        except:\n",
    "                            err = \"chosun: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        # with open(\"response.html\", \"wb\") as f:\n",
    "                        #     f.write(pageSource.content)\n",
    "                        # break\n",
    "                        try:\n",
    "                            sourceDateTimeTag = parsedSource.find(\"p\", id=\"date_text\")\n",
    "                        except:\n",
    "                            err = \"chosun: err: Failed to retrieve date from article : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        sourceDateTime = sourceDateTimeTag.text.strip()\n",
    "                        sourceDateTime = datetime.strptime(sourceDateTime, \"%B %d, %Y %H:%M\").strftime(\"%d-%m-%Y\")\n",
    "                        ArticleDates.append(sourceDateTime)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    # print(ArticleDates)\n",
    "                        try:\n",
    "                            textBodyDiv = parsedSource.find(\"div\", class_=\"par\")\n",
    "                        except:\n",
    "                            err = \"chosun: err: Failed to retrieve article text: \" + link\n",
    "                            ArticleBody.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        for item in textBodyDiv.find_all(\"p\"):\n",
    "                            articleText = textBodyDiv.text.replace(item.text, \"\")\n",
    "                        # print(articleText)\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "                    # print(ArticleBody)\n",
    "\n",
    "                # Clean data\n",
    "                if len(err_index) != 0:\n",
    "                    for index in err_index:\n",
    "                        del scrapedData[\"title\"][index]\n",
    "                        del scrapedData[\"link\"][index]\n",
    "                        if scrapedData[\"publish_date\"][index] == \"Error\":\n",
    "                            del scrapedData[\"publish_date\"][index]\n",
    "                        if scrapedData[\"article\"][index] == \"Error\":\n",
    "                            del scrapedData[\"article\"][index]\n",
    "\n",
    "                # DataFrame creation\n",
    "                chosunDF = pd.DataFrame(scrapedData)\n",
    "                if chosunDF.empty:\n",
    "                    err = \"Chosun news : err : Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(chosunDF)\n",
    "                emptydataframe(\"Chosun\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Chosun not working\")\n",
    "                not_working_functions.append(\"Chosun\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def Forbes():\n",
    "        try :\n",
    "            print(\"Forbes\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.forbes.com/search/?q=IPO&sh=8e278c4279f4\"\n",
    "            domain_url = \"https://www.forbes.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Forbes : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"stream-item__title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Forbes : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "\n",
    "                if(soup.find(\"div\" , {\"class\" :\"metrics-channel light-text with-border\"}) == None):\n",
    "                    continue\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"metrics-channel light-text with-border\"}).time.text)\n",
    "            \n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"fs-headline speakable-headline font-base font-size should-redesign\"}))  \n",
    "                # print(title)\n",
    "            \n",
    "            \n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"article-body fs-article fs-responsive-text current-article\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            # print(len(text) , len(final_links))\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Forbes : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Forbes\",df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Forbes not working\")\n",
    "            not_working_functions.append('Forbes')\n",
    "    def fpj():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"FPJ\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.freepressjournal.in/search?q=\"\n",
    "                domainUrl = \"https://www.freepressjournal.in\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                        pageSource = requests.get(queryUrl, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                    except:\n",
    "                        err = \"fpj: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "\n",
    "                    fpjBigNewsDiv = parsedSource.find(\"div\", class_=\"fpj_bignews\")\n",
    "                    currentArticleTitle = fpjBigNewsDiv.find(\"h3\").text\n",
    "                    currentArticleLink = fpjBigNewsDiv.find(\"a\")[\"href\"]\n",
    "                    if currentArticleLink[0] == \"/\":\n",
    "                        links.append(domainUrl + currentArticleLink)\n",
    "                    else:\n",
    "                        links.append(currentArticleLink)\n",
    "                    titles.append(currentArticleTitle)\n",
    "\n",
    "                    fpjNewListDiv = parsedSource.find(\"div\", class_=\"fpj_newList\")\n",
    "                    for item in fpjNewListDiv.find_all(\"li\"):\n",
    "                        currentArticleTitle = item.find(\"span\", class_=\"fpj_title\").text\n",
    "                        currentArticleLink = item.find(\"a\")[\"href\"]\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "\n",
    "                    fpjLshDiv = parsedSource.find(\"div\", class_=\"fpj_lsh\")\n",
    "                    for item in fpjLshDiv.find_all(\"li\"):\n",
    "                        currentArticleTitle = item.find(\"h3\").text\n",
    "                        currentArticleLink = item.find(\"a\")[\"href\"]\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleDates = []\n",
    "                    ScrapeDates = []\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                        except:\n",
    "                            err = \"fpj: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        # with open(\"response.html\", \"wb\") as f:\n",
    "                        #     f.write(pageSource.content)\n",
    "                        # break\n",
    "                        try:\n",
    "                            sourceDateTimeTag = parsedSource.find(\"span\", class_=\"publishTime\")\n",
    "                        except:\n",
    "                            err = \"fpj: err: Failed to retrieve date from article : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        sourceDateTime = sourceDateTimeTag.text.split(\":\", maxsplit=1)[1].split(\",\", maxsplit=1)[1].replace(\"IST\", \"\").strip()\n",
    "                        sourceDateTime = datetime.strptime(sourceDateTime, \"%B %d, %Y, %I:%M %p\").strftime(\"%d-%m-%Y\")\n",
    "                        ArticleDates.append(sourceDateTime)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    # print(ArticleDates)\n",
    "                        try:\n",
    "                            textBodyDiv = parsedSource.find(\"article\", class_=\"storyMain\")\n",
    "                        except:\n",
    "                            err = \"fpj: err: Failed to retrieve article text: \" + link\n",
    "                            ArticleBody.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        for item in textBodyDiv.find_all(\"p\"):\n",
    "                            articleText += item.text.strip()\n",
    "                        # print(articleText)\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "                    # print(ArticleBody)\n",
    "\n",
    "                # Clean data\n",
    "                if len(err_index) != 0:\n",
    "                    for e in err_index:\n",
    "                        del scrapedData[\"title\"][index]\n",
    "                        del scrapedData[\"link\"][index]\n",
    "                        if scrapedData[\"publish_date\"][index] == \"Error\":\n",
    "                            del scrapedData[\"publish_date\"][index]\n",
    "                        if scrapedData[\"article\"][index] == \"Error\":\n",
    "                            del scrapedData[\"article\"][index]\n",
    "\n",
    "                # DataFrame creation\n",
    "                fpjDF = pd.DataFrame(scrapedData)\n",
    "                if fpjDF.empty:\n",
    "                    err = \"FPJ news : err : Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(fpjDF)\n",
    "                emptydataframe(\"FPJ\",df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"FPJ not working\")\n",
    "                not_working_functions.append(\"FPJ\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def kedg():\n",
    "        try :\n",
    "            print(\"kedg\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.kedglobal.com/newsSearch?keyword=IPO\"\n",
    "            domain_url = \"https://www.kedglobal.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"kedg : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"box\"})\n",
    "            for div in all_divs:\n",
    "                try :\n",
    "                    links.append(domain_url+div.a[\"href\"])\n",
    "                except : \n",
    "                    continue\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"kedg : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                try:\n",
    "                    if not (soup.find(\"p\" , {\"class\" :\"update_time\"})):\n",
    "                        continue\n",
    "                    if not (soup.find(\"h1\" , {\"class\" : \"tit\"})):\n",
    "                        continue\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                \n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                # div = soup.find(\"div\" , {\"class\" : \"par\"})\n",
    "                # t = \"\"\n",
    "                # for i in div.find_all(\"p\"):\n",
    "                #   t += i.text + \" \"\n",
    "                # text.append(t)\n",
    "                    if not (soup.find(\"div\" , {\"class\" : \"cont\"})):\n",
    "                        continue\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "\n",
    "                #Working links\n",
    "                except :\n",
    "                    continue \n",
    "                pub_date.append(soup.find(\"p\" , {\"class\" :\"update_time\"}).text)\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"tit\"}).text)\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"cont\"}).text)\n",
    "                scraped_date.append(str(today))\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"kedg : err: Empty datframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"kedg\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"kedg not working\")\n",
    "            not_working_functions.append('kedg')\n",
    "    def kh():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Kh\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"http://www.koreaherald.com/search/index.php?kr=0&q=\"\n",
    "                domainUrl = \"http://www.koreaherald.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                        pageSource = requests.get(queryUrl, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\")\n",
    "                    except:\n",
    "                        err = \"kh: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "                    for item in parsedSource.find(\"ul\", class_=\"main_sec_li\").find_all(\"li\"):\n",
    "                        requiredTag = item.find(\"a\")\n",
    "                        currentArticleTitle = requiredTag.find(\"div\", class_=\"main_l_t1\").text\n",
    "                        currentArticleLink = requiredTag[\"href\"]\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                # print(titles)\n",
    "                # print(links)\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleDates = []\n",
    "                    ScrapeDates = []\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\")\n",
    "                        except:\n",
    "                            err = \"kh: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        # with open(\"response.html\", \"wb\") as f:\n",
    "                        #     f.write(pageSource.content)\n",
    "                        # break\n",
    "                        try:\n",
    "                            sourceDateTime = parsedSource.find(\"div\", class_=\"view_tit_byline_r\").text\n",
    "                        except:\n",
    "                            err = \"kh: err: Failed to retrieve date from article : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        sourceDateTime = sourceDateTime[sourceDateTime.find(\"Updated\"):].split(\":\", maxsplit=1)[1].strip()\n",
    "                        sourceDateTime = datetime.strptime(sourceDateTime, \"%b %d, %Y - %H:%M\").strftime(\"%d-%m-%Y\")\n",
    "                        ArticleDates.append(sourceDateTime)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    # print(ArticleDates)\n",
    "                        try:\n",
    "                            textBodyDiv = parsedSource.find(\"div\", id=\"articleText\").find_all(\"div\", class_=\"view_con_t\")\n",
    "                        except:\n",
    "                            err = \"kh: err: Failed to retrieve article text: \" + link\n",
    "                            ArticleBody.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        if len(textBodyDiv[0].text) != 0:\n",
    "                            articleText = textBodyDiv[0].text\n",
    "                        else:\n",
    "                            articleText = textBodyDiv[1].text\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "                    print(ArticleBody)\n",
    "\n",
    "                # Clean data\n",
    "                if len(err_index) != 0:\n",
    "                    for index in err_index:\n",
    "                        del scrapedData[\"title\"][index]\n",
    "                        del scrapedData[\"link\"][index]\n",
    "                        if scrapedData[\"publish_date\"][index] == \"Error\":\n",
    "                            del scrapedData[\"publish_date\"][index]\n",
    "                        if scrapedData[\"article\"][index] == \"Error\":\n",
    "                            del scrapedData[\"article\"][index]\n",
    "\n",
    "                # DataFrame creation\n",
    "                khDF = pd.DataFrame(scrapedData)\n",
    "                if khDF.empty:\n",
    "                    err = \"KH news : err : Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(khDF)\n",
    "                emptydataframe(\"Kh\",df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Kh is not working\")\n",
    "                not_working_functions.append(\"Kh\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def kngnet():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"kngnet\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.koreanewsgazette.com/?s=\"\n",
    "                domainUrl = \"https://www.koreanewsgazette.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        pageSource = requests.get(queryUrl)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\")\n",
    "                    except:\n",
    "                        err = \"kng: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    for item in parsedSource.find_all(\"h2\", class_=\"entry-title\"):\n",
    "                        # requiredTag = item.find(\"h2\")\n",
    "                        currentArticleLink = item.a[\"href\"]\n",
    "                        currentArticleTitle = item.text\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                    print(links)\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleDates = []\n",
    "                    ScrapeDates = []\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                      page = Article(link)\n",
    "                      page.download()\n",
    "                      page.parse()\n",
    "                      ArticleDates.append(page.publish_date)\n",
    "                      ArticleBody.append(page.text)\n",
    "                      ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "\n",
    "                # Clean data\n",
    "                if len(err_index) != 0:\n",
    "                    for index in err_index:\n",
    "                        del scrapedData[\"title\"][index]\n",
    "                        del scrapedData[\"link\"][index]\n",
    "                        if scrapedData[\"publish_date\"][index] == \"Error\":\n",
    "                            del scrapedData[\"publish_date\"][index]\n",
    "                        if scrapedData[\"article\"][index] == \"Error\":\n",
    "                            del scrapedData[\"article\"][index]\n",
    "\n",
    "                # DataFrame creation\n",
    "                kngDF = pd.DataFrame(scrapedData)\n",
    "                if kngDF.empty:\n",
    "                    err = \"Kngnet news : err : Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(kngDF)\n",
    "                emptydataframe(\"Kngnet news\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Kngnet news is not working\")\n",
    "                not_working_functions.append(\"Kngnet news\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def pymnts():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Pymnts\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.pymnts.com/?s=\"\n",
    "                domainUrl = \"https://www.pymnts.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                        pageSource = requests.get(queryUrl, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\")\n",
    "                    except:\n",
    "                        err = \"pymnts: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    for item in parsedSource.find_all(\"li\", class_=\"infinite-post\"):\n",
    "                        requiredTag = item.find(\"a\")\n",
    "                        currentArticleTitle = requiredTag[\"title\"]\n",
    "                        currentArticleLink = requiredTag[\"href\"]\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleDates = []\n",
    "                    ScrapeDates = []\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\")\n",
    "                        except:\n",
    "                            err = \"pymnts: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        try:\n",
    "                            sourceDateTime = parsedSource.find(\"time\", class_=\"post-date updated\")\n",
    "                        except:\n",
    "                            err = \"pymnts: err: Failed to retrieve date from article : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        sourceDateTime = datetime.strptime(sourceDateTime[\"datetime\"], \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "                        ArticleDates.append(sourceDateTime)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                        try:\n",
    "                            textBodyDiv = parsedSource.find(\"div\", id=re.compile(\"pymnts-content\"))\n",
    "                        except:\n",
    "                            err = \"pymnts: err: Failed to retrieve article text: \" + link\n",
    "                            ArticleBody.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        for item in textBodyDiv.find_all(\"p\"):\n",
    "                            articleText += item.text\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "\n",
    "                # Clean data\n",
    "                if len(err_index) != 0:\n",
    "                    for index in err_index:\n",
    "                        del scrapedData[\"title\"][index]\n",
    "                        del scrapedData[\"link\"][index]\n",
    "                        if scrapedData[\"publish_date\"][index] == \"Error\":\n",
    "                            del scrapedData[\"publish_date\"][index]\n",
    "                        if scrapedData[\"article\"][index] == \"Error\":\n",
    "                            del scrapedData[\"article\"][index]\n",
    "\n",
    "                # DataFrame creation\n",
    "                pymntsDF = pd.DataFrame(scrapedData)\n",
    "                df = FilterFunction(pymntsDF)\n",
    "                emptydataframe(\"Pymnts\",df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Pymnts not working\")\n",
    "                not_working_functions.append(\"Pymnts\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def toi():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Toi\")\n",
    "                err_logs = []  # Access to view error logs\n",
    "                url = \"https://timesofindia.indiatimes.com/topic/pre-ipo/news\"\n",
    "                domain_url = \"https://timesofindia.indiatimes.com\"\n",
    "\n",
    "                try:\n",
    "                    page = requests.get(url)\n",
    "                    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                except:\n",
    "                    err = \"toi: err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    return None\n",
    "\n",
    "                # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "                div_class = \"Mc7GB\"\n",
    "                h1_class = \"_1Y-96\"\n",
    "                date_div_class = \"yYIu- byline\"\n",
    "                para_div_class = \"_3YYSt\"\n",
    "\n",
    "                links = []\n",
    "\n",
    "                for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                    for a in divtag.find_all(\"a\", href=True):\n",
    "                        link = a[\"href\"]  # Gets the link\n",
    "\n",
    "                    # Checking the link if it is a relative link\n",
    "                        if link[0] == '/':\n",
    "                                link = domain_url + link\n",
    "\n",
    "                        links.append(link)\n",
    "\n",
    "                collection = []\n",
    "\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        l_page = requests.get(link)\n",
    "                        l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                    except:\n",
    "                        err = \"toi: err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    data = []\n",
    "                    # Scraping the heading\n",
    "                    h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                    try:\n",
    "                        data.append(h1_ele.text)\n",
    "                    except:\n",
    "                        err = \"toi: err: Failed to find title in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding the link to data\n",
    "                    data.append(link)\n",
    "\n",
    "                    # Scraping the published date\n",
    "                    date_ele = l_soup.find(\"div\", {\"class\": date_div_class})\n",
    "                    try:\n",
    "                        date_text = date_ele.text\n",
    "                        date_text = (date_text.split('/'))[-1]\n",
    "                        date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                        data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "                    except:\n",
    "                        err = \"toi: err: Failed to find date in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding the scraped date to data\n",
    "                    cur_date = str(datetime.today())\n",
    "                    data.append(cur_date)\n",
    "\n",
    "                    # Scraping the paragraph\n",
    "                    para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "                    try:\n",
    "                        data.append(para_ele.text)  # Need to make this better\n",
    "                    except:\n",
    "                        err = \"toi: err: Failed to find paragraph in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding data to the collection\n",
    "                    collection.append(data)\n",
    "\n",
    "                df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "                if df.empty:\n",
    "                        err = \"Arab news : err : Empty dataframe\"\n",
    "                        err_logs.append(err)\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(df)\n",
    "                emptydataframe(\"Toi\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Toi not working \")\n",
    "                not_working_functions.append(\"Toi\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def wealthx():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Wealthx\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.wealthx.com/?s=\"\n",
    "                domainUrl = \"https://www.wealthx.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                        pageSource = requests.get(queryUrl, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                    except:\n",
    "                        err = \"wealthx: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "                    for item in parsedSource.find_all(\"article\", class_=\"result\"):\n",
    "                        requiredTag = item.find(\"h2\", class_=\"title\").find(\"a\")\n",
    "                        currentArticleTitle = requiredTag.text\n",
    "                        currentArticleLink = requiredTag[\"href\"]\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                # print(titles)\n",
    "                # print(links)\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleDates = []\n",
    "                    ScrapeDates = []\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                        except:\n",
    "                            err = \"wealthx: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        # with open(\"response.html\", \"wb\") as f:\n",
    "                        #     f.write(pageSource.content)\n",
    "                        # break\n",
    "                        try:\n",
    "                            sourceDateTimeTag = parsedSource.find(\"span\", class_=\"meta-date date updated\")\n",
    "                        except:\n",
    "                            err = \"wealthx: err: Failed to retrieve date from article : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        sourceDateTime = datetime.strptime(sourceDateTimeTag.text, \"%B %d, %Y\").strftime(\"%d-%m-%Y\")\n",
    "                        ArticleDates.append(sourceDateTime)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    # print(ArticleDates)\n",
    "                        try:\n",
    "                            textBodyDiv = parsedSource.find(\"div\", class_=\"content-inner\")\n",
    "                        except:\n",
    "                            err = \"wealthx: err: Failed to retrieve article text: \" + link\n",
    "                            ArticleBody.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        for item in textBodyDiv.find_all(\"p\"):\n",
    "                            articleText += item.text.strip()\n",
    "                        # print(articleText)\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "                    # print(ArticleBody)\n",
    "\n",
    "                # Clean data\n",
    "                if len(err_index) != 0:\n",
    "                    for index in err_index:\n",
    "                        del scrapedData[\"title\"][index]\n",
    "                        del scrapedData[\"link\"][index]\n",
    "                        if scrapedData[\"publish_date\"][index] == \"Error\":\n",
    "                            del scrapedData[\"publish_date\"][index]\n",
    "                        if scrapedData[\"article\"][index] == \"Error\":\n",
    "                            del scrapedData[\"article\"][index]\n",
    "\n",
    "                # DataFrame creation\n",
    "                wealthxDF = pd.DataFrame(scrapedData)\n",
    "                if wealthxDF.empty:\n",
    "                    err = \"Wealthx news : err : Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(wealthxDF)\n",
    "                emptydataframe(\"WealthX\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Wealth x not working\")\n",
    "                not_working_functions.append(\"Wealth x\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def AFR():\n",
    "        try :\n",
    "            print(\"AFR\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.afr.com/search?text=ipo\"\n",
    "            domain_url = \"https://www.afr.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"AFR : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"_20-Rx\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"AFR : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                # if(soup.find(\"h1\" , {\"class\" : \"primary-font__PrimaryFontStyles-o56yd5-0 dEdODy headline\"}) == None):\n",
    "                #   continue\n",
    "                if(soup.find(\"h1\" , {\"class\" : \"_3lFzE\"})== None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"_3lFzE\"}))\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"_2cdD4\"}).time.text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"tl7wu\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"AFR : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            # df = FilterFunction(df)\n",
    "            # emptydataframe(\"AFR\",df)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"AFR not working\")\n",
    "            not_working_functions.append('AFR')\n",
    "    def indonesia():\n",
    "\n",
    "        try :\n",
    "            print(\"Antara\")\n",
    "            err_logs = []\n",
    "            url = \"https://en.antaranews.com/search?q=ipo\"\n",
    "            domain_url = \"https://en.antaranews.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Indonesia : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"article\",{\"class\":\"simple-post simple-big clearfix\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Indonesia : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "\n",
    "                # if(soup.find(\"div\" , {\"class\" :\"metrics-channel light-text with-border\"}) == None):\n",
    "                #   continue\n",
    "                pub_date.append(soup.find(\"i\" , {\"class\" :\"fa fa-clock-o\"}).text)\n",
    "            \n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"post-title\"}).text)  \n",
    "                # print(title)\n",
    "            \n",
    "            \n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"post-content clearfix\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"div\"):\n",
    "                # if(i != None):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                \n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            # print(len(text) , len(final_links))\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Indonesia : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"indonesia\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Indonesia not working\")\n",
    "            not_working_functions.append('Indonesia')\n",
    "            \n",
    "    def asiainsurancereview():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Asia Insurance\")\n",
    "                err_logs = []  # Access to view error logs\n",
    "                url = \"https://www.asiainsurancereview.com/Search?search_key=IPO\"\n",
    "                domain_url = \"https://www.asiainsurancereview.com\"\n",
    "\n",
    "                try:\n",
    "                    page = requests.get(url)\n",
    "                    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                except:\n",
    "                    err = \"asiainsurancereview: err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    return None\n",
    "\n",
    "                # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "                div_class = \"items\"\n",
    "                h1_class = \"main-title\"\n",
    "                date_div_class = \"title-right\"\n",
    "                para_div_class = \"article-wrap\"\n",
    "\n",
    "                links = []\n",
    "\n",
    "                for divtag in soup.find_all(\"li\", {\"class\": div_class}):\n",
    "                    for a in divtag.find_all(\"a\", href=True):\n",
    "                        link = a[\"href\"]  # Gets the link\n",
    "\n",
    "                        # Checking the link if it is a relative link\n",
    "                        if link[0] == '/':\n",
    "                            link = domain_url + link\n",
    "\n",
    "                        links.append(link)\n",
    "\n",
    "                collection = []\n",
    "\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        l_page = requests.get(link)\n",
    "                        l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                    except:\n",
    "                        err = \"asiainsurancereview: err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    data = []\n",
    "                    # Scraping the heading\n",
    "                    h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                    try:\n",
    "                        data.append(h1_ele.text)\n",
    "                    except:\n",
    "                        err = \"asiainsurancereview: err: Failed to find title in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding the link to data\n",
    "                    data.append(link)\n",
    "\n",
    "                    # Scraping the published date\n",
    "                    date_ele = l_soup.find(\"span\", {\"class\": date_div_class})\n",
    "                    try:\n",
    "                        date_text = date_ele.text\n",
    "                        date_text = (date_text.split('/'))[-1]\n",
    "                        date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                        data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "                    except:\n",
    "                        err = \"asiainsurancereview: err: Failed to find date in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding the scraped date to data\n",
    "                    cur_date = str(datetime.today())\n",
    "                    data.append(cur_date)\n",
    "\n",
    "                    # Scraping the paragraph\n",
    "                    para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "                    try:\n",
    "                        data.append(para_ele.text)  # Need to make this better\n",
    "                    except:\n",
    "                        err = \"asiainsurancereview: err: Failed to find paragraph in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "                    # Adding data to the collection\n",
    "                    collection.append(data)\n",
    "\n",
    "                df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "                if df.empty:\n",
    "                    print(\"Antara indonesia review : err : Empty dataframe\")\n",
    "                #     err_logs.append(err)\n",
    "                # log_errors(err_logs)\n",
    "                df = FilterFunction(df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Asia insurance not working\")\n",
    "                not_working_functions.append(\"Asia Insurance\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def economic_times():\n",
    "        try :\n",
    "            print(\"economic_times\")\n",
    "            err_logs = []\n",
    "            url = \"https://economictimes.indiatimes.com/markets/ipo\"\n",
    "            domain_url = \"https://economictimes.indiatimes.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"economic_times : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"wrapLines l1\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"economic_times : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"artTitle font_faus\"}).text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\" , {\"class\" :\"jsdtTime\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                # div = soup.find(\"div\" , {\"class\" : \"storyBody\"})\n",
    "                # t = \"\"\n",
    "                # for i in div.find_all(\"p\"):\n",
    "                #   t += i.text + \" \"\n",
    "                # text.append(t)\n",
    "                text.append(soup.find(\"div\" , {\"class\" :\"pageContent flt\"}).text)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"economic_times : err: Empty datframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"economic_times\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"economic_times not working\")\n",
    "            not_working_functions.append('economic_times')\n",
    "    def prnewswire():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"prnnewswire\")\n",
    "                err_logs = []\n",
    "                url = \"https://www.prnewswire.com/search/news/?keyword=pre%20ipo\"\n",
    "                domain_url = \"https://www.prnewswire.com/\"\n",
    "\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "                # Class names of the elements to be scraped\n",
    "                div_class = \"card\"\n",
    "                #h1_class = \"_1Y-96\"\n",
    "                h1_div_class = \"col-xs-12\"\n",
    "                date_div_class = \"mb-no\"\n",
    "                para_div_class = \"col-sm-10\"\n",
    "\n",
    "                links = []\n",
    "\n",
    "                for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                    for a in divtag.find_all(\"a\", href=True):\n",
    "                        link = a[\"href\"]  # Gets the link\n",
    "\n",
    "                        # Checking the link if it is a relative link\n",
    "                        if link[0] == '/':\n",
    "                            link = domain_url + link\n",
    "\n",
    "                        link_start = \"https://www.prnewswire.com//news-releases\"\n",
    "                        if link.startswith(link_start):\n",
    "                            links.append(link)\n",
    "                # Remove duplicates\n",
    "                links = list(set(links))\n",
    "\n",
    "                collection = []\n",
    "\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        l_page = requests.get(link)\n",
    "                        l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                    except:\n",
    "                        err = \"prnewswire: err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                    try:\n",
    "                        div_ele = l_soup.find(\"div\", {\"class\": h1_div_class})\n",
    "                        h1_ele = div_ele.find(\"h1\")\n",
    "                        data.append(h1_ele.text)\n",
    "                    except:\n",
    "                        err = \"prnewswire: err: Failed to find title in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                    data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                    date_ele = l_soup.find(\"p\", {\"class\": date_div_class})\n",
    "                    try:\n",
    "                        date_text = date_ele.text\n",
    "                        date_text = (date_text.split('/'))[-1]\n",
    "                        date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                        data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                    except:\n",
    "                        err = \"prnewswire: err: Failed to find date in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                    cur_date = str(datetime.today())\n",
    "                    data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                    para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                    try:\n",
    "                        data.append(para_ele.text)  # Need to make this better\n",
    "                    except:\n",
    "                        err = \"prnewswire: err: Failed to find paragraph in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                    collection.append(data)\n",
    "\n",
    "                df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "                if df.empty:\n",
    "                    err = \"prnewswire: err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = FilterFunction(df)\n",
    "                emptydataframe(\"Prnewswire\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Prnewswire not working\")\n",
    "                not_working_functions.append(\"Prnewswire\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def arabfinance():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Arab Finanace\")\n",
    "                err_logs = []\n",
    "                url = \"https://www.albawaba.com/search?keyword=IPO&sort_by=created\"\n",
    "                domain_url = \"https://www.albawaba.com\"\n",
    "                title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "                try:\n",
    "                    page = requests.get(url)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                except:\n",
    "                    err = \"Arab Finance : err : Couldn't fetch \" + url \n",
    "                    err_logs.append(err)\n",
    "                    return \n",
    "                # Fetching all links \n",
    "                all_divs = soup.find_all(\"div\",{\"class\":\"field-content title\"})\n",
    "                for div in all_divs:\n",
    "                    links.append(domain_url + div.a[\"href\"])\n",
    "                #Fetch all the necessary data \n",
    "                final_links = []\n",
    "                today = date.today()\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    except:\n",
    "                        err = \"Arab Finance : err : Couldn't fetch url \" + link \n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    #get date of the article \n",
    "                    pub_date.append(\"Date\")\n",
    "                    #get title of the article \n",
    "                    title.append(soup.find(\"h1\",{\"class\":\"page-header\"}).text)\n",
    "                    #get text of the article \n",
    "                    # print(soup.find(\"div\",{\"class\":\"field field--name-body field--type-text-with-summary field--label-hidden field--item\"}).text)\n",
    "                    text.append(soup.find(\"div\",{\"class\":\"field field--name-body field--type-text-with-summary field--label-hidden field--item\"}).text)\n",
    "                    #get today's date i.r scrape date \n",
    "                    scraped_date.append(str(today))\n",
    "                    # get links that works \n",
    "                    final_links.append(link)\n",
    "                df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "                if df.empty:\n",
    "                    err = \"Arab Finance : err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = df.drop_duplicates(subset=[\"link\"])\n",
    "                log_errors(err_logs)\n",
    "                df = FilterFunction(df)\n",
    "                emptydataframe(\"Arab finance\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"Arab Finanace is not working\")\n",
    "                not_working_functions.append(\"Arab Finance\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    def star():\n",
    "        try:\n",
    "            print(\"Star\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.thestar.com.my/news/latest?tag=Business\"\n",
    "            domain_url = \"https://mb.com.ph\"\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "    #             print(page.content)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "            except:\n",
    "                err = \"The star : err : Couldn't fetch the url \"+ url\n",
    "                err_logs.append(err)\n",
    "                return\n",
    "            pub_date,scraped_date,links,final_links,title,text = [],[],[],[],[],[]\n",
    "            #get all links \n",
    "            for h4 in soup.find_all(\"h2\",{\"class\":\"f18\"}):\n",
    "                links.append(h4.a[\"href\"])\n",
    "            # Fetch all the required data for the dataframe \n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                except:\n",
    "                    err = \"The star : err : Couldn't fetch the url \"+ link\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                # Fetch all other data \n",
    "                title.append(soup.find(\"div\",{\"class\":\"headline story-pg\"}).h1.text)\n",
    "                pub_date.append(soup.find(\"p\",{\"class\":\"date\"}).text)\n",
    "                text.append(soup.find(\"div\",{\"class\":\"story bot-15 relative\"}).text)\n",
    "                scraped_date.append(today)\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"the star: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            log_errors(err_logs)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Star\",df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Star is not working\")\n",
    "            not_working_functions.append(\"Star\")\n",
    "    \n",
    "    def interfax():\n",
    "        try:\n",
    "            err_logs = []  # Access to view error logs\n",
    "            url = \"https://en.interfax.com.ua/news/search.html?q=ipo\"\n",
    "            domain_url = \"https://en.interfax.com.ua/\"\n",
    "\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            except:\n",
    "                err = \"toi: err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                return None\n",
    "\n",
    "            # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "            div_class = \"col-57\"\n",
    "            h1_class = \"article-content-title\"\n",
    "            date_div_class = \"col-18 article-time\"\n",
    "            para_div_class = \"article-content\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "\n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "\n",
    "                    links.append(link)\n",
    "\n",
    "            collection = []\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = \"err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                try:\n",
    "                    data.append(h1_ele.text)\n",
    "                except:\n",
    "                    err = \"err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"div\", {\"class\": date_div_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    date_text = (date_text.split('/'))[-1]\n",
    "                    date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "                except:\n",
    "                    err = \"err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = \"toi: err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding data to the collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = \"err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Interfax Ukraine\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Interfax_Ukraine not working\")\n",
    "            not_working_functions.append(\"Interfax_ukraine\")\n",
    "    def vccircle():\n",
    "        try :\n",
    "            print(\"Vccircle\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.vccircle.com/search/result/ipo/all\"\n",
    "            domain_url = \"https://www.vccircle.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Vccircle : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Vccircle : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"publish-time\"})[\"content\"])\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"premium-txt-container\"}).text)  \n",
    "                # print(title)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"col-sm-9 mid-content\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Vccircle : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Vccircle\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Vccircle not working\")\n",
    "            not_working_functions.append('Vccircle')\n",
    "\n",
    "    def allafrica():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Allafrica\")\n",
    "                err_logs = []\n",
    "                url = \"https://allafrica.com/search/?search_string=pre+ipo&search_submit=Search\"\n",
    "                domain_url = \"https://allafrica.com/\"\n",
    "\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "                # Class names of the elements to be scraped\n",
    "                div_class = \"search-results\"  # Class name of div containing the a tag\n",
    "                #h1_class = \"_1Y-96\"\n",
    "                #h1_div_class = \"col-xs-12\"\n",
    "                h2_class = \"headline\"\n",
    "                date_div_class = \"publication-date\"\n",
    "                para_div_class = \"story-body\"\n",
    "\n",
    "                links = []\n",
    "\n",
    "                for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                    for a in divtag.find_all(\"a\", href=True):\n",
    "                        link = a[\"href\"]  # Gets the link\n",
    "                        \n",
    "                        # Checking the link if it is a relative link\n",
    "                        if link[0] == '/':\n",
    "                            link = domain_url + link\n",
    "                        \n",
    "                        # Filtering advertaisment links\n",
    "                        link_start = domain_url \n",
    "                        if link.startswith(link_start):\n",
    "                            links.append(link)\n",
    "                # Remove duplicates\n",
    "                links = list(set(links))\n",
    "                #links # Debugging - if link array is generated\n",
    "\n",
    "                collection = []\n",
    "                scrapper_name = \"allafrica\"\n",
    "\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        l_page = requests.get(link)\n",
    "                        l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                    except:\n",
    "                        err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    data = []\n",
    "                    # Scraping the heading\n",
    "                    #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                    \n",
    "                    try:\n",
    "                        h2_ele = l_soup.find(\"h2\", {\"class\": h2_class})\n",
    "                        data.append(h2_ele.text)\n",
    "                    except:\n",
    "                        err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                    # Adding the link to data\n",
    "                    data.append(link)\n",
    "\n",
    "                    # Scraping the published date\n",
    "                    date_ele = l_soup.find(\"div\", {\"class\": date_div_class})\n",
    "                    try:\n",
    "                        date_text = date_ele.text\n",
    "                        #date_text = (date_text.split('/'))[-1]\n",
    "                        #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                        data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                    except:\n",
    "                        err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "                    \n",
    "                    # Adding the scraped date to data\n",
    "                    cur_date = str(datetime.today())\n",
    "                    data.append(cur_date)\n",
    "                    \n",
    "\n",
    "                    # Scraping the paragraph\n",
    "                    para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                    try:\n",
    "                        data.append(para_ele.text)  # Need to make this better\n",
    "                    except:\n",
    "                        err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "                    # Adding data to a collection\n",
    "                    collection.append(data)\n",
    "\n",
    "                df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "                if df.empty:\n",
    "                    err = scrapper_name + \": err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = FilterFunction(df)\n",
    "                # emptydataframe(\"Allafrica\",df)\n",
    "                # df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "              # pass\n",
    "                not_working_functions.append(\"Allafrica\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def zawya():\n",
    "        try :\n",
    "            print(\"zawya\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.zawya.com/en/search?q=pre+ipo\"\n",
    "            domain_url = \"https://www.zawya.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"zawya : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"h2\",{\"class\":\"teaser-title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"zawya : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"article-title\"}).text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"article-date\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"article-body\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":scraped_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"zawya : err: Empty datzawyaame\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"zawya\",df)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"zawya not working\")\n",
    "            not_working_functions.append('zawya')\n",
    "    def phnompenhpost():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Phnompenhpost\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.phnompenhpost.com/search/node/\"\n",
    "                domainUrl = \"https://www.phnompenhpost.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                ArticleDates = []\n",
    "                ScrapeDates = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        session = HTMLSession()\n",
    "                        resp = session.post(queryUrl)\n",
    "                        resp.html.render()\n",
    "                        pageSource = resp.html.html\n",
    "                        parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "                    except:\n",
    "                        err = \"phnompenhpost: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"w\") as f:\n",
    "                    #     f.write(pageSource)\n",
    "                    # break\n",
    "                    for item in parsedSource.find_all(\"li\", class_=\"search-result\"):\n",
    "                        requiredTag = item.find(\"h3\", class_=\"title\")\n",
    "                        currentArticleTitle = str(requiredTag.text).strip()\n",
    "                        # print(currentArticleTitle)\n",
    "                        currentArticleLink = requiredTag.find(\"a\")[\"href\"]\n",
    "                        # print(currentArticleLink)\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "                        currentArticleDateText = str(item.find(\"div\", class_=\"posted-date\").find(\"span\").text).split(\"by\")[0].strip()\n",
    "                        try:\n",
    "                            currentArticleDate = datetime.strptime(currentArticleDateText,\n",
    "                                                                \"%d %b %Y\").strftime(\"%d-%m-%Y\")\n",
    "                        except:\n",
    "                            err = \"phnompenhpost: err: Failed to retrieve date from article : \" + currentArticleLink\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(currentArticleLink))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        ArticleDates.append(currentArticleDate)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    # print(titles)\n",
    "                    # print(links)\n",
    "                    # print(ArticleDates)\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            # session = HTMLSession()\n",
    "                            # resp = session.get(link)\n",
    "                            # resp.html.render()\n",
    "                            # pageSource = resp.html.html\n",
    "                            # parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "                            headers = {\n",
    "                                'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                        except:\n",
    "                            err = \"phnompenhpost: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        # with open(\"response.html\", \"wb\") as f:\n",
    "                        #     f.write(pageSource.content)\n",
    "                        # break\n",
    "                        divTag = parsedSource.find(\"div\", id=\"ArticleBody\")\n",
    "                        for item in divTag.find_all(\"p\"):\n",
    "                            articleText += item.text\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "                    # print(ArticleBody)\n",
    "\n",
    "                # DataFrame creation\n",
    "                phnompenhpostDF = pd.DataFrame(scrapedData)\n",
    "                if phnompenhpostDF.empty:\n",
    "                    err = \"phnompenhpost: err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = FilterFunction(phnompenhpostDF)\n",
    "                emptydataframe(df)\n",
    "                return df\n",
    "            except:\n",
    "                not_working_functions.append(\"Phnompenhpost\")\n",
    "                print(\"Phnompenhpost not working\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "    def scmp():\n",
    "        try:\n",
    "            try:\n",
    "                print(\"Scmp\")\n",
    "                err_logs = []\n",
    "\n",
    "                baseSearchUrl = \"https://www.scmp.com/search/\"\n",
    "                domainUrl = \"https://www.scmp.com\"\n",
    "                keywords = ['IPO', 'pre-IPO', 'Public', 'Initial', 'Offering', 'initial']\n",
    "\n",
    "                # use this for faster testing\n",
    "                tkeywords = [\"IPO\"]\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                err_index = []\n",
    "                ArticleDates = []\n",
    "                ScrapeDates = []\n",
    "                for keyword in tkeywords:\n",
    "                    queryUrl = baseSearchUrl + keyword\n",
    "                    try:\n",
    "                        session = HTMLSession()\n",
    "                        resp = session.get(queryUrl)\n",
    "                        resp.html.render()\n",
    "                        pageSource = resp.html.html\n",
    "                        parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "                    except:\n",
    "                        err = \"scmp: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "                    for item in parsedSource.find_all(\"li\", class_=\"search-results__item item\"):\n",
    "                        requiredTag = item.find(\"span\", class_=\"content-link__title\")\n",
    "                        currentArticleTitle = requiredTag.text\n",
    "                        # print(currentArticleTitle)\n",
    "                        currentArticleLink = item.find(\"a\", class_=\"content__content-link content-link\")[\"href\"]\n",
    "                        # print(currentArticleLink)\n",
    "                        if currentArticleLink[0] == \"/\":\n",
    "                            links.append(domainUrl + currentArticleLink)\n",
    "                        else:\n",
    "                            links.append(currentArticleLink)\n",
    "                        titles.append(currentArticleTitle)\n",
    "                        try:\n",
    "                            currentArticleDate = datetime.strptime(item.find(\"div\", class_=\"wrapper__published-date\").text,\n",
    "                                                                \"%d %b %Y - %H:%M%p\").strftime(\"%d-%m-%Y\")\n",
    "                        except:\n",
    "                            err = \"scmp: err: Failed to retrieve date from article : \" + currentArticleLink\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(currentArticleLink))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        ArticleDates.append(currentArticleDate)\n",
    "                        ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "                    scrapedData[\"title\"] = titles\n",
    "                    scrapedData[\"link\"] = links\n",
    "                    scrapedData[\"publish_date\"] = ArticleDates\n",
    "                    scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                    # print(titles)\n",
    "                    # print(links)\n",
    "                    # print(ArticleDates)\n",
    "\n",
    "                    # Article's date and description scraping\n",
    "                    ArticleBody = []\n",
    "                    for link in links:\n",
    "                        articleText = \"\"\n",
    "                        try:\n",
    "                            # session = HTMLSession()\n",
    "                            # resp = session.get(link)\n",
    "                            # resp.html.render()\n",
    "                            # pageSource = resp.html.html\n",
    "                            # parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "                            headers = {\n",
    "                                'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                            pageSource = requests.get(link, headers=headers)\n",
    "                            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                        except:\n",
    "                            err = \"scmp: err: Failed to access article link : \" + link\n",
    "                            ArticleDates.append(\"Error\")\n",
    "                            err_index.append(links.append(link))\n",
    "                            err_logs.append(err)\n",
    "                            continue\n",
    "                        # with open(\"response.html\", \"w\") as f:\n",
    "                        #     f.write(pageSource)\n",
    "                        # break\n",
    "                        for item in parsedSource.find_all(\"script\", type=\"application/ld+json\"):\n",
    "                            tempDict = json.loads(item.text)\n",
    "                            try:\n",
    "                                ArticleText = tempDict[\"articleBody\"]\n",
    "                            except:\n",
    "                                continue\n",
    "                        ArticleBody.append(articleText)\n",
    "                    scrapedData[\"text\"] = ArticleBody\n",
    "                    # print(ArticleBody)\n",
    "\n",
    "                # DataFrame creation\n",
    "                scmpDF = pd.DataFrame(scrapedData)\n",
    "                if scmpDF.empty:\n",
    "                    err = \"scmp: err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df =FilterFunction(scmpDF)\n",
    "                emptydataframe(\"Scmp\",df)\n",
    "                return scmpDF\n",
    "            except:\n",
    "                print(\"Scmp not working\")\n",
    "                not_working_functions.append(\"Scmp\")\n",
    "        except:\n",
    "            df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "            return df1\n",
    "\n",
    "    # def theoutreach():\n",
    "    #     try:\n",
    "    #         try:\n",
    "    #             print(\"The out reach\")\n",
    "    #             err_logs = []\n",
    "\n",
    "    #             baseSearchUrl = \"https://theoutreach.in/?s=\"\n",
    "    #             domainUrl = \"hhttps://theoutreach.in\"\n",
    "    #             keywords = ['IPO', 'initial public offering','Pre IPO']\n",
    "\n",
    "    #             # use this for faster testing\n",
    "    #             tkeywords = [\"IPO\"]\n",
    "    #             scrapedData = {}\n",
    "    #             links = []\n",
    "    #             titles = []\n",
    "    #             err_index = []\n",
    "    #             ArticleDates = []\n",
    "    #             ScrapeDates = []\n",
    "    #             for keyword in tkeywords:\n",
    "    #                 queryUrl = baseSearchUrl + keyword\n",
    "    #                 try:\n",
    "    #                     session = HTMLSession()\n",
    "    #                     resp = session.post(queryUrl)\n",
    "    #                     resp.html.render()\n",
    "    #                     pageSource = resp.html.html\n",
    "    #                     parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "    #                 except:\n",
    "    #                     err = \"theoutreach: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "    #                     err_logs.append(err)\n",
    "    #                     continue\n",
    "    #                 # with open(\"response.html\", \"w\") as f:\n",
    "    #                 #     f.write(pageSource)\n",
    "    #                 # break\n",
    "    #                 for item in parsedSource.find(\"div\", class_=\"article-container\").find_all(\"article\"):\n",
    "    #                     requiredTag = item.find(\"h2\", class_=\"entry-title\")\n",
    "    #                     currentArticleTitle = str(requiredTag.find(\"a\")[\"title\"]).strip()\n",
    "    #                     # print(currentArticleTitle)\n",
    "    #                     currentArticleLink = requiredTag.find(\"a\")[\"href\"]\n",
    "    #                     # print(currentArticleLink)\n",
    "    #                     if currentArticleLink[0] == \"/\":\n",
    "    #                         links.append(domainUrl + currentArticleLink)\n",
    "    #                     else:\n",
    "    #                         links.append(currentArticleLink)\n",
    "    #                     titles.append(currentArticleTitle)\n",
    "    #                     currentArticleDateText = item.find(\"time\", class_=\"entry-date published updated\").text\n",
    "    #                     try:\n",
    "    #                         currentArticleDate = datetime.strptime(currentArticleDateText,\n",
    "    #                                                             \"%B %d, %Y\").strftime(\"%d-%m-%Y\")\n",
    "    #                     except:\n",
    "    #                         err = \"theoutreach: err: Failed to retrieve date from article : \" + currentArticleLink\n",
    "    #                         ArticleDates.append(\"Error\")\n",
    "    #                         err_index.append(links.append(currentArticleLink))\n",
    "    #                         err_logs.append(err)\n",
    "    #                         continue\n",
    "    #                     ArticleDates.append(currentArticleDate)\n",
    "    #                     ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "    #                 scrapedData[\"title\"] = titles\n",
    "    #                 scrapedData[\"link\"] = links\n",
    "    #                 scrapedData[\"publish_date\"] = ArticleDates\n",
    "    #                 scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "    #                 # print(titles)\n",
    "    #                 # print(links)\n",
    "    #                 # print(ArticleDates)\n",
    "\n",
    "    #                 # Article's date and description scraping\n",
    "    #                 ArticleBody = []\n",
    "    #                 for link in links:\n",
    "    #                     articleText = \"\"\n",
    "    #                     try:\n",
    "    #                         session = HTMLSession()\n",
    "    #                         resp = session.get(link)\n",
    "    #                         resp.html.render()\n",
    "    #                         pageSource = resp.html.html\n",
    "    #                         parsedSource = BeautifulSoup(pageSource, \"html.parser\")\n",
    "    #                         # headers = {\n",
    "    #                         #     'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "    #                         # pageSource = requests.get(link, headers=headers)\n",
    "    #                         # parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "    #                     except:\n",
    "    #                         err = \"theoutreach: err: Failed to access article link : \" + link\n",
    "    #                         ArticleDates.append(\"Error\")\n",
    "    #                         err_index.append(links.append(link))\n",
    "    #                         err_logs.append(err)\n",
    "    #                         continue\n",
    "    #                     # with open(\"response.html\", \"w\") as f:\n",
    "    #                     #     f.write(pageSource)\n",
    "    #                     # break\n",
    "    #                     divTag = parsedSource.find(\"div\", class_=\"entry-content clearfix\")\n",
    "    #                     for item in divTag.find_all(\"p\"):\n",
    "    #                         articleText += item.text\n",
    "    #                     ArticleBody.append(articleText)\n",
    "    #                 scrapedData[\"text\"] = ArticleBody\n",
    "    #                 # print(ArticleBody)\n",
    "\n",
    "    #             # DataFrame creation\n",
    "    #             theoutreachDF = pd.DataFrame(scrapedData)\n",
    "    #             if theoutreachDF.empty:\n",
    "    #                 err = \"theoutreach: err: Empty dataframe\"\n",
    "    #                 err_logs.append(err)\n",
    "    #             df = FilterFunction(theoutreachDF)\n",
    "    #             emptydataframe(\"The out reach\",df)\n",
    "    #             return df\n",
    "    #         except:\n",
    "    #             print(\"The outreach is not working\")\n",
    "    #             not_working_functions(\"The out reach\")\n",
    "    #     except:\n",
    "    #         df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "    #         return df1\n",
    "    # def dealstreetasia():\n",
    "    #     try:\n",
    "    #         try:\n",
    "    #             print(\"Dealstreetasia\")\n",
    "    #             err_logs = []  # Access to view error logs\n",
    "    #             url = \"https://www.dealstreetasia.com/?s=ipo\"\n",
    "    #             domain_url = \"https://www.dealstreetasia.com/\"\n",
    "\n",
    "    #             try:\n",
    "    #                 page = requests.get(url)\n",
    "    #                 soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    #             except:\n",
    "    #                 err = \"err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "    #                 err_logs.append(err)\n",
    "    #                 return None\n",
    "\n",
    "    #             # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "    #             div_class = \"category-link\"\n",
    "    #             h1_class = \"col-xl-8 col-lg-10 col-main\"\n",
    "    #             date_div_class = \"published-date\"\n",
    "    #             para_div_class = \"content-section\"\n",
    "\n",
    "    #             links = []\n",
    "\n",
    "    #             for divtag in soup.find_all(\"p\", {\"class\": div_class}):\n",
    "    #                 for a in divtag.find_all(\"a\", href=True):\n",
    "    #                     link = a[\"href\"]  # Gets the link\n",
    "\n",
    "    #                     # Checking the link if it is a relative link\n",
    "    #                     if link[0] == '/':\n",
    "    #                         link = domain_url + link\n",
    "\n",
    "    #                     links.append(link)\n",
    "\n",
    "\n",
    "\n",
    "    #             collection = []\n",
    "\n",
    "    #             for link in links:\n",
    "    #                 try:\n",
    "    #                     l_page = requests.get(link)\n",
    "    #                     l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "    #                 except:\n",
    "    #                     err = \"err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "    #                     err_logs.append(err)\n",
    "    #                     continue\n",
    "\n",
    "    #                 data = []\n",
    "    #                 # Scraping the heading\n",
    "    #                 h1_ele = l_soup.find(\"div\", {\"class\": h1_class})\n",
    "    #                 try:\n",
    "    #                     data.append(h1_ele.text)\n",
    "    #                 except:\n",
    "    #                     err = \"err: Failed to find title in page. Link: \" + link\n",
    "    #                     err_logs.append(err)\n",
    "    #                     continue  # drops the complete data if there is an error\n",
    "\n",
    "    #                 # Adding the link to data\n",
    "    #                 data.append(link)\n",
    "\n",
    "    #                 # Scraping the published date\n",
    "    #                 date_ele = l_soup.find(\"p\", {\"class\": date_div_class})\n",
    "    #                 try:\n",
    "    #                     date_text = date_ele.text\n",
    "    #                     date_text = (date_text.split('/'))[-1]\n",
    "    #                     date_text = date_text.replace(\" Updated: \", \"\")\n",
    "    #                     data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "    #                 except:\n",
    "    #                     err = \"err: Failed to find date in page. Link: \" + link\n",
    "    #                     err_logs.append(err)\n",
    "    #                     continue  # drops the complete data if there is an error\n",
    "\n",
    "    #                 # Adding the scraped date to data\n",
    "    #                 cur_date = str(datetime.today())\n",
    "    #                 data.append(cur_date)\n",
    "\n",
    "    #                 # Scraping the paragraph\n",
    "    #                 para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "    #                 try:\n",
    "    #                     data.append(para_ele.text)  # Need to make this better\n",
    "    #                 except:\n",
    "    #                     err = \"err: Failed to find paragraph in page. Link: \" + link\n",
    "    #                     err_logs.append(err)\n",
    "    #                     continue  # drops the complete data if there is an error\n",
    "\n",
    "    #                 # Adding data to the collection\n",
    "    #                 collection.append(data)\n",
    "\n",
    "    #             df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "    #             if df.empty:\n",
    "    #                 err = \"err: Empty dataframe\"\n",
    "    #                 err_logs.append(err)\n",
    "    #             df =FilterFunction(df)\n",
    "    #             emptydataframe(\"Dealstreetasia\",df)\n",
    "    #             log_errors(err_logs)\n",
    "    #             return df\n",
    "    #         except:\n",
    "    #             print(\"Dealstreetasia not working\")\n",
    "    #             not_working_functions.append(\"Dealstreetasia\")\n",
    "    #     except:\n",
    "    #         df1 = pd.DataFrame(columns =['title','link','publish_date','scraped_date'])\n",
    "    #         return df1\n",
    "    def livemint():\n",
    "        try:\n",
    "            print(\"Livemint India\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.livemint.com/Search/Link/Keyword/ipo\"\n",
    "            domain_url = \"https://www.livemint.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            h2_class = \"headline\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_class = \"headline\"\n",
    "            date_span_class = \"pubtime\"\n",
    "            para_ul_class = \"highlights\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"h2\", {\"class\": h2_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"livemint\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": date_span_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    date_text = ''.join((date_text.split(':'))[1:])\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                try:\n",
    "                    para_ele = (l_soup.findAll(\"ul\", {\"class\": para_ul_class}))[-1]\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    new_para_p_class = \"summary\"\n",
    "                    try:\n",
    "                        para_ele = (l_soup.findAll(\"p\", {\"class\": new_para_p_class}))[-1]\n",
    "                        data.append(para_ele.text)  # Need to make this better\n",
    "                    except:\n",
    "                        err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                        err_logs.append(err)\n",
    "                        continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Livemint India \",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"Livemint India\")\n",
    "            print(\"Livemint not working\")\n",
    "    def guardian():\n",
    "        try:\n",
    "            print(\"Bahamas\")\n",
    "            err_logs = []  # Access to view error logs\n",
    "            url = \"https://bahamaspress.com/\"\n",
    "            domain_url = \"https://bahamaspress.com/?s=ipo\"\n",
    "\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            except:\n",
    "                err = \"err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                return None\n",
    "\n",
    "            # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "            div_class = \"item-details\"\n",
    "            h1_class = \"entry-title\"\n",
    "            date_div_class = \"td-post-date\"\n",
    "            para_div_class = \"td-post-content\"\n",
    "\n",
    "            links = []\n",
    "            for h3 in soup.find_all(\"h3\",{\"class\":\"entry-title td-module-title\"}):\n",
    "                link = h3.a[\"href\"]\n",
    "                if link[0] == '/':\n",
    "                    link = domain_url + link\n",
    "                links.append(link)\n",
    "            collection = []\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = \"err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                try:\n",
    "                    data.append(h1_ele.text)\n",
    "                except:\n",
    "                    err = \"err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": date_div_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "                except:\n",
    "                    err = \"err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = \"err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding data to the collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = \"err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            # df = FilterFunction(df)\n",
    "            emptydataframe(\"Guardian Bahamas\",df)\n",
    "            log_errors(err_logs)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"Guardian bahamas\")\n",
    "            print(\"Guardian bahamas not working\")\n",
    "    def azernews():\n",
    "        try:\n",
    "            print(\"Azernews\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.azernews.az/search.php?query=ipo\"\n",
    "            domain_url = \"https://www.azernews.az/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            links = []\n",
    "\n",
    "            for a in soup.find_all(\"a\",{\"class\":\"news-item shadow\"}, href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "            print(links)\n",
    "            collection = []\n",
    "            scrapper_name = \"azernews\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"div\", {\"class\": \"article-content-wrapper\"}).h2\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": \"me-3\"})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = l_soup.find(\"div\", {\"class\": \"article-content\"})\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "            # print(err_logs)\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            # print(df) # For debugging. To check if df is created\n",
    "            # print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Azernews\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"Azernews\")\n",
    "            print(\"Azernews is not working\")\n",
    "    def chosenilboenglish():\n",
    "        try:\n",
    "            print(\"chosenilboenglish\")\n",
    "            err_logs = []\n",
    "            url = \"https://english.chosun.com/svc/list_in/search.html?query=ipo&pageconf=total\"\n",
    "            domain_url = \"http://english.chosun.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            dl_class = \"list_item\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_id = \"news_title_text_id\"\n",
    "            date_p_id = \"date_text\"\n",
    "            para_div_class = \"par\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            # for divtag in soup.find_all(\"dl\", {\"class\": dl_class}):\n",
    "            for a in soup.find_all(\"dl\",{\"class\":\"list_item\"}):\n",
    "                    link = a.dt.a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "            print(links)\n",
    "            collection = []\n",
    "            scrapper_name = \"azernews\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"h1\", {\"id\": title_h1_id})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"p\", {\"id\": date_p_id})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"chosenilboenglish\",df)\n",
    "            log_errors(err_logs)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"chosenilboenglish\")\n",
    "            print(\"chosenilboenglish not working\")\n",
    "    def otempo():\n",
    "        try:\n",
    "            print(\"Brazil\")\n",
    "\n",
    "            err_logs = []\n",
    "            url = \"https://www.otempo.com.br/busca-portal-o-tempo-7.6253516?q=IPO\"\n",
    "            domain_url = \"https://www.otempo.com.br/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Otempo : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"h2\",{\"class\":\"titulo\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            # print(links)\n",
    "            for link in links:\n",
    "                try:\n",
    "                    try:\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    # print(soup)\n",
    "                    # print(link)\n",
    "                    except:\n",
    "                        err = \"Otempo : err : Couldn't fetch url \" + link \n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    #Published Date\n",
    "                    pub_date.append(soup.find(\"div\" , {\"class\" : \"data-publicacao\"}).text)\n",
    "                    \n",
    "\n",
    "                    #Title of article \n",
    "                    title.append(soup.find(\"div\" , {\"class\" : \"cell titulo\"}).h1.text)       \n",
    "\n",
    "                    #Text of article\n",
    "                    text.append(soup.find(\"div\" , {\"class\" : \"cell chamada\"}).h2.text)\n",
    "\n",
    "                    #Scrapped date \n",
    "                    scraped_date.append(str(today))\n",
    "\n",
    "                    #Working links\n",
    "                    final_links.append(link)\n",
    "                except:\n",
    "                    continue\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"otempo : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Otempo brazil\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Otempo brazil not working\")\n",
    "            not_working_functions.append(\"Otempo Brazil\")\n",
    "    def elicudadona():\n",
    "        try:\n",
    "            print(\"Chile\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.elciudadano.com/?s=oferta+p%C3%BAblica+inicial\"\n",
    "            domain_url = \"https://www.elciudadano.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Elicudadona : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"h3\",{\"class\":\"mb-3\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Elicudadona : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\",{\"class\":\"time-ago time-now\"})[\"data-date\"])\n",
    "                # print(pub_date) \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"mb-4 the_title\"}).text)      \n",
    "                # print(title) \n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"the-excerpt-\"}).text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Folha : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df =FilterFunction(df)\n",
    "            emptydataframe(\"Elicudadona Chile\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"elicudadona chile\")\n",
    "            print(\"elicudadona not working\")\n",
    "    \n",
    "    def standartnews(keyword):\n",
    "        try:\n",
    "            print(\"standartnews bulgaria\")\n",
    "            err_logs = []\n",
    "            url = f\"https://www.standartnews.com/articles/search.html?keywords={keyword}&author=&category=-1&date_from=&date_to=\"\n",
    "            domain_url = \"https://www.standartnews.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Standartnews : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"news-general-link\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    # print(soup)\n",
    "                    # print(link)\n",
    "                except:\n",
    "                    err = \"Standartnews : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\")[\"datetime\"])\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"title-cont\"}).h1.text)       \n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"content\"}).p.text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Folha : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"standartnews bulgaria\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"standartnews bulgaria not working\")\n",
    "            not_working_functions.append(\"standartnews bulgaria\")\n",
    "    def lastampa():\n",
    "        try:\n",
    "            print(\"lastampa Italy\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.lastampa.it/ricerca?query=IPO&tracking=LSHHD-S\"\n",
    "            domain_url = \"\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Lastampa : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"entry__content__top\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"lastampa : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                if(soup.find(\"div\" , {\"class\" : \"story__content\"}).p == None or len(soup.find(\"div\" , {\"class\" : \"story__content\"}).p) == 0 ):\n",
    "                    continue\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"story__content\"}).p)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\",{\"class\":\"story__date\"})[\"datetime\"])\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"story__title\"}).text)       \n",
    "                # print(title)\n",
    "                #Text of article\n",
    "                \n",
    "            \n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\" : text , \"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"lastampa : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Lastampa Italy\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Lastampa Italy not working\")\n",
    "            not_working_functions.append(\"Lastampa Italy\")\n",
    "    def liputan6():\n",
    "        try:\n",
    "            print(\"Liputan Indo\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.liputan6.com/search?q=IPO\"\n",
    "            domain_url = \"https://www.liputan6.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Liputan6 : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"ui--a articles--iridescent-list--text-item__title-link\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Liputan6 : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\",{\"class\":\"read-page--header--author__datetime updated\"})[\"datetime\"])\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"read-page--header--title entry-title\"}).text)       \n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"article-content-body__item-content\"}).p.text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Folha : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Liputan\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Liputan indo not working\")\n",
    "            not_working_functions.append(\"Liputan indo\")\n",
    "    def milenio():\n",
    "        try:\n",
    "            print(\"Mexico\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.milenio.com/buscador?text=oferta+p%C3%BAblica+inicial\"\n",
    "            domain_url = \"https://www.milenio.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Milenio : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url + div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Milenio : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" : \"content-date\"}).time.text)\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"title\"}).text)       \n",
    "            \n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"media-container news\"}).p.text)\n",
    "            \n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Milenio : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Millenio Mexico\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Mexico not working\")\n",
    "            not_working_functions.append(\"Millenio Mexico\")\n",
    "    def scoop():\n",
    "        try:\n",
    "            print(\"New Zealand\")\n",
    "            err_logs = []\n",
    "            url = \"https://search.scoop.co.nz/search?q=IPO&submit=\"\n",
    "            domain_url = \"https://search.scoop.co.nz/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Scoop : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"result\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Scoop : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" : \"byline\"}).b.text)\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"story-top\"}).h1.text)     \n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"id\" : \"article\"}).p.text)\n",
    "                \n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Scoop : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"New Zealand\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"New Zealand\")\n",
    "            print(\"New Zealand not working\")\n",
    "    def globallegalchronicle():\n",
    "        try:\n",
    "            print(\"Globallegal\")\n",
    "            err_logs = []\n",
    "            url = \"https://globallegalchronicle.com/?s=ipo\"\n",
    "            domain_url = \"https://globallegalchronicle.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            h3_class = \"entry-title\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_class = \"entry-title\"\n",
    "            date_time_class = \"entry-date\"\n",
    "            para_div_class = \"entry-content\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"h3\", {\"class\": h3_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"globallegalchronicle\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link, headers=headers)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"time\", {\"class\": date_time_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"globalillegalchronicle\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Globallegal not working\")\n",
    "            not_working_functions.append(\"Global legal\")\n",
    "    \n",
    "    def supchina():\n",
    "        try:\n",
    "            print(\"supchina\")\n",
    "            err_logs = []\n",
    "            url = \"https://supchina.com/?s=ipo\"\n",
    "            domain_url = \"https://supchina.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            h3_class = \"card__title\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_class = \"h1\"\n",
    "            date_time_class = \"post__date\"\n",
    "            para_div_class = \"post__chunk\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"h3\", {\"class\": h3_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"supchina\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link, headers=headers)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"time\", {\"class\": date_time_class})\n",
    "                try:\n",
    "                    date_text = \"\".join(((\" \".join((date_ele.text.split(\" \"))[1:])).split(\"\\t\"))[0])\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Supchina\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Supchina not working\")\n",
    "            not_working_functions.append(\"Supchina\")\n",
    "    def aljazeera():\n",
    "        try:\n",
    "            print(\"Al jazeera Qatar\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.aljazeera.com/search/IPO\"\n",
    "            domain_url = \"https://www.aljazeera.com/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Aljazeera : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"h3\",{\"class\":\"gc__title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Aljazeera : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" ,{\"class\" : \"date-simple css-1yjq2zp\"}).text)\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"header\" , {\"class\" : \"article-header\"}).h1.text)       \n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"wysiwyg wysiwyg--all-content css-1ck9wyi\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "            \n",
    "            \n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Aljazeera: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Aljazeera Qatar\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Al Jazeera not working\")\n",
    "            not_working_functions.append(\"Al Jazeera Qatar\")\n",
    "    def aif():\n",
    "            try:\n",
    "                print(\"AIF Russia\")\n",
    "                err_logs = []\n",
    "                url = \"https://aif.ru/search?text=IPO\"\n",
    "                domain_url = \"https://aif.ru/\"\n",
    "                title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "                try:\n",
    "                    page = requests.get(url)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    # print(soup)\n",
    "                    \n",
    "                except:\n",
    "                    err = \"Aif : err : Couldn't fetch \" + url \n",
    "                    err_logs.append(err)\n",
    "                    return \n",
    "\n",
    "                all_divs = soup.find_all(\"div\",{\"class\":\"text_box\"})\n",
    "                for div in all_divs:\n",
    "                    links.append(div.a[\"href\"])\n",
    "                #Fetch all the necessary data \n",
    "                # print(links)\n",
    "                final_links = []\n",
    "                today = date.today()\n",
    "                for link in links:\n",
    "                    try:\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                        # print(soup)\n",
    "                    # print(link)\n",
    "                    except:\n",
    "                        err = \"Aif : err : Couldn't fetch url \" + link \n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    div = soup.find(\"div\" , {\"class\" : \"article_text\"})\n",
    "                    t = \"\"\n",
    "                    for i in div.find_all(\"p\"):\n",
    "                        t += i.text + \" \"\n",
    "                    text.append(t) \n",
    "                    # print(text)\n",
    "                    #Published Date\n",
    "                    pub_date.append(soup.find(\"div\" , {\"class\" : \"date\"}).text)\n",
    "                    \n",
    "\n",
    "                    #Title of article \n",
    "                    title.append(soup.find(\"h1\" , {\"itemprop\" : \"headline\"}).text)       \n",
    "\n",
    "                    #Text of article\n",
    "                \n",
    "                \n",
    "                    #Scrapped date \n",
    "                    scraped_date.append(str(today))\n",
    "\n",
    "                    #Working links\n",
    "                    final_links.append(link)\n",
    "                df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "                if df.empty:\n",
    "                    err = \"Aif : err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = df.drop_duplicates(subset=[\"link\"])\n",
    "                # df = FilterFunction(df)\n",
    "                emptydataframe(\"Aif Russia\",df)\n",
    "                log_errors(err_logs)\n",
    "                df = df[df[\"text\"] != \"\"]\n",
    "                df = translate_dataframe(df)\n",
    "                # df = FilterFunction(df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"AIF Russia not working\")\n",
    "                not_working_functions.append(\"AIF\")\n",
    "    def monitor():\n",
    "        try:\n",
    "            print(\"Monitor uganda\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.monitor.co.ug/service/search/uganda/1822068?pageNum=0&query=IPO%20offering&sortByDate=true&channelId=1448278\"\n",
    "            domain_url = \"https://www.monitor.co.ug/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Monitor : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"li\",{\"class\":\"search-result\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    try:\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                  # print(soup)\n",
    "                  # print(link)\n",
    "                    except:\n",
    "                        err = \"Monitor : err : Couldn't fetch url \" + link \n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "                  #Published Date\n",
    "                    pub_dat = soup.find(\"time\" , {\"class\" : \"date\"}).text\n",
    "                  \n",
    "\n",
    "                  #Title of article \n",
    "                    titl  = soup.find(\"h1\" , {\"class\" : \"title-medium\"}).text      \n",
    "\n",
    "                  #Text of article\n",
    "                    div = soup.find(\"div\" , {\"class\" : \"paragraph-wrapper\"})\n",
    "                    t = \"\"\n",
    "                    for i in div.find_all(\"p\"):\n",
    "                        t += i.text + \" \"\n",
    "                    text.append(t)\n",
    "                  #Scrapped date \n",
    "                    scraped_date.append(str(today))\n",
    "                    pub_date.append(pub_dat)\n",
    "                    title.append(titl)\n",
    "                  #Working links\n",
    "                    final_links.append(link)\n",
    "                except:\n",
    "                    continue\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Monitor : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Monitor Uganda\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Monitor Uganda not working\")\n",
    "            not_working_functions.append(\"Uganda Monitor\")\n",
    "    def thesun():\n",
    "        try:\n",
    "            print(\"the sun UK\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.thesun.co.uk/?s=IPO\"\n",
    "            domain_url = \"https://www.thesun.co.uk/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"The sun  : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"teaser-anchor teaser-anchor--search\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"The sun : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"article__timestamp\"}).text)\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"article__headline\"}).text)       \n",
    "\n",
    "                #Text of article     \n",
    "                div = soup.find(\"div\" , {\"class\" : \"article__content\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "            \n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"The sun  : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"The sun UK\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"The sun uk not working\")\n",
    "            not_working_functions.append(\"The sun UK\")\n",
    "\n",
    "\n",
    "\n",
    "    def koreannewsgazette():\n",
    "        try :\n",
    "            print(\"Koreannewsgazette\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.koreanewsgazette.com/?s=IPO\"\n",
    "            domain_url = \"https://www.koreanewsgazette.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"koreannewsgazette : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"header\",{\"class\":\"entry-header\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"koreannewsgazette : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\" , {\"class\" :\"entry-date published updated\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"entry-title\"}).text)  \n",
    "                # print(title)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"entry-content clearfix\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"koreannewsgazette : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"koreannewsgazette\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"koreannewsgazette not working\")\n",
    "            not_working_functions.append('koreannewsgazette')\n",
    "    \n",
    "    def parool():\n",
    "        try:\n",
    "            print(\"Parool Netherlands\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.parool.nl/search?query=beursgang\"\n",
    "            domain_url = \"https://www.parool.nl\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Parool : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"article\",{\"class\":\"fjs-teaser-compact teaser--compact\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Parool : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "                #Published Date\n",
    "                pub_date.append(str(soup.find(\"time\" , {\"class\" : \"artstyle__production__datetime\"})[\"datetime\"]))\n",
    "                \n",
    "\n",
    "                #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"artstyle__header-title\"}).text)       \n",
    "            \n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"section\" , {\"class\" : \"artstyle__main\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\" , {\"class\" : \"artstyle__paragraph \"}):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                \n",
    "                \n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Parool : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Parool Netherlands\")\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Netherlands is not working\")\n",
    "            not_working_functions.append(\"Netherlands parool\")\n",
    "    def shabiba():\n",
    "        try:\n",
    "            print(\"Oman shabiba\")\n",
    "            err_logs = []\n",
    "            url = \"https://shabiba.com/search?search=%D8%A7%D9%84%D8%B7%D8%B1%D8%AD+%D8%A7%D9%84%D8%B9%D8%A7%D9%85+%D8%A7%D9%84%D8%A3%D9%88%D9%84%D9%8A\"\n",
    "            domain_url = \"https://shabiba.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "              # print(soup)\n",
    "\n",
    "            except:\n",
    "                err = \"Shabiba : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "\n",
    "            all_divs = soup.find_all(\"h2\",{\"class\":\"post-title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "          #Fetch all the necessary data \n",
    "          # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "              # print(soup)\n",
    "              # print(link)\n",
    "                except:\n",
    "                    err = \"Shabiba : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "              #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" : \"text-muted\"}).text)\n",
    "\n",
    "\n",
    "              #Title of article \n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"my-3\"}).text)       \n",
    "\n",
    "              #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"mb-5 post-details\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "              # print(t)\n",
    "\n",
    "              #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "              #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Shabiba : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Oman\",df)\n",
    "            log_errors(err_logs)\n",
    "        #   df = translate_dataframe(df)\n",
    "        #   df  = link_correction(df)\n",
    "        \n",
    "            return df\n",
    "        except:\n",
    "            print(\"Oman not working\")\n",
    "            not_working_functions.append(\"Oman shabiba\")\n",
    "    def sabah():\n",
    "        try :\n",
    "            print(\"Sabah\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.sabah.com.tr/arama?query=IPO\"\n",
    "            domain_url = \"https://www.sabah.com.tr\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Sabah : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"figure\",{\"class\":\"multiple boxShadowSet\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Sabah : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                if(soup.find(\"h1\" , {\"class\" : \"pageTitle\"}) == None or soup.find(\"span\" , {\"class\" :\"textInfo align-center\"}) == None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"pageTitle\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"textInfo align-center\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                text.append(soup.find(\"div\" , {\"class\" : \"newsBox\"}).text)\n",
    "            \n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Sabah : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Sabah\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Sabah not working\")\n",
    "            not_working_functions.append('Sabah')\n",
    "    def swissinfo():\n",
    "        try :\n",
    "            print(\"Swissinfo\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.swissinfo.ch/service/search/eng/45808844?query=IPO\"\n",
    "            domain_url = \"https://www.swissinfo.ch\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Swissinfo : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"si-teaser__link\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"koreannewsgazette : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"si-detail__title\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\" , {\"class\" :\"si-detail__date\"})[\"datetime\"])\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"section\" , {\"class\" : \"si-detail__content\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Swissinfo : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Swissinfo\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Swissinfo not working\")\n",
    "            not_working_functions.append('Swissinfo')\n",
    "    def dziennik():\n",
    "        try :\n",
    "            print(\"Dziennik\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.dziennik.pl/szukaj?c=1&b=1&o=1&s=0&search_term=&q=IPO\"\n",
    "            domain_url = \"https://www.thelocal.se\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Dziennik : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"resultContent\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Dziennik : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"mainTitle\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"datePublished\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"detail intext articleBody\"}) \n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\" , {\"class\" : \"hyphenate\"}):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Dziennik : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Dziennik\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Dziennik not working\")\n",
    "            not_working_functions.append('dziennik')\n",
    "    def aljarida():\n",
    "        try :\n",
    "            print(\"Aljarida\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.aljarida.com/search/%D8%A7%D9%84%D8%B7%D8%B1%D8%AD%20%D8%A7%D9%84%D8%B9%D8%A7%D9%85%20%D8%A7%D9%84%D8%A3%D9%88%D9%84%D9%8A/\"\n",
    "            domain_url = \"https://www.aljarida.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Aljarida : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"text\"})\n",
    "            for div in all_divs:\n",
    "                links.append(domain_url+div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Aljarida : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"title\"}).h1.text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"date\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"section\" , {\"id\" : \"main\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\" , {\"class\" : \"ar\"}):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Aljarida : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Aljarida\",df)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"aljarida not working\")\n",
    "            not_working_functions.append('aljarida')\n",
    "    def hungary():\n",
    "        try :\n",
    "            print(\"Hungary\")\n",
    "            err_logs = []\n",
    "            url = \"https://444.hu/kereses?q=IPO\"\n",
    "            domain_url = \"https://444.hu/\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Hungary : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"h1\",{\"class\":\"kJ eR eF eT eU eV eW kI eZ\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Hungary : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"kH eR eF eT eU eV eW fa eZ kG\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"eE f5 eT eU eV eW fa fh\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"eA f- fG eu eT he hf eW hg lp\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Hungary : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Hungary\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Hungary not working\")\n",
    "            not_working_functions.append('Hungary')\n",
    "    def jauns():\n",
    "        try :\n",
    "            print(\"jauns\")\n",
    "            err_logs = []\n",
    "            url = \"https://jauns.lv/meklet?q=IPO\"\n",
    "            domain_url = \"https://jauns.lv\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"jauns : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"article-small__link\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Jaus : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"span\" , {\"class\" : \"heading__text\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"meta-info__publish-date-full\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"data-io\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text ,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"jauns : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"jauns\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"jauns not working\")\n",
    "            not_working_functions.append('jauns')\n",
    "    def pulse():\n",
    "        try :\n",
    "            print(\"Pulse\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.pulse.ng/search?q=IPO\"\n",
    "            domain_url = \"https://www.pulse.ng\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"Pulse : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"gradient-overlay\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Pulse : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"article-headline \"}).span.text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"time\" , {\"class\" :\"detail-article-date date-type-publicationDate\"})[\"datetime\"])\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"article-content \"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Pulse : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Pulse\",df)\n",
    "            df = df[df[\"text\"] != \"\"]\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"Pulse not working\")\n",
    "            not_working_functions.append('Pulse')\n",
    "    def vnexpress():\n",
    "        try :\n",
    "            print(\"vnexpress\")\n",
    "            err_logs = []\n",
    "            url = \"https://timkiem.vnexpress.net/?q=IPO\"\n",
    "            domain_url = \"https://timkiem.vnexpress.net\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"vnexpress : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"h3\",{\"class\":\"title-news\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"vnexpress : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                if(soup.find(\"h1\" , {\"class\" : \"title-detail\"})== None):\n",
    "                    continue\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"title-detail\"}))\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"date\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"article\" , {\"class\" : \"fck_detail \"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text ,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"vnexpress : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = translate_dataframe(df)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"vnexpress\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"vnexpress not working\")\n",
    "            not_working_functions.append('vnexpress')\n",
    "    def jamaicaobserver():\n",
    "        try :\n",
    "            print(\"jamaicaobserver\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.jamaicaobserver.com/search/?q=IPO\"\n",
    "            domain_url = \"https://www.jamaicaobserver.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"jamaicaobserver : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"div\",{\"class\":\"entry-title\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"Jamaice Observer : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "                title.append(soup.find(\"div\" , {\"class\" : \"headline col-12\"}).text)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"div\" , {\"class\" :\"article-pubdate\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"article-restofcontent\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\" , {\"class\" : \"article__body\"}):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text ,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"jamaicaobserver : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"jamaicaobserver\",df)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"jamaicaobserver not working\")\n",
    "            not_working_functions.append('jamaicaobserver') \n",
    "    def independent():\n",
    "        try :\n",
    "            print(\"independent\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.independent.ie/search/?q=IPO\"\n",
    "            domain_url = \"https://www.independent.ie\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"independent : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"a\",{\"class\":\"c-card1-textlink -d:b -as:1\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    try:\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    # print(soup)\n",
    "                    # print(link)\n",
    "                    except:\n",
    "                        err = \"independent : err : Couldn't fetch url \" + link \n",
    "                        err_logs.append(err)\n",
    "                        continue\n",
    "\n",
    "                    #Title of article\n",
    "                    title.append(soup.find(\"h1\" , {\"class\" : \"title1-main\"}).text)\n",
    "                    #Published Date\n",
    "                    pub_date.append(soup.find(\"time\" , {\"class\" :\"time1\"}).text)\n",
    "                    # print(pub_date)\n",
    "\n",
    "                    #Text of article\n",
    "                    div = soup.find(\"div\" , {\"class\" : \"n-body1\"})\n",
    "                    t = \"\"\n",
    "                    for i in div.find_all(\"p\"):\n",
    "                        t += i.text + \" \"\n",
    "                    text.append(t)\n",
    "                    # print(text)\n",
    "\n",
    "                    #Scrapped date \n",
    "                    scraped_date.append(str(today))\n",
    "\n",
    "                    #Working links\n",
    "                    final_links.append(link)\n",
    "                except:\n",
    "                    continue\n",
    "            df = pd.DataFrame({\"text\":text ,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"independent : err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"independent\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"independent not working\")\n",
    "            not_working_functions.append('independent')\n",
    "    def albaniandailynews():\n",
    "        try:\n",
    "            print(\"Albania\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.albaniandailynews.com/search.php?s=ipo\"\n",
    "            domain_url = \"https://albaniandailynews.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\"\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            h3_class = \"alith_post_title\"  # Class name of h3 containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            h3_class_title = \"alith_post_title\"\n",
    "            date_span_class = \"meta_date\"\n",
    "            para_div_class = \"column-1\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"h3\", {\"class\": h3_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link\n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"albaniandailynews\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link, headers=headers)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    h2_ele = l_soup.find(\"h3\", {\"class\": h3_class_title})\n",
    "                    data.append(h2_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": date_span_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = (l_soup.findAll(\"div\", {\"class\": para_div_class}))[-1]\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"Albania daily news\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Albaniadailynews not working\")\n",
    "            not_working_functions.append(\"Albania\")\n",
    "    def ewn():\n",
    "        try:\n",
    "            print(\"EWN\")\n",
    "            err_logs = []  # Access to view error logs\n",
    "            url = \"https://ewnews.com/?s=ipo\"\n",
    "            domain_url = \"https://ewnews.com\"\n",
    "\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            except:\n",
    "                err = \"err: Failed to access main url: \" + url + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                return None\n",
    "\n",
    "            # Class names of the elements to be scraped - change if the website to be scraped changes them\n",
    "            div_class = \"entry-title\"\n",
    "            h1_class = \"entry-title\"\n",
    "            date_div_class = \"entry-date published\"\n",
    "            para_div_class = \"entry-content col-md-12\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for h3 in soup.find_all(\"h3\",{\"class\":\"entry-title\"}):\n",
    "                link = h3.a[\"href\"]\n",
    "                if link[0] == '/':\n",
    "                    link = domain_url + link\n",
    "                links.append(link)\n",
    "\n",
    "            collection = []\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = \"err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                try:\n",
    "                    data.append(h1_ele.text)\n",
    "                except:\n",
    "                    err = \"err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                date_ele = l_soup.find(\"span\", {\"class\": date_div_class})\n",
    "                try:\n",
    "                    date_text = date_ele.text\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text)  # The date_text could be further modified to represent a proper date format\n",
    "\n",
    "                except:\n",
    "                    err = \"err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "\n",
    "                # Scraping the paragraph\n",
    "                para_ele = l_soup.find(\"div\", {\"class\": para_div_class})\n",
    "                try:\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = \"err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "                # Adding data to the collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = \"err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"EWN\",df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"EWN not working\")\n",
    "            not_working_functions.append(\"Ewn\")\n",
    "    def bloombergquint():\n",
    "        try:\n",
    "            print(\"Bloombergquint\")\n",
    "            err_logs = []  # Access to view error logs\n",
    "            keywords = [\"ipo\",\"pre-ipo\",\"pre ipo\",\"listing\"]\n",
    "            url = \"https://www.bloombergquint.com/search?q=\"\n",
    "            domain_url = \"https://www.bloombergquint.com\"\n",
    "            links = []\n",
    "            pub_date = []\n",
    "            scraped_date = []\n",
    "            title = []\n",
    "            text = []\n",
    "            for keyword in keywords:\n",
    "                try:\n",
    "                    page = requests.get(url+keyword)\n",
    "                    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                except:\n",
    "                    err = \"Bllomberg quint: err: Failed to access main url: \" + url + keyword + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    return None\n",
    "                for a in soup.find_all(\"a\",{\"class\":\"list-story-m__item__link__2mfId\"}):\n",
    "                    links.append(a[\"href\"])\n",
    "            today = date.today()\n",
    "            final_links = []\n",
    "            for link in links:\n",
    "                try:\n",
    "                    # if(link.startswith(\"/\")):\n",
    "                    link = domain_url + link\n",
    "                    try:\n",
    "                        print(link)\n",
    "                        page = requests.get(link)\n",
    "                        soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                    except:\n",
    "                        err = \"Bloomberg Quint: err: Failed to access main url: \" + link + \" and convert it to soup object\"\n",
    "                        err_logs.append(err)\n",
    "                    # return None\n",
    "                    pub_date.append(soup.find(\"time\",{\"class\":\"desktop-only published-info-module__updated-on__2JWey\"}).string)\n",
    "                    title.append(soup.find(\"h1\",{\"class\":\"story-base-template-m__story-headline__2cNwS\"}).text)\n",
    "                    t = [ i.text for i in soup.find_all(\"div\",{\"class\":\"story-element story-element-text\"})]\n",
    "                    text.append(\" \".join(t))\n",
    "                    scraped_date.append(today)\n",
    "                    final_links.append(link)\n",
    "                except:\n",
    "                    err = \"Bloomberg Quint : err: None type object found for \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "\n",
    "            # print(len(pub_date),len(scraped_date),len(title),len(links),len(text))\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"Bloomberg Quint: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            log_errors(err_logs)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Bloombergquint not working\")\n",
    "            not_working_functions.append(\"Bloombergquint\")\n",
    "    def ecns():\n",
    "        try:\n",
    "            print(\"ECNS\")\n",
    "            err_logs = []\n",
    "            baseSearchUrl = \"http://www.ecns.cn/rss/rss.xml\"\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            err_index = []\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            ArticleBody = []\n",
    "            queryUrl = baseSearchUrl\n",
    "            try:\n",
    "                pageSource = requests.get(baseSearchUrl)\n",
    "            except:\n",
    "                err = \"ecns: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "                exit()\n",
    "            # with open(\"response.xml\", \"wb\") as f:\n",
    "            #     f.write(pageSource.content)\n",
    "            # break\n",
    "            parsedSource = BeautifulSoup(pageSource.content, \"xml\")\n",
    "            for eachItem in parsedSource.find_all(\"item\"):\n",
    "                currentArticleTitle = eachItem.find(\"title\").text\n",
    "                currentArticleLink = eachItem.find(\"link\").text\n",
    "                currentArticleDate = datetime.strptime(str(eachItem.find(\"pubDate\").text).split(\"GMT\", maxsplit=2)[0].strip(),\n",
    "                                                    \"%Y-%m-%d %H:%M:%S\").strftime(\"%d-%m-%Y\")\n",
    "                # articleText = str(eachItem.find(\"description\").text).split(\"CDATA[ \", maxsplit=2)[1].rstrip(\" ]]\")\n",
    "                articleText = str(eachItem.find(\"description\").text).split(\"CDATA[ \", maxsplit=2)[0].replace(\"#039;\", \"\")\n",
    "                ArticleBody.append(articleText)\n",
    "\n",
    "                titles.append(currentArticleTitle)\n",
    "                links.append(currentArticleLink)\n",
    "                ArticleDates.append(currentArticleDate)\n",
    "                ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "            scrapedData[\"title\"] = titles\n",
    "            scrapedData[\"link\"] = links\n",
    "            scrapedData[\"publish_date\"] = ArticleDates\n",
    "            scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "            scrapedData[\"text\"] = ArticleBody\n",
    "            print(scrapedData)\n",
    "            # DataFrame creation\n",
    "            ecnsDF = pd.DataFrame(scrapedData)\n",
    "            ecnsDF = ecnsDF.drop_duplicates(subset=[\"link\"])\n",
    "            if ecnsDF.empty:\n",
    "                err = \"ecns: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(ecnsDF)\n",
    "            # emptydataframe(\"ECNS Canada\",df)\n",
    "            # log_errors(err_logs)\n",
    "            # df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"ECNS canada not working\")\n",
    "            not_working_functions.append(\"ECNS\")\n",
    "    def energy_voice():\n",
    "        try :\n",
    "            print(\"energy voice\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.energyvoice.com/?s=ipo\"\n",
    "            domain_url = \"https://www.energyvoice.com\"\n",
    "            title,links,text,pub_date,scraped_date = [],[],[],[],[]\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "                soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(soup)\n",
    "                \n",
    "            except:\n",
    "                err = \"energy voice : err : Couldn't fetch \" + url \n",
    "                err_logs.append(err)\n",
    "                return \n",
    "            all_divs = soup.find_all(\"h2\",{\"class\":\"title title--sm\"})\n",
    "            for div in all_divs:\n",
    "                links.append(div.a[\"href\"])\n",
    "            #Fetch all the necessary data \n",
    "            # print(links)\n",
    "            final_links = []\n",
    "            today = date.today()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    page = requests.get(link)\n",
    "                    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "                # print(\"hi\")\n",
    "                # print(soup)\n",
    "                # print(link)\n",
    "                except:\n",
    "                    err = \"energy voice : err : Couldn't fetch url \" + link \n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                #Title of article\n",
    "\n",
    "                title.append(soup.find(\"h1\" , {\"class\" : \"title entry-title\"}).text)\n",
    "                # print(title)\n",
    "                #Published Date\n",
    "                pub_date.append(soup.find(\"span\" , {\"class\" :\"post-timestamp__published\"}).text)\n",
    "                # print(pub_date)\n",
    "\n",
    "                #Text of article\n",
    "                div = soup.find(\"div\" , {\"class\" : \"cms clearfix\"})\n",
    "                t = \"\"\n",
    "                for i in div.find_all(\"p\"):\n",
    "                    t += i.text + \" \"\n",
    "                text.append(t)\n",
    "                # print(text)\n",
    "\n",
    "                #Scrapped date \n",
    "                scraped_date.append(str(today))\n",
    "\n",
    "                #Working links\n",
    "                final_links.append(link)\n",
    "            df = pd.DataFrame({\"text\":text,\"link\":final_links,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "            if df.empty:\n",
    "                err = \"energy voice : err: Empty datenergy voiceame\"\n",
    "                err_logs.append(err)\n",
    "            df = df.drop_duplicates(subset=[\"link\"])\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"energy voice\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except :\n",
    "            print(\"energy voice not working\")\n",
    "            not_working_functions.append('energy voice')\n",
    "    def euroNews():\n",
    "        try:\n",
    "            print(\"Euronews\")\n",
    "            err_logs = []\n",
    "            err_index = []\n",
    "            ArticleDates = []\n",
    "            ScrapeDates = []\n",
    "            ArticleBody = []\n",
    "            baseSearchUrl = \"https://www.euronews.com/search?query=ipo\"\n",
    "            domainUrl = \"https://www.euronews.com\"\n",
    "\n",
    "            scrapedData = {}\n",
    "            links = []\n",
    "            titles = []\n",
    "            queryUrl = baseSearchUrl\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                pageSource = requests.get(queryUrl, headers=headers)\n",
    "            except:\n",
    "                err = \"euroNews: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                err_logs.append(err)\n",
    "            # with open(\"response.html\", \"wb\") as f:\n",
    "            #     f.write(pageSource.content)\n",
    "            parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "            # if parsedSource.find(\"div\", class_=\"fpj_bignews\"):\n",
    "            #     bigNews = parsedSource.find(\"div\", class_=\"fpj_bignews\")\n",
    "            #     currentArticleTitle = bigNews.find(\"h3\").text\n",
    "            #     titles.append(currentArticleTitle)\n",
    "            #     currentArticleLink = bigNews.find(\"a\")[\"href\"]\n",
    "            #     links.append(currentArticleLink)\n",
    "            #     sourceDateTime = datetime.strptime(bigNews.find(\"span\", class_=\"dateTime\").text, \"%d %B %Y, %I:%M %p\").strftime(\n",
    "            #         \"%d-%m-%Y\")\n",
    "            #     ArticleDates.append(sourceDateTime)\n",
    "            #     ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "            for item in parsedSource.find(\"div\", class_=\"o-block-listing__content\").find_all(\"article\"):\n",
    "                requiredTag = item.find(\"h3\").find(\"a\")\n",
    "                currentArticleTitle = requiredTag[\"title\"]\n",
    "                # print(currentArticleTitle)\n",
    "                currentArticleLink = requiredTag[\"href\"]\n",
    "                # print(currentArticleLink)\n",
    "                if currentArticleLink[0] == \"/\":\n",
    "                    links.append(domainUrl + currentArticleLink)\n",
    "                else:\n",
    "                    links.append(currentArticleLink)\n",
    "                titles.append(currentArticleTitle)\n",
    "                sourceDateTime = datetime.strptime(item.find(\"time\", class_=\"m-object__date u-margin-top-2\").text.strip(), \"%d/%m/%Y\").strftime(\n",
    "                    \"%d-%m-%Y\")\n",
    "                ArticleDates.append(sourceDateTime)\n",
    "                ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "            scrapedData[\"title\"] = titles\n",
    "            scrapedData[\"link\"] = links\n",
    "            scrapedData[\"publish_date\"] = ArticleDates\n",
    "            scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "            # print(len(titles), titles)\n",
    "            # print(len(links), links)\n",
    "            # print(len(ArticleDates), ArticleDates)\n",
    "\n",
    "            # Article's date and description scraping\n",
    "            for link in links:\n",
    "                articleText = \"\"\n",
    "                try:\n",
    "                    pageSource = requests.get(link, headers=headers)\n",
    "                    parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                except:\n",
    "                    err = \"euroNews: err: Failed to access article link : \" + link\n",
    "                    ArticleDates.append(\"Error\")\n",
    "                    err_logs.append(err)\n",
    "                    err_index.append(link)\n",
    "                    continue\n",
    "                # with open(\"response.html\", \"wb\") as f:\n",
    "                #     f.write(pageSource.content)\n",
    "                # break\n",
    "                requiredTag = parsedSource.find(\"div\", class_=\"c-article__full_article\")\n",
    "                if requiredTag:\n",
    "                    for item in requiredTag.find_all(\"p\"):\n",
    "                        articleText += item.text.strip()\n",
    "                    # print(articleText)\n",
    "                    ArticleBody.append(articleText)\n",
    "                else:\n",
    "                    articleText = titles[links.index(link)]\n",
    "                    ArticleBody.append(articleText)\n",
    "\n",
    "            scrapedData[\"text\"] = ArticleBody\n",
    "            # print(len(ArticleBody), ArticleBody)\n",
    "\n",
    "            # Clean and Normalize links\n",
    "            if len(err_index) != 0:\n",
    "                for e in err_index:\n",
    "                    idx = scrapedData[\"link\"].index(e)\n",
    "                    scrapedData[\"link\"].pop(idx)\n",
    "                    scrapedData[\"title\"].pop(idx)\n",
    "                    scrapedData[\"publish_date\"].pop(idx)\n",
    "\n",
    "            # DataFrame creation\n",
    "            euroNewsDF = pd.DataFrame(scrapedData)\n",
    "            euroNewsDF = euroNewsDF.drop_duplicates(subset=[\"link\"])\n",
    "            if euroNewsDF.empty:\n",
    "                err = \"euroNews: err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            df = FilterFunction(euroNewsDF)\n",
    "            emptydataframe(\"Euro news\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            print(\"Euronews not working\")\n",
    "            not_working_functions.append(\"Euro news europe\")\n",
    "    \n",
    "    import requests\n",
    "\n",
    "    def theFreePressJournal():\n",
    "            try:\n",
    "                print(\"The free press journal\")\n",
    "                err_logs = []\n",
    "                err_index = []\n",
    "                ArticleDates = []\n",
    "                ScrapeDates = []\n",
    "                ArticleBody = []\n",
    "                baseSearchUrl = \"https://www.freepressjournal.in/search?q=ipo\"\n",
    "                domainUrl = \"https://www.freepressjournal.in\"\n",
    "\n",
    "                scrapedData = {}\n",
    "                links = []\n",
    "                titles = []\n",
    "                queryUrl = baseSearchUrl\n",
    "                try:\n",
    "                    headers = {\n",
    "                        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/58.0.3029.110 Safari/537.36\"}\n",
    "                    pageSource = requests.get(queryUrl, headers=headers)\n",
    "                except:\n",
    "                    err = \"theFreePressJournal: err: Failed to access search url: \" + queryUrl + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                # with open(\"response.html\", \"wb\") as f:\n",
    "                #     f.write(pageSource.content)\n",
    "                parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                if parsedSource.find(\"div\", class_=\"fpj_bignews\"):\n",
    "                    bigNews = parsedSource.find(\"div\", class_=\"fpj_bignews\")\n",
    "                    currentArticleTitle = bigNews.find(\"h3\").text\n",
    "                    titles.append(currentArticleTitle)\n",
    "                    currentArticleLink = bigNews.find(\"a\")[\"href\"]\n",
    "                    links.append(currentArticleLink)\n",
    "                    sourceDateTime = datetime.strptime(bigNews.find(\"span\", class_=\"dateTime\").text, \"%d %B %Y, %I:%M %p\").strftime(\n",
    "                        \"%d-%m-%Y\")\n",
    "                    ArticleDates.append(sourceDateTime)\n",
    "                    ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "                for item in parsedSource.find(\"div\", class_=\"fpj_newList\").find_all(\"li\"):\n",
    "                    requiredTag = item.find(\"span\", class_=\"fpj_title\")\n",
    "                    currentArticleTitle = requiredTag.text\n",
    "                    # print(currentArticleTitle)\n",
    "                    currentArticleLink = item.find(\"a\")[\"href\"]\n",
    "                    # print(currentArticleLink)\n",
    "                    if currentArticleLink[0] == \"/\":\n",
    "                        links.append(domainUrl + currentArticleLink)\n",
    "                    else:\n",
    "                        links.append(currentArticleLink)\n",
    "                    titles.append(currentArticleTitle)\n",
    "                    sourceDateTime = datetime.strptime(item.find(\"i\", class_=\"dateTime\").text, \"%d %B %Y, %I:%M %p\").strftime(\n",
    "                        \"%d-%m-%Y\")\n",
    "                    ArticleDates.append(sourceDateTime)\n",
    "                    ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "                for item in parsedSource.find(\"div\", class_=\"fpj_lsh\").find_all(\"li\"):\n",
    "                    requiredTag = item.find(\"h3\")\n",
    "                    currentArticleTitle = requiredTag.text\n",
    "                    # print(currentArticleTitle)\n",
    "                    currentArticleLink = item.find(\"a\")[\"href\"]\n",
    "                    # print(currentArticleLink)\n",
    "                    if currentArticleLink[0] == \"/\":\n",
    "                        links.append(domainUrl + currentArticleLink)\n",
    "                    else:\n",
    "                        links.append(currentArticleLink)\n",
    "                    titles.append(currentArticleTitle)\n",
    "                    sourceDateTime = datetime.strptime(item.find(\"span\", class_=\"dateTime\").text, \"%d %B %Y, %I:%M %p\").strftime(\n",
    "                        \"%d-%m-%Y\")\n",
    "                    ArticleDates.append(sourceDateTime)\n",
    "                    ScrapeDates.append(datetime.today().strftime(\"%d-%m-%Y\"))\n",
    "\n",
    "                scrapedData[\"title\"] = titles\n",
    "                scrapedData[\"link\"] = links\n",
    "                scrapedData[\"publish_date\"] = ArticleDates\n",
    "                scrapedData[\"scraped_date\"] = ScrapeDates\n",
    "                # print(len(titles), titles)\n",
    "                # print(len(links), links)\n",
    "                # print(len(ArticleDates), ArticleDates)\n",
    "\n",
    "                # Article's date and description scraping\n",
    "                for link in links:\n",
    "                    articleText = \"\"\n",
    "                    try:\n",
    "                        pageSource = requests.get(link, headers=headers)\n",
    "                        parsedSource = BeautifulSoup(pageSource.content, \"html.parser\", from_encoding=\"iso-8859-1\")\n",
    "                    except:\n",
    "                        err = \"theFreePressJournal: err: Failed to access article link : \" + link\n",
    "                        ArticleDates.append(\"Error\")\n",
    "                        err_logs.append(err)\n",
    "                        err_index.append(link)\n",
    "                        continue\n",
    "                    # with open(\"response.html\", \"wb\") as f:\n",
    "                    #     f.write(pageSource.content)\n",
    "                    # break\n",
    "\n",
    "                    for item in parsedSource.find(\"article\",{\"id\":\"fjp-article\"}).find_all(\"p\"):\n",
    "                        articleText += item.text.strip()\n",
    "                    # print(articleText)\n",
    "                    ArticleBody.append(articleText)\n",
    "\n",
    "                scrapedData[\"text\"] = ArticleBody\n",
    "                # print(len(ArticleBody), ArticleBody)\n",
    "\n",
    "                # Clean and Normalize links\n",
    "                if len(err_index) != 0:\n",
    "                    for e in err_index:\n",
    "                        idx = scrapedData[\"link\"].index(e)\n",
    "                        scrapedData[\"link\"].pop(idx)\n",
    "                        scrapedData[\"title\"].pop(idx)\n",
    "                        scrapedData[\"publish_date\"].pop(idx)\n",
    "\n",
    "                # DataFrame creation\n",
    "                theFreePressJournalDF = pd.DataFrame(scrapedData)\n",
    "                theFreePressJournalDF = theFreePressJournalDF.drop_duplicates(subset=[\"link\"])\n",
    "                if theFreePressJournalDF.empty:\n",
    "                    err = \"theFreePressJournal: err: Empty dataframe\"\n",
    "                    err_logs.append(err)\n",
    "                df = FilterFunction(theFreePressJournalDF)\n",
    "                emptydataframe(\"The free press journal\",df)\n",
    "                df  = link_correction(df)\n",
    "                return df\n",
    "            except:\n",
    "                print(\"thefreepressjournal not working\")\n",
    "                not_working_functions.append(\"Thefreepressjournal\")\n",
    "    def aylien():\n",
    "\n",
    "        import time\n",
    "        import aylien_news_api\n",
    "        from aylien_news_api.rest import ApiException\n",
    "        from datetime import datetime,date\n",
    "        from pprint import pprint\n",
    "        import json\n",
    "        import pandas as pd\n",
    "        def setup_alyien_api():\n",
    "            configuration = aylien_news_api.Configuration()\n",
    "            # Configure API key authorization: app_id\n",
    "            configuration.api_key['X-AYLIEN-NewsAPI-Application-ID'] = '6104f1f4'\n",
    "            # Configure API key authorization: app_key\n",
    "            configuration.api_key['X-AYLIEN-NewsAPI-Application-Key'] = 'f5ddb80c14739c18cc5e40cd260ba9b1'\n",
    "            # Defining host is optional and default to https://api.aylien.com/news\n",
    "            configuration.host = \"https://api.aylien.com/news\"\n",
    "            # Create an instance of the API class\n",
    "            api_instance = aylien_news_api.DefaultApi(aylien_news_api.ApiClient(configuration))\n",
    "            return api_instance\n",
    "        def convert_to_dict(stories):\n",
    "            for index,value in enumerate(stories):\n",
    "                stories[index] = stories[index].to_dict()\n",
    "            return stories\n",
    "        def fetch_news_stories(api_instance, params={}):\n",
    "            fetched_strories = []\n",
    "            stories = None \n",
    "            while (stories is None or len(stories) > 0) and len(fetched_strories)<1300:\n",
    "                try:\n",
    "                    response = api_instance.list_stories(**params)\n",
    "                except ApiException as e:\n",
    "                    print(\"Exception when calling DefaultApi->list_stories: %s\\n\" % e)\n",
    "                stories = response.stories\n",
    "                stories = convert_to_dict(stories)\n",
    "                params['cursor'] = response.next_page_cursor\n",
    "                fetched_strories += stories\n",
    "                print(\"Fetched %d stories.Total story count so far : %d\"%(len(stories),len(fetched_strories)))\n",
    "                return fetched_strories\n",
    "        # # Current date with correct format for the API option \n",
    "        # today = datetime.now()\n",
    "        # time = str(today.strftime(\"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "\n",
    "\n",
    "        params = {\n",
    "            'text': 'IPO',\n",
    "            'published_at_start': '2022-05-12T00:00:00Z',\n",
    "            'published_at_end': '2022-05-13T23:59:00Z',\n",
    "            'cursor':'*',\n",
    "            'per_page':50\n",
    "        }\n",
    "        api_instance = setup_alyien_api()\n",
    "        today = datetime.now()\n",
    "        day = today.strftime(\"%d\")\n",
    "        month = today.strftime(\"%m\")\n",
    "        year = today.strftime(\"%y\")\n",
    "        stories = fetch_news_stories(api_instance,params)\n",
    "        text = []\n",
    "        link = []\n",
    "        pub_date = []\n",
    "        title = []\n",
    "        scraped_date = []\n",
    "        today = str(datetime.now())\n",
    "        for i in stories:\n",
    "            text.append(i[\"body\"])\n",
    "            title.append(i[\"title\"])\n",
    "            pub_date.append(\"-\".join(str(i[\"published_at\"]).strip().split(\" \")[0].split(\"-\")[::-1]))\n",
    "            link.append(i[\"links\"][\"permalink\"])\n",
    "            scraped_date.append(today)\n",
    "\n",
    "        df = pd.DataFrame({\"text\":text,\"link\":link,\"publish_date\":pub_date,\"scraped_date\":scraped_date,\"title\":title})\n",
    "        return df\n",
    "    def dw():\n",
    "        try:\n",
    "            print(\"DW\")\n",
    "            err_logs = []\n",
    "            url = \"https://www.dw.com/search/en?searchNavigationId=9097&languageCode=en&origin=gN&item=ipo\"\n",
    "            domain_url = \"https://www.dw.com/\"\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                'sec-fetch-site': 'none',\n",
    "                'sec-fetch-mode': 'navigate',\n",
    "                'sec-fetch-user': '?1',\n",
    "                'sec-fetch-dest': 'document',\n",
    "                'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "            }\n",
    "            page = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            #soup  # Debugging - if soup is working correctly\n",
    "\n",
    "            # Class names of the elements to be scraped\n",
    "            div_class = \"searchResult\"  # Class name of div containing the a tag\n",
    "            #h1_class = \"_1Y-96\"\n",
    "            #h1_div_class = \"col-xs-12\"\n",
    "            title_h1_class = \"\" # There exists no title here\n",
    "            date_ul_class = \"smallList\"\n",
    "            para_p_class = \"intro\"\n",
    "\n",
    "            links = []\n",
    "\n",
    "            for divtag in soup.find_all(\"div\", {\"class\": div_class}):\n",
    "                for a in divtag.find_all(\"a\", href=True):\n",
    "                    link = a[\"href\"]  # Gets the link\n",
    "                    \n",
    "                    # Checking the link if it is a relative link\n",
    "                    if link[0] == '/':\n",
    "                        link = domain_url + link[1:]\n",
    "                    \n",
    "                    # Filtering advertaisment links\n",
    "                    link_start = domain_url \n",
    "                    if link.startswith(link_start):\n",
    "                        links.append(link)\n",
    "            # Remove duplicates\n",
    "            links = list(set(links))\n",
    "            #links # Debugging - if link array is generated\n",
    "\n",
    "            collection = []\n",
    "            scrapper_name = \"dw\"\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    l_page = requests.get(link, headers=headers)\n",
    "                    l_soup = BeautifulSoup(l_page.content, 'html.parser')\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to retrieve data from link: \" + link + \" and convert it to soup object\"\n",
    "                    err_logs.append(err)\n",
    "                    continue\n",
    "\n",
    "                data = []\n",
    "                # Scraping the heading\n",
    "                #h1_ele = l_soup.find(\"h1\", {\"class\": h1_class})\n",
    "                \n",
    "                try:\n",
    "                    # title_ele = l_soup.find(\"h1\", {\"class\": title_h1_class})\n",
    "                    title_ele = l_soup.find(\"h1\") # There exists only 1 h1 ele in the page\n",
    "                    data.append(title_ele.text)\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find title in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "\n",
    "\n",
    "                # Adding the link to data\n",
    "                data.append(link)\n",
    "\n",
    "                # Scraping the published date\n",
    "                try:\n",
    "                    date_ele_container = l_soup.find(\"ul\", {\"class\": date_ul_class})\n",
    "                    date_text =  date_ele_container.text.split(\"\\n\")[2]\n",
    "                    #date_text = (date_text.split('/'))[-1]\n",
    "                    #date_text = date_text.replace(\" Updated: \", \"\")\n",
    "                    data.append(date_text.replace(\".\",\"-\").strip())  # The date_text could be further modified to represent a proper date format\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find date in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                \n",
    "                # Adding the scraped date to data\n",
    "                cur_date = str(datetime.today())\n",
    "                data.append(cur_date)\n",
    "                \n",
    "\n",
    "                # Scraping the paragraph\n",
    "                try:\n",
    "                    para_ele = (l_soup.findAll(\"p\", {\"class\": para_p_class}))[-1]\n",
    "                    data.append(para_ele.text)  # Need to make this better\n",
    "                except:\n",
    "                    err = scrapper_name + \": err: Failed to find paragraph in page. Link: \" + link\n",
    "                    err_logs.append(err)\n",
    "                    continue  # drops the complete data if there is an error\n",
    "                # Adding data to a collection\n",
    "                collection.append(data)\n",
    "\n",
    "            df = pd.DataFrame(collection, columns =['title', 'link','publish_date','scraped_date','text'])\n",
    "            if df.empty:\n",
    "                err = scrapper_name + \": err: Empty dataframe\"\n",
    "                err_logs.append(err)\n",
    "            #print(df) # For debugging. To check if df is created\n",
    "            #print(err_logs) # For debugging - to check if any errors occoured\n",
    "            df = FilterFunction(df)\n",
    "            emptydataframe(\"DW\",df)\n",
    "            df  = link_correction(df)\n",
    "            return df\n",
    "        except:\n",
    "            not_working_functions.append(\"DW\")\n",
    "            print(\"DW not working\")\n",
    "    df1 = korea()\n",
    "    df2 = proactive(\"ipo\")\n",
    "    df3 = Reuters(\"ipo\")\n",
    "    df4 = TradingChart()\n",
    "    df5 = GoogleAlert()\n",
    "    df6 = live(\"ipo\")\n",
    "    df7 = standartnews(\"ipo\")\n",
    "    df8 = kontan(\"ipo\")\n",
    "    df9 = AZ(\"ipo\")\n",
    "    df10 = xinhuanet()\n",
    "    df11 = AFR()\n",
    "    df12 = MoneyControl()\n",
    "    df13 = Cnbc_Seeking()\n",
    "    df14 = toi()\n",
    "    df15 = German()\n",
    "    # df16 = Italian()\n",
    "    df17 = Japannews()\n",
    "    df18 = Romania()\n",
    "    df19 = Russian()\n",
    "    df20 = Swedish()\n",
    "    df21 = GoogleAlert1()\n",
    "    df22 = GoogleAlert2()\n",
    "    df23 = GoogleAlert3()\n",
    "    df24 = GoogleAlert4()\n",
    "    df25 = GoogleAlert5()\n",
    "    df26 = IPOMonitor()\n",
    "    df27 = globallegalchronicle()\n",
    "    df28 = Seenews()\n",
    "    df29 = Bisnis()\n",
    "    df30 = RomaniaNew()\n",
    "    df31 = RomaniaInsider()\n",
    "    df32 = SpaceMoney()\n",
    "    # df33 = Carteira()\n",
    "    df34 = Kontan1() ##\n",
    "    df35 = euronews()\n",
    "    # df36 = franchdailynews()\n",
    "    # df37 = norway()\n",
    "    # df38 = localde()\n",
    "    df39 = chinatoday()\n",
    "    df40 = aljazeera()\n",
    "    df41 = Koreatimes()\n",
    "    df42 = zdnet()\n",
    "    df43 = arabNews()\n",
    "    df44 = chosun()\n",
    "    # df45 = Forbes()\n",
    "    df46 = kngnet()\n",
    "    df47 = kedg()\n",
    "    df48 = wealthx()\n",
    "    df49 = indonesia()\n",
    "    df50 = asiainsurancereview()\n",
    "    df51 = economic_times()\n",
    "    df52 = prnewswire()\n",
    "    df53 = arabfinance()\n",
    "    df54 = interfax()\n",
    "    df55 = vccircle()\n",
    "    df56 = allafrica()\n",
    "    df57 = zawya()\n",
    "    df58 = aljarida()\n",
    "    df59 = dziennik()\n",
    "    df60 = swissinfo()\n",
    "    df61 = einnews()\n",
    "    df62 = sabah()\n",
    "    df63 = livemint()\n",
    "    df64 = guardian()\n",
    "    df65 = azernews()\n",
    "    df66 = chosenilboenglish()\n",
    "    df67 = otempo()\n",
    "    df68 = elicudadona()\n",
    "    df69 = lastampa()\n",
    "    df70 = liputan6()\n",
    "    df71 = milenio()\n",
    "    df72 = scoop()\n",
    "    df73 = supchina()\n",
    "    df74 = romania_insider()\n",
    "    df75 = cnbc1()\n",
    "    df76 = aif()\n",
    "    df77 = monitor()\n",
    "    df78 = thesun()\n",
    "    df79 = parool()\n",
    "    df80 = shabiba()\n",
    "    df81 = koreannewsgazette()\n",
    "    df82 = hungary()\n",
    "    df83 = jauns()\n",
    "    df84 = pulse()\n",
    "    df85 = vnexpress()\n",
    "    df86 = jamaicaobserver()\n",
    "    df87 = independent()\n",
    "    df88 = albaniandailynews()\n",
    "    df89 = ewn()\n",
    "    df90 = bloombergquint()\n",
    "    df91 = ecns()\n",
    "    df92 = energy_voice()\n",
    "    df93 = euroNews()\n",
    "    df94 = theFreePressJournal()\n",
    "    # df95 = aylien()\n",
    "    df96 = dw()\n",
    "    df97 = star()\n",
    "    df98 = Reuters(\"pre ipo\")\n",
    "    df99 = Reuters(\"Initial Public Offering\")\n",
    "    df100 = rss()\n",
    "    # df67 = scmp()\n",
    "    # df66 = phnompenhpost()\n",
    "    df_final_1 = [df100,df46,df19,df99,df98,df97,df96,df94,df93,df92,df91,df90,df89,df88,df87,df86,df85,df84,df83,df81,df80,df79, df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11, df12,df13,df14,df15, df17,df18,df21,df22 ,df23,df24,df25,df26, df27, df28, df29,df30,df31,df32, df34, df35,df39, df40, df41,df42,df43,df44,df47,df48,df49,df50,df52,df53,df54,df55,df57, df58, df59, df60,df61,  df62,df63,df64,df65, df66,df67, df68, df69, df70, df71, df72,df73,  df74, df75, df76, df77,df78]\n",
    "    # df_final_1 = [df85,df83,df80,df79,df7,df8,df18,df29,df32,df34,df58,df59,df62,df67, df68, df69, df70, df71,df76]\n",
    "    df_final = pd.concat(df_final_1)\n",
    "#     df_final = FilterFunction(df_fin)\n",
    "    # TODO: commented out for testing as it takes too long.  uncomment later\n",
    "    \n",
    "    \n",
    "\n",
    "#     dff = FilterFunction(df_final)\n",
    "    # df_final = remove_navigablestring(df_final)\n",
    "    todays_report_filename = os.path.join(output_dir, 'todays_report.csv')\n",
    "    todays_report_filename1 = os.path.join(output_dir, 'todays_report1.csv')\n",
    "    # df_final.to_csv(todays_report_filename1,index=False)\n",
    "    final = correct_navigable_string(df_final)\n",
    "    final.to_csv(todays_report_filename1,index=False)\n",
    "    # final = translate_dataframe(final)\n",
    "    #evening \n",
    "#     final =df_final.loc[public_date == scrap_date]\n",
    "#     dff = FilterFunction(final)\n",
    "    # from googletrans import Translator\n",
    "    # import googletrans\n",
    "    # translator = Translator()\n",
    "    # t = final['title']\n",
    "    # t1 =[]\n",
    "    # for i in t:\n",
    "    # #     print(detect(i))\n",
    "    #     translations=translator.translate(i,dest=\"en\")\n",
    "    #     tt = translations.text\n",
    "    #     t1.append(tt)\n",
    "    # final['title'] = t1\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#     print(output_dir)\n",
    "    textfile = open(\"logs.txt\",\"w\")\n",
    "    for i in not_working_functions:\n",
    "        textfile.write(i+\"\\n\")\n",
    "    textfile.close()\n",
    "    logging.info(\"writing output artifact \" + todays_report_filename + \" to \" + output_dir)\n",
    "    final.to_csv(todays_report_filename,index=False)\n",
    "    logging.info(\"completed writing output artifact \" + todays_report_filename + \" to \" + output_dir)\n",
    "\n",
    "  # # final =final.loc[public_date == scrap_date]\n",
    "\n",
    "multilex_scraper(\"/home/prachi_multilex2\", \"/home/prachi_multilex2\")       # uncomment this line to run this as a python script\n",
    "\n",
    "# multilex_scraper( \"\", \"\")  \n",
    "logging.info(\"last line of scraper\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9a84a-739f-4723-aecb-801c137b0477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026898f-ad5a-4874-9ec6-704f0bb767ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9f577-ac4c-4006-8b3b-f482ea059fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bc89e3-5e44-4b85-8c0c-28cdd6cb7874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aylien_news_api in /home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages (5.1.0)\n",
      "Requirement already satisfied: python-dateutil in /home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages (from aylien_news_api) (2.8.2)\n",
      "Requirement already satisfied: six>=1.10 in /home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages (from aylien_news_api) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages (from aylien_news_api) (1.26.9)\n",
      "Requirement already satisfied: certifi in /home/prachi_multilex2/miniconda3/envs/infinstor/lib/python3.7/site-packages (from aylien_news_api) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install aylien_news_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d10229-400e-428a-b047-008e33d0c3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
